<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fitzeng</title>
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://fitzeng.org/"/>
  <updated>2018-03-21T03:40:03.821Z</updated>
  <id>http://fitzeng.org/</id>
  
  <author>
    <name>fitzeng</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>辗转相除法证明了解一下</title>
    <link href="http://fitzeng.org/2018/03/21/AlgoGCD/"/>
    <id>http://fitzeng.org/2018/03/21/AlgoGCD/</id>
    <published>2018-03-21T03:27:00.000Z</published>
    <updated>2018-03-21T03:40:03.821Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：本着弄懂每一个小知识点的学习原则，打算彻底理清一下辗转相除法。</p>
</blockquote>
<a id="more"></a>
<h2 id="1-什么是辗转相除法？"><a href="#1-什么是辗转相除法？" class="headerlink" title="1 什么是辗转相除法？"></a>1 什么是辗转相除法？</h2><p><a href="https://zh.wikipedia.org/wiki/%E8%BC%BE%E8%BD%89%E7%9B%B8%E9%99%A4%E6%B3%95" target="_blank" rel="external">辗转相除法，又称欧几里得算法……</a></p>
<h2 id="2-为什么辗转相除可以求解最大公约数？"><a href="#2-为什么辗转相除可以求解最大公约数？" class="headerlink" title="2 为什么辗转相除可以求解最大公约数？"></a>2 为什么辗转相除可以求解最大公约数？</h2><p>可能我们会凭记忆使用这个简单的算法，但是如果我们多问自己一个为什么？思路就有点模糊了，可能会想：“是啊，为什么呢？难道又是玄学？”。下面就给出一系列疑问的证明：</p>
<p>首先我们给定两个数 \( a, b \) 并且约定 \( a \ge b &gt; 0\)：</p>
<p>如果 \( a\%b = 0 \)，则 \( gcd(a, b) = b \) <br><br>如果 \( a\%b \neq 0 \)，则设 \( a = n*b + m \) <br><br>根据辗转相除法，我们可以得出 \( gcd(a, b) = gcd(b, a\%b) \)<br>疑问就在这里，等式为什么成立？我们拆分为两个子问题：</p>
<p>Q1: \( gcd(a, b) \) 是 \( (b, a\%b) \) 的一个公约数，其中 \( a\%b = m \)。</p>
<p>因为:</p>
<p>$$<br>a = k_1 * gcd(a, b) \\<br>b = k_2 * gcd(a, b)<br>$$</p>
<p>所以：</p>
<p>$$<br>a\%b = m = a - n * b = (k_1 - n * k_2) * gcd(a, b)<br>$$</p>
<p>由于 \( (k_1 - n * k_2) \) 是正整数，所以 \( gcd(a, b) \) 是 \( (b, a\%b) \) 的一个公约数。</p>
<p>Q2: 如何证明 \( gcd(a, b) \) 就是 \( gcd(b, a\%b) \)，换就话说如何证明是“最大”公约数？</p>
<p>我们来重新审视一下条件：</p>
<p>$$<br>\begin{aligned}<br>b &amp;= k_2 * gcd(a, b) \\<br>a\%b = m = a - n * b &amp;= (k_1 - n * k_2) * gcd(a, b)<br>\end{aligned}<br>$$</p>
<p>要想证明 \( gcd(a, b) \) 是 \( gcd(b, a\%b) \)，也就是说证明 \( k_2, (k_1 - n <em> k_2) \) 互质，即 \( gcd(k_2, k_1 - n </em> k_2) = 1 \)。</p>
<p>由于 \( n \) 是正整数，不好直接证明，那么我们取特殊的当 \( n = 1 \) 时到底是否成立？即 \( gcd(k_2, k_1 - k_2) = 1 \)。如果你对数论有点了解可能立马就知道 \( gcd(num1, num2) = gcd(num1, num1 - num2) \)，又因为 \( gcd(k_1, k_2) = 1 \) 所以得证；如果你不知道也没关系，我们可以证明，毕竟我们就是来搞懂一个个细节的。</p>
<p>因为：<br>$$<br>a = k_1 * gcd(a, b) \\<br>b = k_2 * gcd(a, b)<br>$$</p>
<p>所以：<br>$$<br>gcd(k_1, k_2) = 1<br>$$</p>
<p>在 \( k_1, k_2 \) 互质的情况下，\( k_1 - k_2 \) 不可能含有数值为 \( k_1 \) 或 \( k_2 \) 因子。 否则 \( gcd(k_1, k_2) \neq 1 \)。</p>
<p>所以结论 \( gcd(k_2, k_1 - k_2) = 1 \) 得证。<br>继续使用这个结论可以得出：</p>
<p>$$<br>\begin{aligned}<br>1 &amp;= gcd(k_2, k_1 - k_2) \\<br>&amp;= gcd(k_2, k_1 - k_2 - k_2) \\<br>&amp;= \ldots \\<br>&amp;= gcd(k_2, k_1 - n * k_2)<br>\end{aligned}<br>$$</p>
<p>所以有：</p>
<p>$$<br>gcd(a, b) = gcd(b, a\%b)<br>$$</p>
<p>然后一直迭代就可以啦！</p>
<p>如果里面有什么错误欢迎指出来，可以一起探讨。</p>
<h2 id="3-如何高效实现？"><a href="#3-如何高效实现？" class="headerlink" title="3 如何高效实现？"></a>3 如何高效实现？</h2><p>请参考<a href="https://zhuanlan.zhihu.com/p/31824895" target="_blank" rel="external">漫画算法：辗转相除法是什么鬼？</a></p>
<h2 id="4-参考"><a href="#4-参考" class="headerlink" title="4 参考"></a>4 参考</h2><ul>
<li><a href="https://zh.wikipedia.org/wiki/%E8%BC%BE%E8%BD%89%E7%9B%B8%E9%99%A4%E6%B3%95" target="_blank" rel="external">wiki：辗转相除法</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/31824895" target="_blank" rel="external">zhihu: 漫画算法：辗转相除法是什么鬼？</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：本着弄懂每一个小知识点的学习原则，打算彻底理清一下辗转相除法。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Algo" scheme="http://fitzeng.org/tags/Algo/"/>
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 初级 API 之 Tensor 篇</title>
    <link href="http://fitzeng.org/2018/03/20/GPTFLAPITensor/"/>
    <id>http://fitzeng.org/2018/03/20/GPTFLAPITensor/</id>
    <published>2018-03-20T03:27:00.000Z</published>
    <updated>2018-03-21T01:12:49.418Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>写在前面：此文的行文风格参考<a href="http://fitzeng.org/2018/03/13/GPTFInstallation/">此文</a>，主线为主，不会过多介绍细枝末节。</p>
</blockquote>
<a id="more"></a>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<p>正如框架名所暗示那样，TensorFlow 就是一系列 Tensor 在图之间 Flow 从而完成各种计算。Tensor 是矢量、矩阵往高维推广的一般形式。</p>
<p>在 TensorFlow 中，张量（Tensor）是不存储具体数值的，它有一下三个关键的属性：name，dtype，shape。名字是在数据流图中的一个关键标志；类型用来标注数据；形状可以用于检测数据流图中的数据是否符合数学规范和广播机制。</p>
<p>在编程过程中，使用频率较高的张量类是：tf.Variable，tf.constant，tf.placeholder，tf.SparseTensor。其中除了 tf.Variable 是可变的之外，其它都是不可变的，但是在运行时输入的数据不同可以产生不同的结果。</p>
<h2 id="1-秩"><a href="#1-秩" class="headerlink" title="1 秩"></a>1 秩</h2><p>秩是张量的维数，它和线性代数中矩阵的秩的定义有所不同。下表给出了张量秩的直观理解：</p>
<table>
<thead>
<tr>
<th style="text-align:center">秩</th>
<th style="text-align:center">数学称呼</th>
<th style="text-align:center">例子</th>
<th style="text-align:center">形状</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">标量</td>
<td style="text-align:center">$$ 1 $$</td>
<td style="text-align:center">$$ () $$</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">矢量</td>
<td style="text-align:center">$$ [1, 1, 1, 1] $$</td>
<td style="text-align:center">$$ (4, ) $$</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">矩阵</td>
<td style="text-align:center">$$ \begin{align} [&amp;[1,1,1], \\ &amp;[2,2,2]] \end{align} $$</td>
<td style="text-align:center">$$ (2, 3) $$</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">3 维张量</td>
<td style="text-align:center">$$ \begin{align} [[&amp;[1,1,1], \\ &amp;[2,2,2]], \\ [&amp;[1,1,1], \\ &amp;[2,2,2]]] \end{align} $$</td>
<td style="text-align:center">$$ (2, 2, 3) $$</td>
</tr>
<tr>
<td style="text-align:center">n</td>
<td style="text-align:center">n 维张量</td>
<td style="text-align:center">$$ \ldots $$</td>
<td style="text-align:center">$$ \ldots $$</td>
</tr>
</tbody>
</table>
<h3 id="1-1-标量：0-秩"><a href="#1-1-标量：0-秩" class="headerlink" title="1.1 标量：0 秩"></a>1.1 标量：0 秩</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">strver = tf.Variable(&quot;Hello&quot;, tf.string)</div><div class="line">num = tf.Variable(1, tf.int16)</div><div class="line">pi = tf.Variable(3.14, tf.float64)</div><div class="line">complex = tf.Variable(1 - 2j, tf.complex64)</div></pre></td></tr></table></figure>
<h3 id="1-2-矢量：1-秩"><a href="#1-2-矢量：1-秩" class="headerlink" title="1.2 矢量：1 秩"></a>1.2 矢量：1 秩</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">strver = tf.Variable([&quot;Hello&quot;], tf.string)</div><div class="line">num = tf.Variable([1, 2, 3], tf.int16)</div><div class="line">pi = tf.Variable([3.14], tf.float64)</div><div class="line">complex = tf.Variable([1 - 2j], tf.complex64)</div></pre></td></tr></table></figure>
<h3 id="1-3-n维张量：高秩"><a href="#1-3-n维张量：高秩" class="headerlink" title="1.3 n维张量：高秩"></a>1.3 n维张量：高秩</h3><p>从前面可以得知，秩和维数息息相关。秩增加意味着维度的提升。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">matrix = tf.Variable([[1, 1], [2, 2]], tf.int16)</div></pre></td></tr></table></figure>
<h3 id="1-4-获取秩"><a href="#1-4-获取秩" class="headerlink" title="1.4 获取秩"></a>1.4 获取秩</h3><p>通过 tf.rank() 函数获取张量的秩。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">r_test = tf.Variable([[[1],[1]]])</div><div class="line">r = tf.rank(r_test)</div><div class="line">with tf.Session() as sess:</div><div class="line">    print(sess.run(r))</div><div class="line">    </div><div class="line"># Output:</div><div class="line"># 3</div></pre></td></tr></table></figure>
<h3 id="1-5-索引"><a href="#1-5-索引" class="headerlink" title="1.5 索引"></a>1.5 索引</h3><p>在提取某些具有共同特征的数据时，我们要使用一种比较特殊的索引方式：切片。</p>
<p>对于标量来说，没有必要进行索引。<br>对于矢量来说，通过 <code>[]</code> 进行索引，可以获取某个位置的数值。<br>对于 2 维以上的张量，可以采取切片的形式，和 numpy 中操作一致。</p>
<h2 id="2-形状"><a href="#2-形状" class="headerlink" title="2 形状"></a>2 形状</h2><p>形状是各个维度的元素数量，在构建图的过程中，TensorFlow 会自行检测维度。</p>
<table>
<thead>
<tr>
<th style="text-align:center">秩</th>
<th style="text-align:center">形状</th>
<th style="text-align:center">维数</th>
<th style="text-align:center">示例</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">0</td>
<td style="text-align:center">[]</td>
<td style="text-align:center">0-D</td>
<td style="text-align:center">2</td>
</tr>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">[D0 ]</td>
<td style="text-align:center">1-D</td>
<td style="text-align:center">(5)</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">[D0, D1]</td>
<td style="text-align:center">2-D</td>
<td style="text-align:center">(3, 4)</td>
</tr>
<tr>
<td style="text-align:center">3</td>
<td style="text-align:center">[D0, D1, D2]</td>
<td style="text-align:center">3-D</td>
<td style="text-align:center">(1, 4, 3)</td>
</tr>
<tr>
<td style="text-align:center">n</td>
<td style="text-align:center">[D0, D1, … Dn-1]</td>
<td style="text-align:center">n-D</td>
<td style="text-align:center">(D0, D1, … Dn-1)</td>
</tr>
</tbody>
</table>
<h3 id="2-1-获取形状"><a href="#2-1-获取形状" class="headerlink" title="2.1 获取形状"></a>2.1 获取形状</h3><p>有两种方式可以获取：在构建图时，读取 tf.Tensor 对象，返回一个 TensorShape 对象，这种方式往往只能获取某些维度的数据；另一种方式是在运行时获取的 tf.Tensor 对象进行 tf.shape 操作获取形状。</p>
<h3 id="2-2-改变形状"><a href="#2-2-改变形状" class="headerlink" title="2.2 改变形状"></a>2.2 改变形状</h3><p>张量元素和是形状元组元素的乘积，标量是 1。只需要调用 tf.reshape 操作便可将张量改成需要的形状。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">rank_3d = tf.ones([3, 4, 5])</div><div class="line">matrix = tf.reshape(rank_3d, [6, 10])</div><div class="line">matrixB = tf.reshape(matrix, [3, -1])</div><div class="line">matrixAlt = tf.reshape(matrixB, [4, 3, -1])</div><div class="line">error = tf.reshape(matrixAlt, [11, 2, -1])</div></pre></td></tr></table></figure>
<p>原始三维张量总数为 60，所以可以变成 [6, 10]，[3, 20], [4, 3, 5]。而 [11, 2, -1] 就会报错：60 无法被 22 整除。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ValueError: Dimension size must be evenly divisible by 22 but is 60 for &apos;Reshape_3&apos; (op: &apos;Reshape&apos;) with input shapes: [4,3,5], [3] and with input tensors computed as partial shapes: input[1] = [11,2,?].</div></pre></td></tr></table></figure>
<h2 id="3-数据类型"><a href="#3-数据类型" class="headerlink" title="3 数据类型"></a>3 数据类型</h2><p>每一个张量都有其唯一的类型，其中，最常用的就是下表的 14 种实数类型：</p>
<p>14 种实数类型表格</p>
<table>
<thead>
<tr>
<th style="text-align:center">数据类型</th>
<th style="text-align:center">Python 类型</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">浮点型</td>
<td style="text-align:center">tf.float32, tf.float64</td>
</tr>
<tr>
<td style="text-align:center">整型</td>
<td style="text-align:center">tf.int8, tf.int16, tf.int32, tf.int64, tf.uint8</td>
</tr>
<tr>
<td style="text-align:center">布尔型</td>
<td style="text-align:center">tf.bool</td>
</tr>
<tr>
<td style="text-align:center">复数</td>
<td style="text-align:center">tf.complex64, tf.complex128</td>
</tr>
</tbody>
</table>
<p>同时可以将其它类型的数据结构序列化成 string 存储。tf.cast 可以进行类型装转化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">int_tensor = tf.constant([1, 2, 3], dtype=tf.int32)</div><div class="line">float_tensor = tf.cast(int_tensor, dtype=tf.float32)</div><div class="line">print(int_tensor.dtype, float_tensor.dtype)</div><div class="line"></div><div class="line"># Output:</div><div class="line"># &lt;dtype: &apos;int32&apos;&gt; &lt;dtype: &apos;float32&apos;&gt;</div></pre></td></tr></table></figure>
<p>值得注意的是：在创建张量时，可以不显示指定类型，TensorFlow 会自行选择一个。</p>
<h2 id="4-评估张量"><a href="#4-评估张量" class="headerlink" title="4 评估张量"></a>4 评估张量</h2><p>通常来说，可以以通过直接 run() 获取张量的信息。但是 TensorFlow 提供了一个更加方便的函数：eval()，必须激活默认会话环境才会有效。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">constant = tf.constant([1, 2, 3])</div><div class="line">tensor = constant * constant</div><div class="line">with tf.Session().as_default():</div><div class="line">    print(tensor.eval())</div><div class="line"></div><div class="line"># Output:</div><div class="line"># [1 4 9]</div></pre></td></tr></table></figure>
<p>有些依赖于动态信息的值是无法评估的，因为在运行时才能确定的信息评估函数无法获取。比如带有占位符的张量只有在提供其值得情况下才能评估。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">p = tf.placeholder(tf.float32)</div><div class="line">t = p + 1.0</div><div class="line">with tf.Session().as_default():</div><div class="line">    print(t.eval())</div><div class="line">    print(t.eval(feed_dict=&#123;p: 2.0&#125;))</div></pre></td></tr></table></figure>
<p>其它的模型可能会使评估变得复杂，比如说依赖于队列的张量，TensorFlow 不能直接评估，它只能在有元素入队后才能进行评估，没有的话就会挂起。但是可以在评估前调用 tf.train.start_queue_runners 来解决这个问题。</p>
<h2 id="5-打印张量"><a href="#5-打印张量" class="headerlink" title="5 打印张量"></a>5 打印张量</h2><p>在调试过程中，使用频率较高的方法就是打印。最直接的凡是就是直接使用 print() 函数，但是 TensorFlow 提供了一个内置的 tf.Print 操作。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">t = tf.constant(3)</div><div class="line">tf.Print(t, [t, t, t])</div><div class="line">t = tf.Print(t, [t, t, t])</div><div class="line">result = t + 1</div><div class="line">with tf.Session().as_default():</div><div class="line">    print(result.eval())</div><div class="line"></div><div class="line"># Output:</div><div class="line"># 4</div><div class="line"># [3][3][3]</div></pre></td></tr></table></figure>
<p>第一个参数传递一个张量，第二个参数是一个链表，代表在评估时需要打印的张量。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;写在前面：此文的行文风格参考&lt;a href=&quot;http://fitzeng.org/2018/03/13/GPTFInstallation/&quot;&gt;此文&lt;/a&gt;，主线为主，不会过多介绍细枝末节。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="TensorFlow" scheme="http://fitzeng.org/tags/TensorFlow/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 初级 API 之简介篇</title>
    <link href="http://fitzeng.org/2018/03/19/GPTFLAPIIntroduction/"/>
    <id>http://fitzeng.org/2018/03/19/GPTFLAPIIntroduction/</id>
    <published>2018-03-19T03:27:00.000Z</published>
    <updated>2018-03-20T08:25:50.013Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>写在前面：此文的行文风格参考<a href="http://fitzeng.org/2018/03/13/GPTFInstallation/">此文</a>，主线为主，不会过多介绍细枝末节。</p>
</blockquote>
<a id="more"></a>
<h2 id="1-基本介绍"><a href="#1-基本介绍" class="headerlink" title="1 基本介绍"></a>1 基本介绍</h2><p>TensorFlow 初级 API 是学习了解这个框架的核心内容。通过这些基础的 API 可以自行控制数据流图和运行时内容。</p>
<h2 id="2-准备"><a href="#2-准备" class="headerlink" title="2 准备"></a>2 准备</h2><p>在一切安装工作都就绪的情况下，直接将 TensorFlow 包导入就可以使用了，为了方便数据的处理，可以使用 numpy 作为辅助工具。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div></pre></td></tr></table></figure>
<h2 id="2-数据模型"><a href="#2-数据模型" class="headerlink" title="2 数据模型"></a>2 数据模型</h2><p>TensorFlow 的核心数据单元就是 Tensor，可译为<code>张量</code>。可以简单理解为它是由大量数字按照某种排列组成的数组。秩是维数，形状是每一维的元素个数。为了方便可以直接使用 numpy 数组进行操作。</p>
<h2 id="3-核心流程"><a href="#3-核心流程" class="headerlink" title="3 核心流程"></a>3 核心流程</h2><p>TensorFlow 的核心编程思想有两个步骤：</p>
<ol>
<li>构建图</li>
<li>计算图</li>
</ol>
<h3 id="3-1-图"><a href="#3-1-图" class="headerlink" title="3.1 图"></a>3.1 图</h3><p>图是由一连串数据操作组成的。里面包含了两个基本元素：点和边。点代表操作，可以是数据输入输出，也可以是数学操作，例如四则运算等；边代表数据流向。下面给出一个小示例：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">x = tf.constant(1)</div><div class="line">y = tf.constant(1)</div><div class="line">sum = x + y</div><div class="line">print(&quot;x:&quot;, x, &quot;\ny:&quot;, y, &quot;\nsum:&quot;, sum)</div><div class="line"></div><div class="line"># Output:</div><div class="line"># x: Tensor(&quot;Const:0&quot;, shape=(), dtype=int32) </div><div class="line"># y: Tensor(&quot;Const_1:0&quot;, shape=(), dtype=int32) </div><div class="line"># sum: Tensor(&quot;add:0&quot;, shape=(), dtype=int32)</div></pre></td></tr></table></figure>
<p>从输出可以看出，在构建图的时候并不会把数据填充进去。</p>
<h3 id="3-2-TensorBoard"><a href="#3-2-TensorBoard" class="headerlink" title="3.2 TensorBoard"></a>3.2 TensorBoard</h3><p>TensorFlow 提供了一个供机器学习者将数据可视化的工具：TensorBoard。可以通过以下示例进行可视化：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">writer = tf.summary.FileWriter(&apos;./logs&apos;)</div><div class="line">writer.add_graph(tf.get_default_graph())</div></pre></td></tr></table></figure>
<p>在 <code>./logs</code> 目录下会生成一个命名为 <code>events.out.tfevents.1521526975.zzj</code> 的文件。格式为<code>events.out.tfevents.{timestamp}.{hostname}</code>，在终端中执行命令<code>tensorboard --logdir ./logs</code>。打开 TensorBoard（默认开启的是本地 6006 端口），可以看到下图面板。</p>
<p><img src="/2018/03/19/GPTFLAPIIntroduction/TensorBoardAddOp.png" alt=""></p>
<h3 id="3-3-会话"><a href="#3-3-会话" class="headerlink" title="3.3 会话"></a>3.3 会话</h3><p>会话（Session）是对数据流图运行时的一个封装，如果把图理解为一个文件，那么会话就是执行这个文件。</p>
<p>创建会话</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">print(sess.run(&#123;&apos;x, y&apos;: (x, y), &apos;sum&apos;: sum&#125;))</div><div class="line"></div><div class="line"># Output:</div><div class="line"># &#123;&apos;x, y&apos;: (1, 1), &apos;sum&apos;: 2&#125;</div></pre></td></tr></table></figure>
<p>会话对象通过执行 <code>run()</code> 操作来计算图中的节点的值。</p>
<h3 id="3-3-运行时赋值"><a href="#3-3-运行时赋值" class="headerlink" title="3.3 运行时赋值"></a>3.3 运行时赋值</h3><p>前面的操作都是提前准备好数据然后构建图进行计算，TensorFlow 还提供了一种更加强的功能：运行时赋值。通过占位符的形式构建图，在运算之前不消耗大量资源，开启会话时把需要的数据填充进去，这种策略和张量不保存具体值的设计形式息息相关。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">feedx = tf.placeholder(tf.float32)</div><div class="line">feedy = tf.placeholder(tf.float32)</div><div class="line">feedsum = feedx + feedy</div><div class="line">sess = tf.Session()</div><div class="line">print(sess.run(feedsum, feed_dict=&#123;feedx: 1, feedy: 1.23&#125;))</div><div class="line">print(sess.run(feedsum, feed_dict=&#123;feedx: [1, 1], feedy: [1, 2]&#125;))</div><div class="line"></div><div class="line"># Output:</div><div class="line"># 2.23</div><div class="line"># [2. 3.]</div></pre></td></tr></table></figure>
<p>值得一提的是，对于这种设计，在开启会话前只检查图的合理性，也就是数据流的维数是否符合数学规范。开启会话后，如果没有给输入节点输入初始值的话会直接抛出异常。</p>
<h3 id="4-数据集"><a href="#4-数据集" class="headerlink" title="4 数据集"></a>4 数据集</h3><p>在前面提到的填充操作不太符合用户友好的设计理念，因为每一条数据都需要用户自己去维护。但是 TensorFlow 的设计团队提供了一个数据集的类，里面有很多数据批处理的操作：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">datasets = [</div><div class="line">    [0, 4, ],</div><div class="line">    [1, 5, ],</div><div class="line">    [2, 6, ],</div><div class="line">    [3, 7, ],</div><div class="line">]</div><div class="line">slices = tf.data.Dataset.from_tensor_slices(datasets)</div><div class="line">next_item = slices.make_one_shot_iterator().get_next()</div><div class="line">with tf.Session() as sess:</div><div class="line">    while True:</div><div class="line">        try:</div><div class="line">            print(sess.run(next_item))</div><div class="line">        except tf.errors.OutOfRangeError:</div><div class="line">            break</div><div class="line"></div><div class="line"># Output:</div><div class="line"># [0 4]</div><div class="line"># [1 5]</div><div class="line"># [2 6]</div><div class="line"># [3 7]</div></pre></td></tr></table></figure>
<p>其核心思想就是将原始数据包装成 <code>Dataset</code>，然后通过内置的迭代器对数据进行读取。</p>
<h2 id="5-分层"><a href="#5-分层" class="headerlink" title="5 分层"></a>5 分层</h2><p>在复杂模型计算的时候，参数控制就显得尤为重要了，规范化图是一种必要手段。TensorFlow 提供了一种分层机制，可以在某一个或者某一些计算节点对数据进行统一操作，比如修改参数，激活值等等，同时也方便对模型运行状态的检测。</p>
<h3 id="5-1-创建一个-Dense-层"><a href="#5-1-创建一个-Dense-层" class="headerlink" title="5.1 创建一个 Dense 层"></a>5.1 创建一个 <code>Dense</code> 层</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">x = tf.placeholder(tf.float32, shape=[None, 3])</div><div class="line">linear_model = tf.layers.Dense(units=1)</div><div class="line">y = linear_model(x)</div></pre></td></tr></table></figure>
<h3 id="5-2-初始化"><a href="#5-2-初始化" class="headerlink" title="5.2 初始化"></a>5.2 初始化</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">with tf.Session() as sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div></pre></td></tr></table></figure>
<p>注意这里初始话的是全局变量。</p>
<h3 id="5-3-执行"><a href="#5-3-执行" class="headerlink" title="5.3 执行"></a>5.3 执行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">print(sess.run(y, &#123;x: [[1, 2, 3],[4, 5, 6]]&#125;))</div></pre></td></tr></table></figure>
<h2 id="6-特征列"><a href="#6-特征列" class="headerlink" title="6 特征列"></a>6 特征列</h2><p>特征列将原本的非数字化的特征提取成数字化的特征。最典型的一种特征提取就是采用独热（one-hot）编码表示的分类。</p>
<p>数据特征处理</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">features = &#123;&apos;sales&apos;: [[5], [10], [8], [10]], </div><div class="line">			&apos;department&apos;: [&apos;sports&apos;, &apos;sports&apos;, &apos;gardening&apos;, &apos;mathematics&apos;]&#125;</div><div class="line">department_column = tf.feature_column.categorical_column_with_vocabulary_list(</div><div class="line">						&apos;department&apos;, [&apos;sports&apos;, &apos;gardening&apos;, &apos;mathematics&apos;])</div><div class="line">department_column = tf.feature_column.indicator_column(department_column)</div><div class="line">columns = [tf.feature_column.numeric_column(&apos;sales&apos;), department_column]</div><div class="line">inputs = tf.feature_column.input_layer(features, columns)</div></pre></td></tr></table></figure>
<p>初始化操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">var_init = tf.global_variables_initializer()</div><div class="line">table_init = tf.tables_initializer()</div><div class="line">sess = tf.Session()</div><div class="line">sess.run((var_init, table_init))</div></pre></td></tr></table></figure>
<p>抽象化的数字特征</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">print(sess.run(inputs))</div><div class="line"></div><div class="line"># Output:</div><div class="line"># [[ 1.  0.  0.  5.]</div><div class="line">#  [ 1.  0.  0. 10.]</div><div class="line">#  [ 0.  1.  0.  8.]</div><div class="line">#  [ 0.  0.  1. 10.]]</div></pre></td></tr></table></figure>
<h2 id="7-训练"><a href="#7-训练" class="headerlink" title="7 训练"></a>7 训练</h2><p>在熟悉基本操作之后，通过一个线性规划的小例子来了解机器学习的训练过程。</p>
<h3 id="7-1-准备数据"><a href="#7-1-准备数据" class="headerlink" title="7.1 准备数据"></a>7.1 准备数据</h3><p>准备符合线性拟合条件的数据：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">x = tf.constant([[1], [2], [3], [4]], dtype=tf.float32)</div><div class="line">y_true = tf.constant([[0], [-1], [-2], [-3]], dtype=tf.float32)</div></pre></td></tr></table></figure>
<h3 id="7-2-定义模型"><a href="#7-2-定义模型" class="headerlink" title="7.2 定义模型"></a>7.2 定义模型</h3><p>定义一个单一输入的线性模型：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">linear_model = tf.layers.Dense(units=1)</div><div class="line"></div><div class="line">y_pred = linear_model(x)</div></pre></td></tr></table></figure>
<h3 id="7-3-损失函数"><a href="#7-3-损失函数" class="headerlink" title="7.3 损失函数"></a>7.3 损失函数</h3><p>在训练过程中，我们需要对模型进行评估，选取损失函数对模型预测至关重要。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">loss = tf.losses.mean_squared_error(labels=y_true, predictions=y_pred)</div></pre></td></tr></table></figure>
<h3 id="7-4-训练"><a href="#7-4-训练" class="headerlink" title="7.4 训练"></a>7.4 训练</h3><p>训练是一个对模型参数更新迭代的过程，为了模型评估效果最优，需要时刻观测模型参数并进行相应的调参设置。</p>
<p>选取优化函数和步长：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">optimizer = tf.train.GradientDescentOptimizer(0.01)</div><div class="line">train = optimizer.minimize(loss)</div></pre></td></tr></table></figure>
<p>迭代训练：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">sess = tf.Session()</div><div class="line">sess.run(tf.global_variables_initializer())</div><div class="line"></div><div class="line">for i in range(100):</div><div class="line">    _, loss_value = sess.run((train, loss))</div><div class="line">    if i % 10 is 0:</div><div class="line">        print(loss_value)</div><div class="line"></div><div class="line"># Output:</div><div class="line"># 0.23138471</div><div class="line"># 0.14980102</div><div class="line"># 0.1393195</div><div class="line"># 0.13116479</div><div class="line"># 0.12352913</div><div class="line"># 0.11633903</div><div class="line"># 0.1095675</div><div class="line"># 0.103190124</div><div class="line"># 0.0971839</div><div class="line"># 0.09152731</div></pre></td></tr></table></figure>
<p>损失函数值在一步一步缩小，说明我们选取的参数是比较合理的。</p>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;写在前面：此文的行文风格参考&lt;a href=&quot;http://fitzeng.org/2018/03/13/GPTFInstallation/&quot;&gt;此文&lt;/a&gt;，主线为主，不会过多介绍细枝末节。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="TensorFlow" scheme="http://fitzeng.org/tags/TensorFlow/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>TensorFlow 安装</title>
    <link href="http://fitzeng.org/2018/03/13/GPTFInstallation/"/>
    <id>http://fitzeng.org/2018/03/13/GPTFInstallation/</id>
    <published>2018-03-13T03:27:00.000Z</published>
    <updated>2018-03-19T09:07:30.023Z</updated>
    
    <content type="html"><![CDATA[<blockquote>
<p>写在前面：出于某些目的，此文的行文风格将以主线为主，不会介绍细枝末节。此系列主要围绕 TensorFlow 官网介绍进行叙述性实践介绍。</p>
</blockquote>
<a id="more"></a>
<p>注：TensorFlow 目前所支持的语言有 C、Go、Java、Python。这里将以 Python 3.X 作为示范语言。</p>
<h2 id="1-MacOS-X"><a href="#1-MacOS-X" class="headerlink" title="1 MacOS X"></a>1 MacOS X</h2><p>在 MacOS X 上，官方介绍了四种安装方式，需要注意的是 1.2 版本之后的 TensorFlow 不再提供 GPU 支持。</p>
<h3 id="1-1-通过虚拟环境安装"><a href="#1-1-通过虚拟环境安装" class="headerlink" title="1.1 通过虚拟环境安装"></a>1.1 通过虚拟环境安装</h3><p>这是一种官方建议的安装方式，虚拟环境是独立于主机上的其他环境，可以进行纯粹的机器学习方面知识的学习。</p>
<ol>
<li>开启终端。</li>
<li><p>安装 pip 包管理工具和 Virtualenv。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo easy_install pip</div><div class="line">pip install --upgrade virtualenv</div></pre></td></tr></table></figure>
</li>
<li><p>创建虚拟环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">virtualenv --system-site-packages -p python3</div></pre></td></tr></table></figure>
</li>
<li><p>激活虚拟环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> targetDirectory</div><div class="line"><span class="built_in">source</span> ./bin/activate</div></pre></td></tr></table></figure>
<p> 如果看到 <code>(targetDirectory)$</code> 说明激活成功。</p>
</li>
<li><p>确认安装的 pip 版本高于 8.1。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(targetDirectory)$ easy_install -U pip</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow 依赖包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(targetDirectory)$ pip3 install --upgrade tensorflow</div></pre></td></tr></table></figure>
</li>
<li><p>如果第六步安装失败可以通过 tfBinaryURL 下载安装。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl</div></pre></td></tr></table></figure>
</li>
</ol>
<p>环境安装好后可以使用 <code>import tensorflow</code> 进行检测环境是否可用。使用命令 <code>deactivate</code> 退出虚拟环境，使用 <code>rm -r ~/tensorflow</code> 删除虚拟环境。</p>
<h3 id="1-2-通过-pip-安装"><a href="#1-2-通过-pip-安装" class="headerlink" title="1.2 通过 pip 安装"></a>1.2 通过 pip 安装</h3><p>这种方式安装要求 Python 3.3+。</p>
<ol>
<li><p>安装初始包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo easy_install --upgrade pip</div><div class="line">sudo easy_install --upgrade six</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install tensorflow</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow 相关包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl</div></pre></td></tr></table></figure>
</li>
</ol>
<p>这里卸载环境直接使用 <code>pip3 uninstall tensorflow</code>。</p>
<h3 id="1-3-通过-Docker-安装"><a href="#1-3-通过-Docker-安装" class="headerlink" title="1.3 通过 Docker 安装"></a>1.3 通过 Docker 安装</h3><p>使用 Docker 安装的前提是主机已经好了 Docker。</p>
<ol>
<li><p>启动 Docker 容器。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -it -p hostPort:containerPort TensorFlowImage</div></pre></td></tr></table></figure>
<p> 其中镜像可以使用 DockerHub 提供的 <code>gcr.io/tensorflow/tensorflow</code>。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker run -it gcr.io/tensorflow/tensorflow bash</div><div class="line">docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow</div></pre></td></tr></table></figure>
<p> 第一种启动直接进入容器的 Bash 交互界面，第二种进行了端口映射，可以在主机的交互界面进行操作。</p>
</li>
</ol>
<p>由于 Docker 本就是将环境打包共享的工具，所以只需要一条命令就可以把前辈配好的环境下载下来直接使用了。</p>
<h3 id="1-4-通过-Anaconda-安装"><a href="#1-4-通过-Anaconda-安装" class="headerlink" title="1.4 通过 Anaconda 安装"></a>1.4 通过 Anaconda 安装</h3><p>Anaconda 是非官方支持的，但是在社区的努力下能过在 Anaconda 下使用 TensorFlow 进行机器学习。</p>
<ol>
<li>安装 Anaconda。<br> 官网给出了安装包，只需要下载对应操作系统的对应版本就可以。</li>
<li><p>创建 TensorFlow 环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda create -n tensorflow pip python=3.X</div></pre></td></tr></table></figure>
<p> 如果 <code>conda</code> 不能使用可以自行手动配置环境变量。</p>
</li>
<li><p>激活 conda 环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">source</span> activate tensorflow</div><div class="line">(targetDirectory)$</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow 相关包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install --upgrade https://storage.googleapis.com/tensorflow/mac/cpu/tensorflow-1.6.0-py3-none-any.whl</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="1-5-环境验证"><a href="#1-5-环境验证" class="headerlink" title="1.5 环境验证"></a>1.5 环境验证</h3><ol>
<li>进入终端工具，激活虚拟环境。使用 Docker 直接运行 <code>docker run -it gcr.io/tensorflow/tensorflow bash</code>。</li>
<li>进入 Python 交互环境。</li>
<li><p>编写验证程序。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">import tensorflow as tf</div><div class="line">hello = tf.constant(<span class="string">'Hello, TensorFlow!'</span>)</div><div class="line">sess = tf.Session()</div><div class="line"><span class="built_in">print</span>(sess.run(hello))</div></pre></td></tr></table></figure>
<p> 如果输出 <code>Hello, TensorFlow!</code> 则说明环境配置成功。</p>
</li>
</ol>
<h2 id="2-Ubuntu"><a href="#2-Ubuntu" class="headerlink" title="2 Ubuntu"></a>2 Ubuntu</h2><p>这里官方是直接使用 Ubuntu 作为示例，对于其它 Linux 发行版都可以参考在 Ubuntu 上的安装过程。在 Ubuntu 上提供了 CPU 和 GPU 两个版本，这里统一使用 CPU 版本，下文不再重述。</p>
<h3 id="2-1-通过虚拟环境安装"><a href="#2-1-通过虚拟环境安装" class="headerlink" title="2.1 通过虚拟环境安装"></a>2.1 通过虚拟环境安装</h3><p>这种方式和 MacOS X 差不多，只是命令有些差异。</p>
<ol>
<li>开启终端。</li>
<li><p>安装 pip 包管理工具和 Virtualenv。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install python3-pip python3-dev python-virtualenv</div></pre></td></tr></table></figure>
</li>
<li><p>创建虚拟环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">virtualenv --system-site-packages -p python3 targetDirectory</div></pre></td></tr></table></figure>
<p> 默认 <code>targetDirectory</code> 为 <code>targetDirectory</code></p>
</li>
<li><p>激活虚拟环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">source</span> ~/tensorflow/bin/activate</div></pre></td></tr></table></figure>
<p> 如果看到 <code>(tensorflow)$</code> 说明激活成功。</p>
</li>
<li><p>确认安装的 pip 版本高于 8.1。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(targetDirectory)$ easy_install -U pip</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow 依赖包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">(targetDirectory)$ pip3 install --upgrade tensorflow</div></pre></td></tr></table></figure>
</li>
<li><p>如果第六步安装失败可以通过 tfBinaryURL 下载安装。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.6.0-cp34-cp34m-linux_x86_64.whl</div></pre></td></tr></table></figure>
</li>
</ol>
<p>环境安装好后可以使用 <code>import tensorflow</code> 进行检测环境是否可用。使用命令 <code>deactivate</code> 退出虚拟环境，使用 <code>rm -r ~/tensorflow</code> 删除虚拟环境。</p>
<h3 id="2-2-通过-pip-安装"><a href="#2-2-通过-pip-安装" class="headerlink" title="2.2 通过 pip 安装"></a>2.2 通过 pip 安装</h3><p>这种方式安装要求 Python 3.3+。</p>
<ol>
<li><p>安装初始包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install python3-pip python3-dev</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install tensorflow</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow 相关包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.6.0-cp34-cp34m-linux_x86_64.whl</div></pre></td></tr></table></figure>
</li>
</ol>
<p>这里卸载环境直接使用 <code>pip3 uninstall tensorflow</code>。</p>
<h3 id="2-3-通过-Docker-安装"><a href="#2-3-通过-Docker-安装" class="headerlink" title="2.3 通过 Docker 安装"></a>2.3 通过 Docker 安装</h3><p>使用 Docker 安装的前提是主机已经好了 Docker。</p>
<ol>
<li><p>启动 Docker 容器。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">docker run -it -p hostPort:containerPort TensorFlowCPUImage</div></pre></td></tr></table></figure>
<p> 其中镜像可以使用 DockerHub 提供的 <code>gcr.io/tensorflow/tensorflow</code>。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">docker run -it gcr.io/tensorflow/tensorflow bash</div><div class="line">docker run -it -p 8888:8888 gcr.io/tensorflow/tensorflow</div></pre></td></tr></table></figure>
</li>
</ol>
<p>这里基本和 MacOS X 步骤差不多，这也是使用 Docker 的优势。</p>
<h3 id="2-4-通过-Anaconda-安装"><a href="#2-4-通过-Anaconda-安装" class="headerlink" title="2.4 通过 Anaconda 安装"></a>2.4 通过 Anaconda 安装</h3><ol>
<li>安装 Anaconda。<br> 下载官网给出了安装包进行安装。</li>
<li><p>创建 TensorFlow 环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda create -n tensorflow pip python=3.X</div></pre></td></tr></table></figure>
<p> 如果 <code>conda</code> 不能使用可以自行手动配置环境变量。</p>
</li>
<li><p>激活 conda 环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">source</span> activate tensorflow</div><div class="line">(targetDirectory)$</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow 相关包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install --ignore-installed --upgrade https://storage.googleapis.com/tensorflow/linux/cpu/tensorflow-1.6.0-cp34-cp34m-linux_x86_64.whl</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="2-5-环境验证"><a href="#2-5-环境验证" class="headerlink" title="2.5 环境验证"></a>2.5 环境验证</h3><p>请参考 MacOS X 上的验证方式，这里不赘述。</p>
<h2 id="3-Windows"><a href="#3-Windows" class="headerlink" title="3 Windows"></a>3 Windows</h2><p>在 Windows 上安装有以下几种方式：</p>
<h3 id="3-1-通过-pip-安装"><a href="#3-1-通过-pip-安装" class="headerlink" title="3.1 通过 pip 安装"></a>3.1 通过 pip 安装</h3><p>这种方式安装要求 Python 3.3+。</p>
<ol>
<li>安装初始包。<br> TensorFlow 在 Windows 上 只支持 Python 3.5.X 64-bit 和 Python 3.6.X 64-bit。所以需要选择其一进行安装。</li>
<li>安装 TensorFlow。 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip3 install --upgrade tensorflow</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-2-通过-Anaconda-安装"><a href="#3-2-通过-Anaconda-安装" class="headerlink" title="3.2 通过 Anaconda 安装"></a>3.2 通过 Anaconda 安装</h3><ol>
<li>安装 Anaconda。<br> 下载官网给出了安装包进行安装。</li>
<li><p>创建 TensorFlow 环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda create -n tensorflow pip python=3.5</div></pre></td></tr></table></figure>
<p> 如果 <code>conda</code> 不能使用可以自行手动配置环境变量。</p>
</li>
<li><p>激活 conda 环境。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">activate tensorflow</div><div class="line">(targetDirectory)$</div></pre></td></tr></table></figure>
</li>
<li><p>安装 TensorFlow 相关包。</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install --ignore-installed --upgrade tensorflow</div></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="3-3-环境验证"><a href="#3-3-环境验证" class="headerlink" title="3.3 环境验证"></a>3.3 环境验证</h3><p>请参考 MacOS X 上的验证方式，这里不赘述。</p>
<h2 id="4-从源码构建"><a href="#4-从源码构建" class="headerlink" title="4 从源码构建"></a>4 从源码构建</h2><p>官方提供了源码和二进制文件，并且给出了详细的步骤去编译安装，这里给出最新源码配置的参考信息：</p>
<h3 id="4-1-Linux"><a href="#4-1-Linux" class="headerlink" title="4.1 Linux"></a>4.1 Linux</h3><table>
<thead>
<tr>
<th style="text-align:center">Version</th>
<th style="text-align:center">CPU/GPU</th>
<th style="text-align:center">Python Version</th>
<th style="text-align:center">Compiler</th>
<th style="text-align:center">Build Tools</th>
<th style="text-align:center">cuDNN</th>
<th style="text-align:center">CUDA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">tensorflow-1.6.0</td>
<td style="text-align:center">CPU</td>
<td style="text-align:center">2.7, 3.3-3.6</td>
<td style="text-align:center">GCC 4.8</td>
<td style="text-align:center">Bazel 0.9.0</td>
<td style="text-align:center">N/A</td>
<td style="text-align:center">N/A</td>
</tr>
<tr>
<td style="text-align:center">tensorflow_gpu-1.6.0</td>
<td style="text-align:center">GPU</td>
<td style="text-align:center">2.7, 3.3-3.6</td>
<td style="text-align:center">GCC 4.8</td>
<td style="text-align:center">Bazel 0.9.0</td>
<td style="text-align:center">7</td>
<td style="text-align:center">9</td>
</tr>
</tbody>
</table>
<h3 id="4-2-Mac"><a href="#4-2-Mac" class="headerlink" title="4.2 Mac"></a>4.2 Mac</h3><table>
<thead>
<tr>
<th style="text-align:center">Version</th>
<th style="text-align:center">CPU/GPU</th>
<th style="text-align:center">Python Version</th>
<th style="text-align:center">Compiler</th>
<th style="text-align:center">Build Tools</th>
<th style="text-align:center">cuDNN</th>
<th style="text-align:center">CUDA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">tensorflow-1.6.0</td>
<td style="text-align:center">CPU</td>
<td style="text-align:center">2.7, 3.3-3.6</td>
<td style="text-align:center">Clang from xcode</td>
<td style="text-align:center">Bazel 0.8.1</td>
<td style="text-align:center">N/A</td>
<td style="text-align:center">N/A</td>
</tr>
</tbody>
</table>
<h3 id="4-3-Windows"><a href="#4-3-Windows" class="headerlink" title="4.3 Windows"></a>4.3 Windows</h3><table>
<thead>
<tr>
<th style="text-align:center">Version</th>
<th style="text-align:center">CPU/GPU</th>
<th style="text-align:center">Python Version</th>
<th style="text-align:center">Compiler</th>
<th style="text-align:center">Build Tools</th>
<th style="text-align:center">cuDNN</th>
<th style="text-align:center">CUDA</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">tensorflow-1.6.0</td>
<td style="text-align:center">CPU</td>
<td style="text-align:center">3.5-3.6</td>
<td style="text-align:center">MSVC 2015 update 3</td>
<td style="text-align:center">Cmake v3.6.3</td>
<td style="text-align:center">N/A</td>
<td style="text-align:center">N/A</td>
</tr>
<tr>
<td style="text-align:center">tensorflow_gpu-1.6.0</td>
<td style="text-align:center">GPU</td>
<td style="text-align:center">3.5-3.6</td>
<td style="text-align:center">MSVC 2015 update 3</td>
<td style="text-align:center">Cmake v3.6.3</td>
<td style="text-align:center">7</td>
<td style="text-align:center">9</td>
</tr>
</tbody>
</table>
]]></content>
    
    <summary type="html">
    
      &lt;blockquote&gt;
&lt;p&gt;写在前面：出于某些目的，此文的行文风格将以主线为主，不会介绍细枝末节。此系列主要围绕 TensorFlow 官网介绍进行叙述性实践介绍。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="TensorFlow" scheme="http://fitzeng.org/tags/TensorFlow/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>使用反向传播算法训练多层神经网络的原理【译】</title>
    <link href="http://fitzeng.org/2018/02/25/MLTBackpropagation/"/>
    <id>http://fitzeng.org/2018/02/25/MLTBackpropagation/</id>
    <published>2018-02-25T03:27:00.000Z</published>
    <updated>2018-02-25T09:05:10.704Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：之前写了一篇<a href="http://fitzeng.org/2018/02/19/MLNeuralNetwork/">机器学习基本算法之神经网络</a>里面涉及了一个重要算法，今天看到一篇关于这个算法的图示解释，所以打算翻译一下作为以后学习的一个参考。这是一篇关于反向传播算法的译文，译自 <a href="http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html" target="_blank" rel="external">Principles of training multi-layer neural network using backpropagation</a>。里面通过通俗易懂的图示方式介绍了神经网络中的前向和后向传播的整个过程。如果要深究其中原理和进行数学推导还是要参考其他资料，这篇文章中不涉及推理证明。图片来自原文，如果有侵权我会自行删除。</p>
</blockquote>
<a id="more"></a>
<p>这篇文章描述了采用反向传播算法的多层神经网络的训练过程。为了说明这个过程，我们采用三层神经网络，两个输入一个输出作为示例。如下图所示：</p>
<p><img src="/2018/02/25/MLTBackpropagation/img01.gif" alt=""></p>
<p>每个神经元由两个单元组成。第一个单元是权重系数和输入型号的乘积和。第二个单元是实现非线性功能，这种功能叫做神经元激活。信号 \( e \) 是加法器输出信号，\( y=f(e) \) 是非线性元素的输出信号。信号 \( y \) 也是神经元的输出信号。</p>
<p><img src="/2018/02/25/MLTBackpropagation/img01b.gif" alt=""></p>
<p>为了训练神经网络，我们需要训练数据集。训练数据集是由输入信号（\( x_1 , x_2 \)）和相应的正确目标（期望输出） \( z \)组成的。训练网络是一个迭代的过程。在每一次迭代中，节点的权重系数都会被来自训练数据集中的新的数据修改。修改是通过下面描述的算法计算的：每次训练步骤开始都是强制使用来自训练集的数据作为输入数据。这个步骤之后，我们就能确定在每一个网络层的每一个神经元的输出信号的值。下面的图片显示了信号是怎么通过网络传播的，信号 \( w_{(xm)n} \) 代表在网络输入 \( x_m \) 和在输入层的神经元 \( n \) 之间连接的权重。信号 \( y_n \) 代表神经元 \( n \) 的输出信号。</p>
<p><img src="/2018/02/25/MLTBackpropagation/img02.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img03.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img04.gif" alt=""></p>
<p>信号通过隐藏层传播。信号 \( w_{mn} \) 代表在输出神经元 \( m \) 和下一层的输入神经元 \( n \) 之间的连接权重。</p>
<p><img src="/2018/02/25/MLTBackpropagation/img05.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img06.gif" alt=""></p>
<p>信号通过输出层的传播过程。</p>
<p><img src="/2018/02/25/MLTBackpropagation/img07.gif" alt=""></p>
<p>在接下来的算法步骤中，把网络输出信号 \( y \) 和在训练数据集中的期望输出值（目标）比较。它们之间的差称为输出层神经元的误差信号 \( d \)。</p>
<p><img src="/2018/02/25/MLTBackpropagation/img08.gif" alt=""></p>
<p>直接计算内部神经元的误差信号是不可能的，因为这些神经元的输出值是不知道的。多年来，训练多层神经网络的方法不为人所知。只有在。只有在八十年代中期，反向传播算法才被制定出来。这个思想是把误差信号 \( d \) （在单层中所计算的）往回传到各个神经元，输出神经元视为输入。</p>
<p><img src="/2018/02/25/MLTBackpropagation/img09.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img10.gif" alt=""></p>
<p>权重系数 \( w_{mn} \) 用来传播误差时和它在计算输出值是类似。只有这个数据流的方向是改变的（信号一个接一个地从输出传播到输入）。这个技术引用于所有的网络层，如果传播误差来自于它们添加的少数几个神经元。看下面例子：</p>
<p><img src="/2018/02/25/MLTBackpropagation/img11.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img12.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img13.gif" alt=""></p>
<p>当每一个神经元的误差信号被计算，每一个神经元的权重系数可能被修改。在下面公式，\( \frac{df(e)}{de} \) 代表神经元激活函数的导数（权重被修改）。</p>
<p><img src="/2018/02/25/MLTBackpropagation/img14.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img15.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img16.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img17.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img18.gif" alt=""><br><img src="/2018/02/25/MLTBackpropagation/img19.gif" alt=""></p>
<p>系数 \( h \) 影响网络训练速度。选择参数也是有技术含量的。第一个方法是开始训练时使用较大的参数。当权重系数被确定参数可以逐渐减小。第二个稍微复杂一点。在开始时使用较小的参数去训练，在训练过程中，随着训练趋势变好增加参数的值，最后阶段再减小。以小参数值开始的训练过程可以确定权重系数符号。</p>
<p>参考：<br>References<br>Ryszard Tadeusiewcz “Sieci neuronowe”, Kraków 1992</p>
<p>译者注：里面有一个误差参数 \( d \) 和学习系数 \( h \) 应该是分别对应图中的 \( \delta \) 和 \( \eta \)。可能是作者不方便编写从而使用  \( d \) 和  \( h \) 做取舍吧。</p>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：之前写了一篇&lt;a href=&quot;http://fitzeng.org/2018/02/19/MLNeuralNetwork/&quot;&gt;机器学习基本算法之神经网络&lt;/a&gt;里面涉及了一个重要算法，今天看到一篇关于这个算法的图示解释，所以打算翻译一下作为以后学习的一个参考。这是一篇关于反向传播算法的译文，译自 &lt;a href=&quot;http://galaxy.agh.edu.pl/~vlsi/AI/backp_t_en/backprop.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Principles of training multi-layer neural network using backpropagation&lt;/a&gt;。里面通过通俗易懂的图示方式介绍了神经网络中的前向和后向传播的整个过程。如果要深究其中原理和进行数学推导还是要参考其他资料，这篇文章中不涉及推理证明。图片来自原文，如果有侵权我会自行删除。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Algo" scheme="http://fitzeng.org/tags/Algo/"/>
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基本算法之神经网络</title>
    <link href="http://fitzeng.org/2018/02/19/MLNeuralNetwork/"/>
    <id>http://fitzeng.org/2018/02/19/MLNeuralNetwork/</id>
    <published>2018-02-19T03:27:00.000Z</published>
    <updated>2018-03-14T01:40:32.541Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：我将从一个入门者的视角（水平）将机器学习中的常用算法娓娓道来。自身水平确实有限，如果其中有什么错误的话希望大家指出，避免误导大家。</p>
<p>这个系列已经有两篇前作了（跳票了一篇<a href="http://fitzeng.org/2018/02/18/MLDecisionTree/">决策树</a>😂😂😂），欢迎感兴趣的读者去阅读：</p>
<ul>
<li><a href="http://fitzeng.org/2018/02/11/MLLinearRegression/">机器学习基本算法之线性回归</a></li>
<li><a href="http://fitzeng.org/2018/02/16/MLLogisticRegression/">机器学习基本算法之逻辑回归</a></li>
</ul>
<p><a href="https://github.com/mk43/machine-learning/tree/master/algorithm/neural-network" target="_blank" rel="external">实验代码</a>-<a href="https://github.com/mk43/machine-learning/tree/master/reference" target="_blank" rel="external">参考书籍</a>-[参考博客详见最后]</p>
</blockquote>
<a id="more"></a>
<h2 id="0-基本介绍"><a href="#0-基本介绍" class="headerlink" title="0 基本介绍"></a>0 基本介绍</h2><h3 id="0-1-为什么会产生神经网络"><a href="#0-1-为什么会产生神经网络" class="headerlink" title="0.1 为什么会产生神经网络"></a>0.1 为什么会产生神经网络</h3><p>至于历史上是如何的大家可以网上查阅，这里讲讲我对于神经网络产生的必然性的一些看法。先回到上一篇文章，可以看到，如果要找一个简单的能用现有函数规则描述的特征边界（比如直线，圆，椭圆，球面……）我们可以直接选取它的特征量，然后对这个模型进行训练，也就是说，对于某种问题有它的某种特定的模型进行运算，但是如果稍微复杂一点的呢？我们该如何做？回到我们对机器学习什么都不知道的时候，在开篇线性回归中，我们希望的是有这样一个黑盒，把数据输入进去，它能把结果给我反馈出来，我不需要知道这个黑盒内部的结构，也就是说，我并不需要去特定的规划如何构建特征量，而是直接将原始数据输进去就好，输出结果符合我们的预期就代表这个模型可以为我们所用。那么，问题来了，我们的预期是怎么产什么的呢？我也不知道，但是我知道，如果给人一张图片，人能够识别这是猫还是狗，那这个人的思维过程不就是类似一个黑盒吗？虽然里面的运行机制肯定都是确定的。那我们是不是可以模仿这个过程，把类似于这种识别问题都通过这一条流水线处理下来，得出的结果应该是我们想要的，至于准确度，那就是之前说的小孩和大人的区别了，所以说后天习得（训练模型）是人成长的关键一步。</p>
<h3 id="0-2-什么是神经网络"><a href="#0-2-什么是神经网络" class="headerlink" title="0.2 什么是神经网络"></a>0.2 什么是神经网络</h3><p>我还不太了解，所以可以看看别人的看法：<a href="https://www.zhihu.com/question/22553761/answer/36429105" target="_blank" rel="external">如何简单形象又有趣地讲解神经网络是什么？</a> </p>
<h3 id="0-3-神经网络能解决什么问题"><a href="#0-3-神经网络能解决什么问题" class="headerlink" title="0.3 神经网络能解决什么问题"></a>0.3 神经网络能解决什么问题</h3><p><a href="http://www.ruanyifeng.com/blog/2017/07/neural-network.html" target="_blank" rel="external">神经网络入门</a><br><a href="https://deeplearning4j.org/cn/neuralnet-overview" target="_blank" rel="external">深度神经网络简介</a></p>
<h2 id="1-初识神经网络"><a href="#1-初识神经网络" class="headerlink" title="1 初识神经网络"></a>1 初识神经网络</h2><p>前面已经对从线性回归到逻辑回归时我们能处理问题的局限性进行分析，个人倾向于认为线性回归更多的是一种连续性的值预测，逻辑回归更多的是一种线性可分的类型划分。而对于非线性可分的问题我们就必须借助其它的手段了。</p>
<h3 id="1-1-从逻辑运算看神经网络"><a href="#1-1-从逻辑运算看神经网络" class="headerlink" title="1.1 从逻辑运算看神经网络"></a>1.1 从逻辑运算看神经网络</h3><p>很多很多基础的例子教学就是从逻辑运算开始的。相信看了前面的一些对神经网络的介绍性文章，你对神经网络应该有一定的感知。</p>
<p><img src="/2018/02/19/MLNeuralNetwork/1_1_01.png" alt=""><br><img src="/2018/02/19/MLNeuralNetwork/1_1_02.png" alt=""><br><img src="/2018/02/19/MLNeuralNetwork/1_1_03.png" alt=""></p>
<p>以上是与或非的图，红色圆圈代表正例（1），黑色三角代表反例（0）。下面我们来把这个转换成神经网络的图。</p>
<p><img src="/2018/02/19/MLNeuralNetwork/1_1_04.png" alt=""></p>
<p>根据真值表，由上面的权重构造的神经网络是能够实现与或非逻辑运算的。其实可以看出神经网络最简单的一种处理对象可以是线性可分的，也就是简单的做一次映射（激活）。</p>
<p>下面我们来看下异或运算如何处理：</p>
<p><img src="/2018/02/19/MLNeuralNetwork/1_1_05.png" alt=""></p>
<p>通过观察你会发下这图是线性不可分的，也就是我们想要通过绘制前面的类似于与或非的操作的神经网络现在是绘制不出来的。因为这个分布不是线性可分，没办法通过一个 \( W \) 变换把 \( X \) 变成单调的，所以仅仅通过一次单调函数 \( f \) 的激活是不可能实现类的划分的。</p>
<p>但是我们可以转变一下思路，既然与或非是线性可分的，那么我们尽力把当前的异或运算转化成与或非就解决了。</p>
<p>$$<br>A\ XOR\ B=(A\land \lnot B) \lor (\lnot A\land B)<br>$$</p>
<p>有了上面的公式就好办了，我们直接通过 \( X \) 构造两个神经元 \( (A\land \lnot B), (\lnot A\land B) \)，然后多这两个神经元进行激活，之后就把这两个神经元进行或操作，激活后所得到的结果就是 \( A\ XOR\ B \)。</p>
<p><img src="/2018/02/19/MLNeuralNetwork/1_1_06.png" alt=""></p>
<p>注意这里对于下标的规定我们不做约束，在下一节具体讲解的时候进行规范化介绍。</p>
<p>既然看样子我们的神经网络可以进行异或操作了，那么我们写一段代码验证一下吧：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">x1 = np.asarray([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line">x2 = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">X = np.row_stack((np.ones(shape=(<span class="number">1</span>, <span class="number">4</span>)), x1, x2))</div><div class="line">print(<span class="string">"X:\n%s"</span> % X)</div><div class="line">y = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</div><div class="line">W1 = np.asarray([[<span class="number">-1</span>, <span class="number">2</span>, <span class="number">-2</span>],</div><div class="line">                 [<span class="number">-1</span>, <span class="number">-2</span>, <span class="number">2</span>]])</div><div class="line">W2 = np.asarray([<span class="number">-1</span>, <span class="number">2</span>, <span class="number">2</span>])</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(input)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.power(np.e, <span class="number">-10</span> * (input)))</div><div class="line"></div><div class="line"></div><div class="line">np.set_printoptions(precision=<span class="number">6</span>, suppress=<span class="keyword">True</span>)</div><div class="line">z1 = np.matmul(W1, X)</div><div class="line">print(<span class="string">"W1*X = z1:\n%s"</span> % z1)</div><div class="line">a1 = np.row_stack((np.ones(shape=(<span class="number">1</span>, <span class="number">4</span>)), sigmoid(z1)))</div><div class="line">print(<span class="string">"sigmoid(z1) = a1:\n%s"</span> % a1)</div><div class="line">z2 = np.matmul(W2, a1)</div><div class="line">print(<span class="string">"W2*a1 = z2:\n%s"</span> % z2)</div><div class="line">a2 = sigmoid(z2)</div><div class="line">print(<span class="string">"------------------------"</span>)</div><div class="line">print(<span class="string">"prediction: %s"</span> % a2)</div><div class="line">print(<span class="string">"target: %s"</span> % y)</div><div class="line">print(<span class="string">"------------------------"</span>)</div><div class="line"></div><div class="line"><span class="comment"># output:</span></div><div class="line"><span class="comment"># X:</span></div><div class="line"><span class="comment"># [[1. 1. 1. 1.]</span></div><div class="line"><span class="comment">#  [0. 0. 1. 1.]</span></div><div class="line"><span class="comment">#  [0. 1. 0. 1.]]</span></div><div class="line"><span class="comment"># W1*X = z1:</span></div><div class="line"><span class="comment"># [[-1. -3.  1. -1.]</span></div><div class="line"><span class="comment">#  [-1.  1. -3. -1.]]</span></div><div class="line"><span class="comment"># sigmoid(z1) = a1:</span></div><div class="line"><span class="comment"># [[1.       1.       1.       1.      ]</span></div><div class="line"><span class="comment">#  [0.000045 0.       0.999955 0.000045]</span></div><div class="line"><span class="comment">#  [0.000045 0.999955 0.       0.000045]]</span></div><div class="line"><span class="comment"># W2*a1 = z2:</span></div><div class="line"><span class="comment"># [-0.999818  0.999909  0.999909 -0.999818]</span></div><div class="line"><span class="comment"># ------------------------</span></div><div class="line"><span class="comment"># prediction: [0.000045 0.999955 0.999955 0.000045]</span></div><div class="line"><span class="comment"># target: [0 1 1 0]</span></div><div class="line"><span class="comment"># ------------------------</span></div></pre></td></tr></table></figure>
<p>可以看到，预测值和目标值一致。那么这里面到底发生了什么？</p>
<p>一开始，我们的特征是 \( [x1; x2] \)。<br>然后经过第一层，激活之后的 \( a1 \) 就是我们转化后的特征 \( [a_1^{(1)}; a_2^{(1)}]=sigmoid(W_1X) \) 前面我们看到了 \( [x1; x2] \) 的图线性不可分，那么以 \( [a_1^{(1)}; a_2^{(1)}] \) 作为特征是否线性可分？看数据和图吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># sigmoid(z1) = a1:</span></div><div class="line"><span class="comment"># [[1.       1.       1.       1.      ]</span></div><div class="line"><span class="comment">#  [0.000045 0.       0.999955 0.000045]</span></div><div class="line"><span class="comment">#  [0.000045 0.999955 0.       0.000045]]</span></div><div class="line"><span class="comment"># target: [0 1 1 0]</span></div></pre></td></tr></table></figure>
<p><img src="/2018/02/19/MLNeuralNetwork/1_1_07.png" alt=""></p>
<p>咦？线性可分啦！然后就直接进行类似线性回归的方法直接求得结果，没必要在寻找其它的转换特征。</p>
<p>那么这里神奇之处就在于矩阵 \( W_1 \)，它能把原来的特征转换成另一类特征，这里需要对矩阵理解比较深刻，我不太懂就不瞎说了。。。可以看看<a href="https://www.bilibili.com/video/av6731067/" target="_blank" rel="external">【官方双语/合集】线性代数的本质 - 系列合集</a> PS: 说实话，我还是好久之前看了一点点，看来得抽点时间仔细看一遍，毕竟机器学习中充满着矩阵。</p>
<p>所以，通过特征转换，可以把线性不可分的问题在另外的特征上线性可分。然后通过激活函数使数据在下一次输入时符合规范化，这就是神经网络的神奇之处。</p>
<h2 id="2-训练神经网络"><a href="#2-训练神经网络" class="headerlink" title="2 训练神经网络"></a>2 训练神经网络</h2><p>前面为了对神经网络“威力”的了解，直接给出了 \( W \)。而我们训练数据就是为了找到合适的 \( W \) 去进行预测，所以接下来我们根据前面逻辑回归的机器学习分析方式对神经网络进行一一分析。</p>
<h3 id="2-1-损失函数"><a href="#2-1-损失函数" class="headerlink" title="2.1 损失函数"></a>2.1 损失函数</h3><p>这里我们完全可以把这个模型定义为一个黑盒，只要看输出和我们的目标值是否匹配，那么这里的损失函数和逻辑回归就是基本一样了。</p>
<p>理解可以作为黑盒理解，计算可就不行了，因为每一个参数都得提供确切的计算过程。接下来就开始吧：</p>
<p><img src="/2018/02/19/MLNeuralNetwork/2_1_01.png" alt=""></p>
<p>先介绍一下命名：一般大写代表矩阵，上标代表层数，小写只带上标代表列向量，如果下标齐全代表某个数值。</p>
<p><img src="/2018/02/19/MLNeuralNetwork/2_1_02.png" alt=""></p>
<p>\( W^{(i)} \)：第 \( i \) 个权重矩阵</p>
<p>$$<br>W^{(1)} =<br>\begin{bmatrix}<br>w_{1,0}^{(1)} &amp; w_{1,1}^{(1)} &amp; \ldots \\<br>w_{2,0}^{(1)} &amp; w_{2,1}^{(1)} &amp; \ldots \\<br>\vdots &amp; \vdots &amp; \ddots<br>\end{bmatrix}<br>bias^{(i)} =<br>\begin{bmatrix}<br>w_{1,0}^{(i)} \\<br>w_{2,0}^{(i)} \\<br>\vdots<br>\end{bmatrix}<br>$$</p>
<p>\( X^{(0)} \)：输入，并且添加一个恒为\( 1 \)的神经元，每一列代表一组数据</p>
<p>$$<br>X^{(0)} =<br>\begin{bmatrix}<br>1 &amp; 1 &amp; 1 &amp; \ldots \\<br>x_{1}^{(1)} &amp; x_{1}^{(2)} &amp; x_{1}^{(3)} &amp;\ldots \\<br>x_{2}^{(1)} &amp; x_{2}^{(2)} &amp; x_{2}^{(3)} &amp; \ldots \\<br>\vdots &amp; \vdots &amp; \vdots &amp; \ddots<br>\end{bmatrix}<br>x^{(i)} =<br>\begin{bmatrix}<br>1 \\<br>x_{1}^{(i)} \\<br>x_{2}^{(i)} \\<br>\vdots<br>\end{bmatrix}<br>$$</p>
<p>\( z^{(i)} \)：训练值，可以把 \( x \) 看做 \( a^{(0)} \)</p>
<p>$$<br>W^{(1)}x =<br>\begin{bmatrix}<br>w_{1,0}^{(1)} &amp; w_{1,1}^{(1)} &amp; w_{1,2}^{(1)} \\<br>w_{2,0}^{(1)} &amp; w_{2,1}^{(1)} &amp; w_{2,2}^{(1)} \\<br>\end{bmatrix}<br>\begin{bmatrix}<br>1 \\<br>x_{1} \\<br>x_{2}<br>\end{bmatrix}<br> =<br>\begin{bmatrix}<br>z_{1}^{(1)} \\<br>z_{2}^{(1)}<br>\end{bmatrix}<br> = z^{(1)} \\<br>W^{(i + 1)}a^{(i)} =<br>\begin{bmatrix}<br>w_{1,0}^{(i + 1)} &amp; w_{1,1}^{(i + 1)} &amp; w_{1,2}^{(i + 1)} \\<br>w_{2,0}^{(i + 1)} &amp; w_{2,1}^{(i + 1)} &amp; w_{2,2}^{(i + 1)} \\<br>\end{bmatrix}<br>\begin{bmatrix}<br>1 \\<br>a_{1}^{(i)} \\<br>a_{2}^{(i)}<br>\end{bmatrix}<br> =<br>\begin{bmatrix}<br>z_{1}^{(i + 1)} \\<br>z_{2}^{(i + 1)}<br>\end{bmatrix}<br> = z^{(i + 1)}<br>$$</p>
<p>\( a^{(i)} \)：激活值，注意在这里激活函数采用 \( f=sigmoid \)</p>
<p>$$<br>a^{(i)} =<br>\begin{bmatrix}<br>1 \\<br>a_{1}^{(i)} \\<br>a_{2}^{(i)}<br>\end{bmatrix}<br> =<br>\begin{bmatrix}<br>1 \\<br>f(z_{1}^{(i)}) \\<br>f(z_{2}^{(i)})<br>\end{bmatrix}<br>$$</p>
<p>那在之前那个异或的例子中，我们可以得出：</p>
<p>$$<br>\begin{aligned}<br>z^{(1)} &amp;= W^{(1)}x \\<br>a^{(1)} &amp;= f(z^{(1)})\ (add\ a_{0}^{(i)}=1) \\<br>z^{(2)} &amp;= W^{(2)}a^{(1)} \\<br>a^{(2)} &amp;= f(z^{(2)}) \\<br>\end{aligned}<br>$$</p>
<p>损失函数 \( J \)：</p>
<p>$$<br>J(W) = - y * log(a^{(2)}) - (1 - y) * log(1 - a^{(2)})<br>$$</p>
<p>我们对于一个输入 \( x_1, x_2 \) 的损失值是 \( J(W) \)。注意这里 \( W \) 看做是一个黑盒的 \( W \)，为了简单这里只采用一维数据</p>
<h3 id="2-2-参数更新：反向传播算法"><a href="#2-2-参数更新：反向传播算法" class="headerlink" title="2.2 参数更新：反向传播算法"></a>2.2 参数更新：反向传播算法</h3><p>如果你之前从来没有接触过反向传播算法，可以这样思考：我们这里的目的是更新 \( W^{(i)} \)，而我们目前熟悉的最优化算法就只有梯度下降算法（PS：看来要去复习一遍数值计算了😂😂😂），那么自然就有：</p>
<p>$$<br>W^{(i)} = W^{(i)} - \alpha\frac{\partial\ J}{\partial\ W^{(i)}}<br>$$</p>
<p>那么</p>
<p>$$<br>W^{(2)} = W^{(2)} - \alpha\frac{\partial\ J}{\partial\ W^{(2)}} \\<br>W^{(1)} = W^{(1)} - \alpha\frac{\partial\ J}{\partial\ W^{(1)}}<br>$$</p>
<p>接下来要用到一大波矩阵求导的知识，这也是我准备最久的地方，因为要手写代码必需每一个细节都得知道，由于这里重点是机器学习，所以我会给出计算的推导过程，但是矩阵求导的原理大家就看参考资料吧。</p>
<p><a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="external">Matrix calculus</a>：主要是了解 <code>Numerator-layout notation</code> 、<code>Denominator-layout notation</code> 的表示方法和 <code>Scalar-by-matrix identities</code>，其它的也推荐瞟一瞟。<br><a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="external">矩阵求导术（上）</a><a href="https://zhuanlan.zhihu.com/p/24863977" target="_blank" rel="external">矩阵求导术（下）</a>看看原理与实操。然后<a href="https://zhuanlan.zhihu.com/p/22473137" target="_blank" rel="external">神经网络反向传播的数学原理</a>也可以看下，里面对逻辑推理部分可能不是很严谨，但是整体思路很适合阅读，对于我们来说可能不需要这么缜密的逻辑推理过程，不过我在下面会把计算过程列出来的，有兴趣的可以阅读。</p>
<p>好了，到这里就当大家有一定求导基础了。这里的 \( \frac{\partial\ J}{\partial\ W^{(i)}} \) 很明显是一个 <code>scalar-by-matrix</code> 那么我们采用迹的形式求解。</p>
<p>$$<br>\frac{\partial\ J}{\partial\ W^{(i)}} = \frac{\partial\ J}{\partial\ z^{(i)}}\frac{\partial\ z^{(i)}}{\partial\ W^{(i)}} \\<br>$$</p>
<p>其中</p>
<p>$$<br>\frac{\partial\ z^{(i)}}{\partial\ W^{(i)}} = {a^{(i - 1)}}^T \\<br>\frac{\partial\ J}{\partial\ z^{(i)}} = \frac{\partial\ J}{\partial\ z^{(i + 1)}}\frac{\partial\ z^{(i + 1)}}{\partial\ a^{(i)}}\frac{\partial\ a^{(i)}}{\partial\ z^{(i)}} = {W^{(i + 1)}}^T\frac{\partial\ J}{\partial\ z^{(i + 1)}}.*f’(z^{(i)})<br>$$</p>
<p>如果你已经知道计算，或者不需要认为不需要计算那么就可以直接跳过证明过程了。</p>
<p>证明：<br>已知</p>
<p>$$<br>\sum_{i,\ j}A_{ij}B_{ij} = tr(A^TB) \\<br>df = \sum_{i,\ j}\frac{\partial f}{\partial X_{ij}}dX_{ij} = tr(\frac{\partial f}{\partial X}^TdX) \\<br>tr(AB) = tr(BA) \\<br>tr(A^T(B .* C)) = tr((A .* B)^TC) \\<br>df(X) = f’(X) .* X<br>$$</p>
<p>对以上公式的证明感兴趣的可以在网上查询，因为……因为我也只是感觉公式对（使用测试用例），严谨证明我也不太会😂😂😂</p>
<p>那么对 \( z^{(i)} = W^{(i)}a^{(i - 1)} \) 微分：</p>
<p>$$<br>\begin{aligned}<br>dz^{(i)} &amp; = dW^{(i)}a^{(i - 1)} \\<br>&amp; = tr(a^{(i - 1)}dW^{(i)}) \\<br>&amp; = tr(({a^{(i - 1)}}^T)^TdW^{(i)}) \\<br>&amp; \Rightarrow \frac{\partial\ z^{(i)}}{\partial\ W^{(i)}} = {a^{(i - 1)}}^T \\<br>\end{aligned}<br>$$</p>
<p>很简单，第一个证明完成，接着证明第二个：</p>
<p>$$<br>\begin{aligned}<br>dJ &amp; = tr({\frac{\partial\ J}{\partial\ z^{(i + 1)}}}^Tdz^{(i + 1)}) \\<br>&amp; = tr({\frac{\partial\ J}{\partial\ z^{(i + 1)}}}^TW^{(i + 1)}da^{(i)}) \\<br>&amp; = tr({\frac{\partial\ J}{\partial\ z^{(i + 1)}}}^TW^{(i + 1)}f’(z^{(i)}) .* dz^{(i)}) \\<br>&amp; = tr([{W^{(i + 1)}}^T{\frac{\partial\ J}{\partial\ z^{(i + 1)}}}]^Tf’(z^{(i)}) .* dz^{(i)})  \\<br>&amp; = tr([{W^{(i + 1)}}^T{\frac{\partial\ J}{\partial\ z^{(i + 1)}}} .* f’(z^{(i)})]^T dz^{(i)})  \\<br>&amp; \Rightarrow \frac{\partial\ J}{\partial\ z^{(i)}} = {W^{(i + 1)}}^T{\frac{\partial\ J}{\partial\ z^{(i + 1)}}} .* f’(z^{(i)})<br>\end{aligned}<br>$$</p>
<p>这就是所有证明过程了。其中只有 \( \frac{\partial\ J}{\partial\ z^{(i + 1)}} \) 是未知的，这个是由 \( J \)的定义决定的。</p>
<p>由前面分析，我们采用的极大似然估计计算误差，所以：</p>
<p>$$<br>\begin{aligned}<br>\frac{\partial\ J}{\partial\ z^{(i + 1)}} &amp; = \frac{\partial\ J}{\partial\ a^{(i + 1)}}\frac{\partial\ a^{(i + 1)}}{\partial\ z^{(i + 1)}} \\<br>&amp; = \frac{\partial\ J}{\partial\ a^{(i + 1)}}f’(z^{(i + 1)}) \\<br>&amp; = \frac{\partial\ (- y * log(a^{(i + 1)}) - (1 - y) * log(1 - a^{(i + 1)}))}{\partial\ a^{(i + 1)}}f’(z^{(i + 1)}) \\<br>&amp; = \frac{k(1 - y(1+e^{-kz^{(i + 1)}}))}{e^{-kz^{(i + 1)}}}<br>\end{aligned}<br>$$</p>
<p>其中 \( f(x) = \frac{1}{1 + e^{-kx}} \)，在调试代码时可以把 \( k \) 值调得合适大，使得 \( sigmoid \) 函数更像阶跃函数，获得激活之后的值在 \( 0 \) 附近的邻域内的跨度更大，训练效果更好（正例更靠近 \( 1 \)，反例更靠近 \( 0 \)）。然后这里的推理偷懒了直接使用前面博文的“维数相容原理”，因为这个看起来还是比较简单的。</p>
<h2 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3 代码实现"></a>3 代码实现</h2><p>这里我会用手写代码实现和使用 \( TensorFlow \)实现两种方式。</p>
<h3 id="3-1-手写代码实现"><a href="#3-1-手写代码实现" class="headerlink" title="3.1 手写代码实现"></a>3.1 手写代码实现</h3><p>这里我按照写代码的思路介绍吧，因为一开始不熟悉直接挑一块代码讲根本不知道为什么要这么做，没有一个循序渐进的概念。</p>
<ul>
<li>首要目的就是训练：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></div><div class="line">    np.set_printoptions(precision=<span class="number">4</span>, suppress=<span class="keyword">True</span>)</div><div class="line">    x1 = np.asarray([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line">    x2 = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">    X = np.row_stack((x1, x2))</div><div class="line">    y = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</div><div class="line">    shape = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]</div><div class="line">    Learning_Rate = <span class="number">0.1</span></div><div class="line">    Training_Times = <span class="number">4000</span></div><div class="line">    W = gradientDescent(X, y, shape, learningrate=Learning_Rate, trainingtimes=Training_Times)</div></pre></td></tr></table></figure>
<p>参数什么的先不用管，要什么就传什么。参数设置现在也不用太在意，关键点在于用 \( shape \) 来控制神经网络的形状，和前面逻辑回归确定拟合函数有点类似的感觉。</p>
<ul>
<li>接着就是梯度下降的具体实现：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, y, shape, learningrate=<span class="number">0.001</span>, trainingtimes=<span class="number">500</span>)</span>:</span></div><div class="line">    W, z, a = [], [], []</div><div class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(len(shape) - <span class="number">1</span>):</div><div class="line">        row = shape[layer + <span class="number">1</span>]</div><div class="line">        col = shape[layer] + <span class="number">1</span></div><div class="line">        W.append(np.random.normal(<span class="number">0</span>, <span class="number">1</span>, row * col).reshape(row, col))</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(trainingtimes):</div><div class="line">        <span class="keyword">for</span> x, j <span class="keyword">in</span> zip(X.T, range(len(X[<span class="number">0</span>]))):</div><div class="line">            z, a = forward(W, np.asarray([x]).T)</div><div class="line">            W = backward(y[j], W, z, a, learningrate)</div><div class="line">    <span class="keyword">return</span> W</div></pre></td></tr></table></figure>
<p>主要就是初始化 \( W \)，然后就对数据进行训练，先前向传播，然后反向传播根更新 \( W \)</p>
<ul>
<li>然后就是定义 \( sigmoid \) 和激活函数，接着进行前向传播：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">k = <span class="number">2</span></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.power(np.e, -k * (x)))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">actication</span><span class="params">(data)</span>:</span></div><div class="line">    <span class="keyword">return</span> sigmoid(data)</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(W, data)</span>:</span></div><div class="line">    z, a = [], []</div><div class="line">    a.append(data)</div><div class="line">    data = np.row_stack(([<span class="number">1</span>], data))</div><div class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> W:</div><div class="line">        z.append(np.matmul(w, data))</div><div class="line">        a.append(actication(z[<span class="number">-1</span>]))</div><div class="line">        data = np.row_stack(([<span class="number">1</span>], a[<span class="number">-1</span>]))</div><div class="line">    <span class="keyword">return</span> z, a</div></pre></td></tr></table></figure>
<p>这里不涉及什么数学知识很简单。</p>
<ul>
<li>最后反向传播，精髓全在这里，我们之前的计算结果全应用在这：</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">backward</span><span class="params">(y, W, z, a, learningrate)</span>:</span></div><div class="line">    length = len(z) + <span class="number">1</span></div><div class="line">    Jtoz = k * (<span class="number">1</span> - y * (<span class="number">1</span> + np.power(np.e, -(k * z[<span class="number">-1</span>])))) / np.power(np.e, -(k * z[<span class="number">-1</span>]))</div><div class="line">    <span class="comment"># print("loss = %s" % (-y * np.log(a[-1]) - (1 - y) * np.log(1 - a[-1])))</span></div><div class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> range(length - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</div><div class="line">        i = layer - length</div><div class="line">        <span class="keyword">if</span> (i != <span class="number">-1</span>):</div><div class="line">            Jtoz = np.matmul(W[i + <span class="number">1</span>][:, <span class="number">1</span>:].T, Jtoz) * k * np.power(np.e, -(k * z[i])) / np.power(</div><div class="line">                <span class="number">1</span> + np.power(np.e, -(k * z[i])), <span class="number">2</span>)</div><div class="line">        W[i] = W[i] - learningrate * np.matmul(Jtoz, np.row_stack(([<span class="number">1</span>], a[i - <span class="number">1</span>])).T)</div><div class="line">    <span class="keyword">return</span> W</div></pre></td></tr></table></figure>
<p>代码写完，下面我们来测试一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">()</span>:</span></div><div class="line">    np.set_printoptions(precision=<span class="number">4</span>, suppress=<span class="keyword">True</span>)</div><div class="line">    x1 = np.asarray([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line">    x2 = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">    X = np.row_stack((x1, x2))</div><div class="line">    y = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>])</div><div class="line">    shape = [<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]</div><div class="line">    Learning_Rate = <span class="number">0.1</span></div><div class="line">    Training_Times = <span class="number">4000</span></div><div class="line">    W = gradientDescent(X, y, shape, learningrate=Learning_Rate, trainingtimes=Training_Times)</div><div class="line"></div><div class="line">    print(W)</div><div class="line">    testData = np.row_stack((np.ones(shape=(<span class="number">1</span>, <span class="number">4</span>)), X))</div><div class="line">    <span class="keyword">for</span> w <span class="keyword">in</span> W:</div><div class="line">        testData = np.matmul(w, testData)</div><div class="line">        testData = np.row_stack((np.ones(shape=(<span class="number">1</span>, <span class="number">4</span>)), actication(testData)))</div><div class="line">    print(testData[<span class="number">1</span>])</div></pre></td></tr></table></figure>
<p>看下输出：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># output1:</span></div><div class="line"><span class="comment"># [array([[-8.3273,  5.5208,  5.4758],</span></div><div class="line"><span class="comment">#        [ 2.7417, -5.944 , -5.9745]]), array([[ 18.5644, -22.4426, -22.4217]])]</span></div><div class="line"><span class="comment"># [0.0005 1.     1.     0.0005]</span></div><div class="line"><span class="comment"># [array([[ 3.0903, -6.3961,  6.928 ],</span></div><div class="line"><span class="comment">#        [ 3.0355,  6.7901, -6.2563]]), array([[ 41.2259, -22.2455, -22.0939]])]</span></div><div class="line"><span class="comment"># [0.0024 1.     1.     0.0021]</span></div><div class="line"><span class="comment"># [array([[ 5.3893,  4.913 , -7.0756],</span></div><div class="line"><span class="comment">#        [ 6.2289, -1.3519, -4.7387]]), array([[  9.8004, -20.0023,  10.2014]])]</span></div><div class="line"><span class="comment"># [0.5    1.     0.4995 0.0002]</span></div></pre></td></tr></table></figure>
<p>有时候训练结果非常好，有时候结果不如人意，当我把 \( W \) 的初始值不随机生成时，情况就有所好转：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">    W.append(np.asarray([[<span class="number">-1</span>, <span class="number">1</span>, <span class="number">-1</span>], [<span class="number">-1</span>, <span class="number">-1</span>, <span class="number">1</span>]]))</div><div class="line">    W.append(np.asarray([[<span class="number">-1</span>, <span class="number">1</span>, <span class="number">1</span>]]))</div><div class="line"></div><div class="line"><span class="comment"># output2:</span></div><div class="line"><span class="comment"># [array([[-2.8868,  5.6614, -5.9766],</span></div><div class="line"><span class="comment">#        [-2.9168, -5.9789,  5.6363]]), array([[-2.1866, 21.2065, 21.1815]])]</span></div><div class="line"><span class="comment"># [0.016  1.     1.     0.0142]</span></div><div class="line"><span class="comment"># [array([[-2.9942,  5.7925, -6.0901],</span></div><div class="line"><span class="comment">#        [-3.0228, -6.0924,  5.7687]]), array([[-3.6425, 22.3914, 22.3658]])]</span></div><div class="line"><span class="comment"># [0.0009 1.     1.     0.0008]</span></div></pre></td></tr></table></figure>
<p>至于为什么会这样，由于我学识尚浅，所以只能猜测，对于之前讲过的梯度下降算法其实依赖于初始值，如果初始值离全局最优比较远，那么不仅收敛时会比较慢，而且很可能收敛到局部最优。其次，我们设置的激活函数 \( k \) 值不能太大，不然会指数增长影响精度，又因为 \( k \) 值比较小，所以在 \( [-1, 1] \) 区间并非像我们理想中的阶跃函数，所以造成在有限次训练下时的数据集中于 \( 0.5 \) 而不是分散在 \( [0, 1] \) 区间的端点处。当然这只是猜测，具体原因如果有知道的小伙伴欢迎告知。</p>
<h3 id="3-2-TensorFlow-实现"><a href="#3-2-TensorFlow-实现" class="headerlink" title="3.2 TensorFlow 实现"></a>3.2 TensorFlow 实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(x)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.power(np.e, <span class="number">-2</span> * (x)))</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_layer</span><span class="params">(inputs, in_size, out_size, activation_function=None, )</span>:</span></div><div class="line">    Weights = tf.Variable(tf.random_normal([in_size, out_size]))</div><div class="line">    biases = tf.Variable(tf.zeros([<span class="number">1</span>, out_size]) + <span class="number">0.1</span>)</div><div class="line">    Wx_plus_b = tf.matmul(inputs, Weights) + biases</div><div class="line">    <span class="keyword">if</span> activation_function <span class="keyword">is</span> <span class="keyword">None</span>:</div><div class="line">        outputs = Wx_plus_b</div><div class="line">    <span class="keyword">else</span>:</div><div class="line">        outputs = activation_function(Wx_plus_b)</div><div class="line">    <span class="keyword">return</span> outputs</div><div class="line"></div><div class="line"></div><div class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</div><div class="line">    x1 = np.asarray([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line">    x2 = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">    X = np.row_stack((x1, x2))</div><div class="line">    y = np.asarray([<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]).reshape(<span class="number">1</span>, <span class="number">4</span>)</div><div class="line">    data_X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">2</span>])</div><div class="line">    data_y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>])</div><div class="line"></div><div class="line">    layer_one = add_layer(data_X, <span class="number">2</span>, <span class="number">2</span>, activation_function=sigmoid)</div><div class="line">    prediction = add_layer(layer_one, <span class="number">2</span>, <span class="number">1</span>, activation_function=sigmoid)</div><div class="line"></div><div class="line">    loss = tf.reduce_mean(tf.reduce_sum(- data_y * tf.log(prediction) - (<span class="number">1</span> - data_y) * tf.log(<span class="number">1</span> - prediction)))</div><div class="line">    train = tf.train.GradientDescentOptimizer(<span class="number">0.1</span>).minimize(loss)</div><div class="line"></div><div class="line">    <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">        sess.run(tf.global_variables_initializer())</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4000</span>):</div><div class="line">            sess.run(train, feed_dict=&#123;data_X: X.T, data_y: y.T&#125;)</div><div class="line">        print(sess.run(prediction, feed_dict=&#123;data_X: X.T, data_y: y.T&#125;))</div><div class="line"></div><div class="line"><span class="comment"># output:</span></div><div class="line"><span class="comment"># [[0.00200064]</span></div><div class="line"><span class="comment">#  [0.9985947 ]</span></div><div class="line"><span class="comment">#  [0.9985983 ]</span></div><div class="line"><span class="comment">#  [0.00144795]]</span></div><div class="line"><span class="comment"># --------------</span></div><div class="line"><span class="comment"># [[0.01765717]</span></div><div class="line"><span class="comment">#  [0.98598236]</span></div><div class="line"><span class="comment">#  [0.98598194]</span></div><div class="line"><span class="comment">#  [0.0207849 ]]</span></div><div class="line"><span class="comment"># --------------</span></div><div class="line"><span class="comment"># [[0.00104381]</span></div><div class="line"><span class="comment">#  [0.9991435 ]</span></div><div class="line"><span class="comment">#  [0.49951136]</span></div><div class="line"><span class="comment">#  [0.5003463 ]]</span></div></pre></td></tr></table></figure>
<p>这里很简单，就是使用 \( add\_layer \) 一层一层添加，并且直接使用 \( TensorFlow \) 给我们提供的优化算法。结果也是十分诡异，时好时坏的训练结果。。。可能，这个例子就是对初始化值要求高吧……当然不排除我的过程有错误😂</p>
<p>这是<a href="https://github.com/mk43/machine-learning/tree/master/algorithm/neural-network" target="_blank" rel="external">实验代码</a>，感兴趣欢迎 Star ^_^</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p>说实话，准备这篇文章花了很多时间，而且还是有些地方理解不够深刻，毕竟这是神经网络开篇，以后了解更多会写更多的文章，有时间就会补充实践代码。然后就是发现数学是越来越来越来越来越重要，发现自己懂得东西太少了，还有好多好多要学习。最后，刚接触机器学习不久，这篇文章难免会有错误的地方，欢迎大家批评指正。</p>
<h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5 参考"></a>5 参考</h2><ul>
<li><a href="https://www.zhihu.com/question/22553761/answer/36429105" target="_blank" rel="external">如何简单形象又有趣地讲解神经网络是什么？</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2017/07/neural-network.html" target="_blank" rel="external">神经网络入门</a></li>
<li><a href="https://deeplearning4j.org/cn/neuralnet-overview" target="_blank" rel="external">深度神经网络简介</a></li>
<li><a href="https://www.bilibili.com/video/av6731067/" target="_blank" rel="external">【官方双语/合集】线性代数的本质 - 系列合集</a></li>
<li><a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="external">Matrix calculus</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/24709748" target="_blank" rel="external">矩阵求导术（上）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/24863977" target="_blank" rel="external">矩阵求导术（下）</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/22473137" target="_blank" rel="external">神经网络反向传播的数学原理</a></li>
</ul>
<p>注意：其中还零零星星参考了其它的博客，不是主要的就没找链接贴上来了，感谢他们的分享，还有知乎某些问题下的回答对我很有启发也不一一给链接了。其中穿插的部分知识在前面两篇文章中给出了参考，这里就不赘述。</p>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：我将从一个入门者的视角（水平）将机器学习中的常用算法娓娓道来。自身水平确实有限，如果其中有什么错误的话希望大家指出，避免误导大家。&lt;/p&gt;
&lt;p&gt;这个系列已经有两篇前作了（跳票了一篇&lt;a href=&quot;http://fitzeng.org/2018/02/18/MLDecisionTree/&quot;&gt;决策树&lt;/a&gt;😂😂😂），欢迎感兴趣的读者去阅读：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://fitzeng.org/2018/02/11/MLLinearRegression/&quot;&gt;机器学习基本算法之线性回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://fitzeng.org/2018/02/16/MLLogisticRegression/&quot;&gt;机器学习基本算法之逻辑回归&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mk43/machine-learning/tree/master/algorithm/neural-network&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;实验代码&lt;/a&gt;-&lt;a href=&quot;https://github.com/mk43/machine-learning/tree/master/reference&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;参考书籍&lt;/a&gt;-[参考博客详见最后]&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
      <category term="NumPy" scheme="http://fitzeng.org/tags/NumPy/"/>
    
      <category term="Matplotlib" scheme="http://fitzeng.org/tags/Matplotlib/"/>
    
      <category term="NeuralNetwork" scheme="http://fitzeng.org/tags/NeuralNetwork/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基本算法之决策树</title>
    <link href="http://fitzeng.org/2018/02/18/MLDecisionTree/"/>
    <id>http://fitzeng.org/2018/02/18/MLDecisionTree/</id>
    <published>2018-02-18T03:27:00.000Z</published>
    <updated>2018-02-19T03:17:09.831Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：我将从一个入门者的视角（水平）将机器学习中的常用算法娓娓道来。自身水平确实有限，如果其中有什么错误的话希望大家指出，避免误导大家。</p>
<p>这个系列已经有两篇前作了，欢迎感兴趣的读者去阅读：</p>
<ul>
<li><a href="http://fitzeng.org/2018/02/11/MLLinearRegression/">机器学习基本算法之线性回归</a></li>
<li><a href="http://fitzeng.org/2018/02/16/MLLogisticRegression/">机器学习基本算法之逻辑回归</a></li>
</ul>
<p><a href="https://github.com/mk43/machine-learning/tree/master/algorithm/logistic-regression" target="_blank" rel="external">实验代码</a>-<a href="https://github.com/mk43/machine-learning/tree/master/reference" target="_blank" rel="external">参考书籍</a>-[参考博客详见最后]</p>
</blockquote>
<a id="more"></a>
<h1 id="gt-TODO"><a href="#gt-TODO" class="headerlink" title="==&gt; TODO"></a>==&gt; TODO</h1><p>这个坑先留着，先介绍神经网络吧，我发现决策树虽然好理解但是和前面的线性回归逻辑回归有点联系不上，为了能够系统地体会机器学习的思考过程并且把前面的知识进行迁移，一步一步深入分析，所以决定上神经网络，为了不犯大错误，所以下一篇我会花点时间多多学习一下，更新可能会有点慢。决策树配合图文好理解但是用文字描述不好怎么写，等有时间我会争取把这篇写出来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">## 0 基本介绍</div><div class="line"></div><div class="line">### 0.1 什么是决策树</div><div class="line"></div><div class="line">### 0.2 决策树能解决什么问题</div></pre></td></tr></table></figure>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：我将从一个入门者的视角（水平）将机器学习中的常用算法娓娓道来。自身水平确实有限，如果其中有什么错误的话希望大家指出，避免误导大家。&lt;/p&gt;
&lt;p&gt;这个系列已经有两篇前作了，欢迎感兴趣的读者去阅读：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://fitzeng.org/2018/02/11/MLLinearRegression/&quot;&gt;机器学习基本算法之线性回归&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://fitzeng.org/2018/02/16/MLLogisticRegression/&quot;&gt;机器学习基本算法之逻辑回归&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mk43/machine-learning/tree/master/algorithm/logistic-regression&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;实验代码&lt;/a&gt;-&lt;a href=&quot;https://github.com/mk43/machine-learning/tree/master/reference&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;参考书籍&lt;/a&gt;-[参考博客详见最后]&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
      <category term="NumPy" scheme="http://fitzeng.org/tags/NumPy/"/>
    
      <category term="Matplotlib" scheme="http://fitzeng.org/tags/Matplotlib/"/>
    
      <category term="DecisionTree" scheme="http://fitzeng.org/tags/DecisionTree/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基本算法之逻辑回归</title>
    <link href="http://fitzeng.org/2018/02/16/MLLogisticRegression/"/>
    <id>http://fitzeng.org/2018/02/16/MLLogisticRegression/</id>
    <published>2018-02-16T03:27:00.000Z</published>
    <updated>2018-02-22T10:35:08.310Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：我将从一个入门者的视角（水平）将机器学习中的常用算法娓娓道来。自身水平确实有限，如果其中有什么错误的话希望大家指出，避免误导大家。然后这是这个系列的第二篇了，对于初学者来说，如果你没看过第一篇，推荐看看<a href="http://fitzeng.org/2018/02/11/MLLinearRegression/">机器学习基本算法之线性回归</a>，里面提及到了很多基础的数学知识和一些的机器学习思维，对于理解这篇文章很有帮助，很多机器学习流程化的东西这里就不在具体介绍为什么这样做了，而是直接解释为什么在逻辑回归中需要使用这样的方式来实现，更加聚焦于逻辑回归实现本身，而非机器学习的流程理解。</p>
<p><a href="https://github.com/mk43/machine-learning/tree/master/algorithm/logistic-regression" target="_blank" rel="external">实验代码</a>-<a href="https://github.com/mk43/machine-learning/tree/master/reference" target="_blank" rel="external">参考书籍</a>-[参考博客详见最后]</p>
</blockquote>
<a id="more"></a>
<h2 id="0-准备"><a href="#0-准备" class="headerlink" title="0 准备"></a>0 准备</h2><h3 id="0-1-环境"><a href="#0-1-环境" class="headerlink" title="0.1 环境"></a>0.1 环境</h3><blockquote>
<p>Python: 3.5</p>
<p>TensorFlow: Anaconda3 envs</p>
<p>IDE: PyCharm 2017.3.3</p>
</blockquote>
<h3 id="0-2-基本认识"><a href="#0-2-基本认识" class="headerlink" title="0.2 基本认识"></a>0.2 基本认识</h3><p>如果你看过前面的线性回归介绍，可以知道，线性回归是通过对一类呈线性分布的数据进行拟合，然后训练一个线性模型对数据进行预测。那么在逻辑回归中，可以暂时理解为处理分类问题。比如给出一个人的经纬度，要判断这个人在哪个国家，如果仅仅以线性回归的思维，我们很难根据经纬度去构造出一个很好的线性模型去预测地区，我们就需要知道的是国家分界线，而这些分界线的获得就是通过逻辑回归。</p>
<h2 id="1-二元分类"><a href="#1-二元分类" class="headerlink" title="1 二元分类"></a>1 二元分类</h2><h3 id="1-1-提出拟合函数-Sigmoid"><a href="#1-1-提出拟合函数-Sigmoid" class="headerlink" title="1.1 提出拟合函数 Sigmoid"></a>1.1 提出拟合函数 <code>Sigmoid</code></h3><p>如果你还没有太理解逻辑回归处理的问题和线性回归比的独特性，我们再来举个小例子(注意：为了简单，这里的数据都是特殊取值的)，来引出今天的主角：</p>
<p><img src="/2018/02/16/MLLogisticRegression/linear2LogisticreGressionGraph.png" alt=""></p>
<p>对于上面这张图，很明显，怎么画直线也不好，总会产生较大的误差。而如果有一个函数能够表示：</p>
<p>$$<br>y=\begin{cases}<br>1,\quad x &gt; 5 \\<br>0,\quad x \leq 5<br>\end{cases}<br>$$</p>
<p>那么就完美符合当前的数据分布，直觉告诉我们有很大可能其对数据的预测能力也比较强。接下来，就是找一个 \( S \) 形函数来作为拟合函数。这里很明显我们的函数不是线性的。这里提出一个叫 \( Sigmoid \) 的函数：</p>
<p>$$<br>y = \frac{1}{1 + e^{-x}}<br>$$</p>
<p><img src="/2018/02/16/MLLogisticRegression/linear2LogisticreGressionGraphAddSigmoid.png" alt=""></p>
<p>从图像中可以看到，这个函数的完美符合我们心中的预期。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</div><div class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</div><div class="line"></div><div class="line">train_X = np.asarray(x)</div><div class="line">train_Y = np.asarray(y)</div><div class="line"></div><div class="line">fig = plt.figure()</div><div class="line">plt.xlim(<span class="number">-1</span>, <span class="number">12</span>)</div><div class="line">plt.ylim(<span class="number">-0.5</span>, <span class="number">1.5</span>)</div><div class="line">plt.scatter(train_X, train_Y)</div><div class="line"></div><div class="line">s_X = np.linspace(<span class="number">-2</span>, <span class="number">12</span>, <span class="number">100</span>)</div><div class="line">s_Y = <span class="number">1</span>/(<span class="number">1</span> + np.power(np.e, <span class="number">-6</span>*(s_X - <span class="number">5</span>)))</div><div class="line">plt.plot(s_X, s_Y)</div><div class="line">plt.savefig(<span class="string">"linear2LogisticreGressionGraphAddSigmoid.png"</span>)</div><div class="line"><span class="comment"># plt.savefig("linear2LogisticreGressionGraph.png")</span></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<h3 id="1-2-扩充分类"><a href="#1-2-扩充分类" class="headerlink" title="1.2 扩充分类"></a>1.2 扩充分类</h3><p>前面已经说了，为了更好体会逻辑回归，所以数据有点特殊和单一，下面我们再来看一组数据：</p>
<p><img src="/2018/02/16/MLLogisticRegression/linear2LogisticreGressionGraphAddScatter.png" alt=""></p>
<p>像这种有明显的聚拢效果的散点图，可以看做是一个分类问题，所以对于这里我们怎么利用前面提到的逻辑回归来处理类似于这种分类问题？</p>
<p>这里思维要稍微转换一下，以前我们以 \( y \) 的值作为输出，这里不是了，这里需要预测的结果是给定 \( (x, y) \) 求出该点是红色的圆形还是蓝色的三角形？所以要找一个形如  \( g(x, y) \)<br>的函数来处理，前面的 \( Sigmoid\)函数可以帮我们分类，只要假设 \( 1 \) 和 \( 0 \) 各代表一种情况，那么得出 \( 1/0 \) 时就可以推断出 \( (x, y) \)现在的情况是红色的圆形还是蓝色的三角形，现在的情况是，如何将 \( g(x, y) \) 预处理的结果与 \( Sigmoid \) 函数联系起来。可能你已经想到了，将 \( g(x, y) \) 的值域当做 \( Sigmoid \) 的定义域。那么给定一个 \( (x,y) \) 我们就能得出一个在区间 \( [0, 1] \) 内的值。这下，函数就能够满足我们的要求了。</p>
<p>模仿前面机器学习的思维，我们对于给定的输入统一用一个输入矩阵 \( X \) 表示 \( (x, y) \) 然后加入权重 \( W \)，则得出函数 \( g(W, X) \)，接下来我们要定义一个模型去区分这种分布？如果你看过代码就知道，在做数据的时候默认就是利用 \( x + y &gt; 7 \) 来分的类，所以自然取的是线性模型作为首选。至于怎么选模型我认为最简洁的表示往往就是最好的，下面看看吴恩达老师的例子体会一下这个过程。</p>
<p><img src="/2018/02/16/MLLogisticRegression/wuenda_1.png" alt=""></p>
<p>既然是线性，那么直接得出 \( g(W, X) = W^TX \) 这里提醒下加 \({}^T \) 是因为我们默认的向量都是列向量。 理解了这里那就好办了，综合我们的 \( Sigmoid \) 可以得出拟合函数：</p>
<p>$$<br>h_W(X) = \frac{1}{1 + e^{-W^TX}}<br>$$</p>
<h3 id="1-3-从直观感受推导损失函数"><a href="#1-3-从直观感受推导损失函数" class="headerlink" title="1.3 从直观感受推导损失函数"></a>1.3 从直观感受推导损失函数</h3><p>既然我们都已经把数据做了处理，所以对于输入是二元还是一元没差了，就是一个 \( h_W(X) \)而已。下面我们根据之前线性函数的例子，找一个适合描述误差的函数。看图：</p>
<p><img src="/2018/02/16/MLLogisticRegression/linear2LogisticreGressionGraphAddSigmoid2.png" alt=""></p>
<p>下面我们先只分析为 \( 1 \) 的点如何通过 \( h_W(x_1) &lt; h_W(x_2) &lt; h_W(x_3) \) 的关系找到一个函数来表达损失。首先可以肯定的是，要对 \( h_W(x_1) \) 的惩罚力度是最大的。因为从图中看出对它的预测最离谱，所以损失函数要是一个递减函数，然后继续分析损失函数是否对于 \( h_W(x_i) \) 的变化呈均匀变化，也就是这个递减函数的递减幅度是设么样的？很明显，\( h_W(x_i) \) 越靠近 \( 0 \)，它的惩罚力度应该和指数增长有点类似。越小一点点，惩罚力度立刻激增，这样对于最后损失函数收敛时的参数拟合结果比较符合大多数数据的情况，不会出现它极端的分类，因为对极端的数据处罚力度实在是太大了。好了，这个函数符合三个特点，<code>递减</code>、<code>导数绝对值递减</code>（递减幅度要减小）和定义域在 \( [0, 1] \)内的跨度必须大，最好是从正无穷到 \( 0 \) 因为如果函数值为 \( 0 \)我们就不要对这个点进行惩罚了，所以肯定是要有 \( 0 \) 这个函数值的。这时你可能想到了，对数函数正好满足情况。所以有如下结果：</p>
<p>$$<br>loss(h_W(x_i), y_i) = -log(h_W(x_i))\quad if\ y_i = 1<br>$$</p>
<p>这里就不贴函数图了，下面大家自己分析一下 \( y = 0 \) 的情况。自己实在想不出来再往下阅读吧，如果直接看缺少思考过程对自己的理解可能有点欠缺。就像很多书很多博客都是直接给公式，虽然勉强也能看懂，但是更多时候是自己记住，对于其中的思考过程没有一点自己的体会和心得。给个图大家自己猜猜：</p>
<p><img src="/2018/02/16/MLLogisticRegression/linear2LogisticreGressionGraphAddSigmoid3.png" alt=""></p>
<p>下面直接给出最后的结果：</p>
<p>$$<br>loss(h_W(x_i), y_i) = - y_i * log(h_W(x_i)) - (1 - y_i) * log(1 - h_W(x_i))<br>$$</p>
<h3 id="1-4-从概率论角度分析损失函数"><a href="#1-4-从概率论角度分析损失函数" class="headerlink" title="1.4 从概率论角度分析损失函数"></a>1.4 从概率论角度分析损失函数</h3><p>还记得前面说的逻辑回归可以看做是分类问题吗？而对于分类问题的预测我们可以看做概率来计算。所以函数 \( h_W(x_i) \) 可以看做，在 \( W \) 下，输入 \( x_i \) 取值为 \( 1 \) 的概率。而我们要做的就是求出一个 \( W \) 是的对于给定的 \( x_i \) 的概率尽可能和 \( y_i \) 符合。这时候你会发现，结果 \( y_i \) 服从伯努利分布，也就是随机变量 \( Y \)的取值只能是 \( 0/1 \)，在 \( h_W(x_i) \) 看做是取 \( 1 \) 的概率的情况下，当这个值大于 \( 0.5 \) 我们认为分类为 \( 1 \)。</p>
<p>好了，把上面的东西都“忘掉”，接下来进入一个概率论的思维模式。假设这一个平面内有无数个点，而我们给出的数据只是一个抽样数据。但是我们不可能把所有的情况都列举出来，所以我们要做的就是通过给出的样本来估测整体分布。其实这也和机器学习需要大量样本训练有关，很明显数据量越大，偶然误差越小，样本的分布越接近整体分布。收回到概率论，我发现容易写着写着就跑偏了😂😂😂。现在我们统计数据的情况是：符合伯努利分布。而这种知道分布的求能够和分布匹配的参数的方法概率论中有一个参数估计方法叫：<code>极大似然估计</code>。如果不懂看一下简短介绍：</p>
<blockquote>
<p>极大似然估计，通俗理解来说，就是利用已知的样本结果信息，反推最具有可能（最大概率）导致这些样本结果出现的模型参数值！<br>换句话说，极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”。</p>
</blockquote>
<p>参考自<a href="https://zhuanlan.zhihu.com/p/26614750" target="_blank" rel="external">@忆臻：一文搞懂极大似然估计</a></p>
<p>依照上面的意思那很明显，我们是把 \( h_W(x_i) = P(y = 1|x_i;W)\) 那自然就有：</p>
<p>$$<br>P(y_i|x_i;W) = {h_W(x_i)}^{y_i} * {(1 - h_W(x_i))}^{1 - y_i}<br>$$</p>
<p>这是单个 \(（x_i, y_i）\)的概率。如果把整个样本看做 \( n \) 次的独立重复实验呢？那发生的概率就是这个的乘积了。这里使用 \( X, Y \) 向量表示整个数据。即：</p>
<p>$$<br>P(Y|X;W) = \prod_{i = 1}^{n}P(y_i|x_i; W) = \prod_{i = 1}^{n}{h_W(x_i)}^{y_i} * {(1 - h_W(x_i))}^{1 - y_i}<br>$$</p>
<p>可以看出这是一个关于 \( W \) 的函数，现在比较玄学的部分来了，我们认为，这个事件既然发生了，那么它发生的概率就应该是所有事件中概率最大的，如果不是那它凭什么发生？哈哈，就是这么讲道(xuan)理(xue)。最大值？那就好办了，直接求导？不行不行，太难了。。。高中最常用的把戏：乘变加，用对数。然后乘以 \( -\frac{1}{n} \) 就编程求最小值了。</p>
<p>$$<br>loss(W) = - \frac{1}{n}log(P(Y|X;W)) = - \frac{1}{n} \sum_{i = 1}^{n}y_i * log(h_W(x_i)) + (1 - y_i) * log(1 - h_W(x_i))<br>$$</p>
<h3 id="1-5-梯度下降求极值"><a href="#1-5-梯度下降求极值" class="headerlink" title="1.5 梯度下降求极值"></a>1.5 梯度下降求极值</h3><p>前面已经给出了损失函数，我们再次利用梯度下降算法更新权重。如果自己手写代码来实现，我们还是要求偏导的😂😂😂</p>
<p>高数的东西，这里偷懒了，字有点难打就直接截图了，毕竟这不是什么难点：</p>
<p><img src="/2018/02/16/MLLogisticRegression/wuenda_2.png" alt=""></p>
<p>那么更新函数就是：</p>
<p>$$<br>w_j := w_j - \frac{\alpha}{n}\sum_{i=1}^{n}(h_W(x_i) - y_i)x_{i_j}<br>$$</p>
<p>注意这里的 \( i, j \)，\( x_i \) 代表的是一个列向量，\( x_{i_j} \) 代表第 \( i \) 列第 \( j \) 个数字。其实仔细想想就可以发现，更新的 \( w_j \)就是一个数，对它求偏导的时候余出来就是和它相乘的那个 \( x_{i_j} \)。由于前面都是从单个点开始研究，为了好理解所以直接使用下标表示，但是在使用线性边界函数的时候要有一个矩阵的思想。这里就不细讲了，后面多元的就必须使用向量形式了。</p>
<h3 id="1-6-手写代码"><a href="#1-6-手写代码" class="headerlink" title="1.6 手写代码"></a>1.6 手写代码</h3><p>好了，基本逻辑已经搞清楚了，就是和线性回归类似，只不过这里处理的是一个线性二分类问题，需要对原始数据处理成线性值然后把线性值映射到 \( sigmoid \) 函数的定义域上，根据 \( sigmoid \) 函数特征我们可以得出数据分类为正 \( (1) \) 的概率大小，然后进行评估。</p>
<p>如果对公式还有什么不懂的话，参考这篇 <a href="http://blog.csdn.net/ligang_csdn/article/details/53838743" target="_blank" rel="external">机器学习–Logistic回归计算过程的推导</a> 里面讲解了如何向量化，如果你对线性代数不熟悉可以自己去看看这篇文章的讲解。下面我只讲解代码，就不对原理重复介绍了。</p>
<p>需要的库和数据，这里为了获取更好的精度最好使用 <code>float64</code>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</div><div class="line"></div><div class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</div><div class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</div><div class="line"></div><div class="line">train_X = np.asarray(np.row_stack((np.ones(shape=(<span class="number">1</span>, len(x))), x)), dtype=np.float64)</div><div class="line">train_Y = np.asarray(y, dtype=np.float64)</div><div class="line">train_W = np.asarray([<span class="number">-1</span>, <span class="number">-1</span>], dtype=np.float64).reshape(<span class="number">1</span>, <span class="number">2</span>)</div></pre></td></tr></table></figure>
<p>定义 \( sigmoid \) 和 \( loss \) 函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.power(np.e, -(X)))</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossfunc</span><span class="params">(X, Y, W)</span>:</span></div><div class="line">    n = len(Y)</div><div class="line">    <span class="keyword">return</span> (<span class="number">-1</span> / n) * np.sum(Y * np.log(sigmoid(np.matmul(W, X))) + (<span class="number">1</span> - Y) * np.log((<span class="number">1</span> - sigmoid(np.matmul(W, X)))))</div></pre></td></tr></table></figure>
<p>实现参数更新（梯度下降算法）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, Y, W, learningrate=<span class="number">0.001</span>, trainingtimes=<span class="number">500</span>)</span>:</span></div><div class="line">    n = len(Y)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(trainingtimes):</div><div class="line">        W = W - (learningrate / n) * np.sum((sigmoid(np.matmul(W, X)) - Y) * X, axis=<span class="number">1</span>)</div></pre></td></tr></table></figure>
<p>这里的一个重点是把之前的分析向量化了，然后注意维度变化就很简单了。其中 \( axis=1 \) 是一个值得注意的点，它代表求某个维度的和。</p>
<p>其实到这里整个算法就算是完成了。下面我们进行可视化，不会的可以参考我之前的文章 <a href="http://fitzeng.org/2018/02/08/MatplotlibGenerateGif/">Matplotlib 保存 GIF 动图</a>，下面给出效果和源代码：</p>
<p><img src="/2018/02/16/MLLogisticRegression/logisticregression.gif" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.animation <span class="keyword">as</span> animation</div><div class="line"></div><div class="line">x = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</div><div class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]</div><div class="line"></div><div class="line">train_X = np.asarray(np.row_stack((np.ones(shape=(<span class="number">1</span>, len(x))), x)), dtype=np.float64)</div><div class="line">train_Y = np.asarray(y, dtype=np.float64)</div><div class="line">train_W = np.asarray([<span class="number">-1</span>, <span class="number">-1</span>], dtype=np.float64).reshape(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">sigmoid</span><span class="params">(X)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="number">1</span> / (<span class="number">1</span> + np.power(np.e, -(X)))</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">lossfunc</span><span class="params">(X, Y, W)</span>:</span></div><div class="line">    n = len(Y)</div><div class="line">    <span class="keyword">return</span> (<span class="number">-1</span> / n) * np.sum(Y * np.log(sigmoid(np.matmul(W, X))) + (<span class="number">1</span> - Y) * np.log((<span class="number">1</span> - sigmoid(np.matmul(W, X)))))</div><div class="line"></div><div class="line"></div><div class="line">Training_Times = <span class="number">100000</span></div><div class="line">Learning_Rate = <span class="number">0.3</span></div><div class="line"></div><div class="line">loss_Trace = []</div><div class="line">w_Trace = []</div><div class="line">b_Trace = []</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(X, Y, W, learningrate=<span class="number">0.001</span>, trainingtimes=<span class="number">500</span>)</span>:</span></div><div class="line">    n = len(Y)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(trainingtimes):</div><div class="line">        W = W - (learningrate / n) * np.sum((sigmoid(np.matmul(W, X)) - Y) * X, axis=<span class="number">1</span>)</div><div class="line">        <span class="comment"># for GIF</span></div><div class="line">        <span class="keyword">if</span> <span class="number">0</span> == i % <span class="number">1000</span> <span class="keyword">or</span> (<span class="number">100</span> &gt; i <span class="keyword">and</span> <span class="number">0</span> == i % <span class="number">2</span>):</div><div class="line">            b_Trace.append(W[<span class="number">0</span>, <span class="number">0</span>])</div><div class="line">            w_Trace.append(W[<span class="number">0</span>, <span class="number">1</span>])</div><div class="line">            loss_Trace.append(lossfunc(X, Y, W))</div><div class="line">    <span class="keyword">return</span> W</div><div class="line"></div><div class="line"></div><div class="line">final_W = gradientDescent(train_X, train_Y, train_W, learningrate=Learning_Rate, trainingtimes=Training_Times)</div><div class="line"></div><div class="line">print(<span class="string">"Final Weight:"</span>, final_W)</div><div class="line">print(<span class="string">"Weight details trace: "</span>, np.asarray([b_Trace, w_Trace]))</div><div class="line">print(<span class="string">"Loss details trace: "</span>, loss_Trace)</div><div class="line"></div><div class="line">fig, ax = plt.subplots()</div><div class="line">ax.scatter(np.asarray(x), np.asarray(y))</div><div class="line">ax.set_title(<span class="string">r'$Fitting\ line$'</span>)</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">update</span><span class="params">(i)</span>:</span></div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        ax.lines.pop(<span class="number">0</span>)</div><div class="line">    <span class="keyword">except</span> Exception:</div><div class="line">        <span class="keyword">pass</span></div><div class="line">    plot_X = np.linspace(<span class="number">-1</span>, <span class="number">12</span>, <span class="number">100</span>)</div><div class="line">    W = np.asarray([b_Trace[i], w_Trace[i]]).reshape(<span class="number">1</span>, <span class="number">2</span>)</div><div class="line">    X = np.row_stack((np.ones(shape=(<span class="number">1</span>, len(plot_X))), plot_X))</div><div class="line">    plot_Y = sigmoid(np.matmul(W, X))</div><div class="line">    line = ax.plot(plot_X, plot_Y[<span class="number">0</span>], <span class="string">'r-'</span>, lw=<span class="number">1</span>)</div><div class="line">    ax.set_xlabel(<span class="string">r"$Cost\ %.6s$"</span> % loss_Trace[i])</div><div class="line">    <span class="keyword">return</span> line</div><div class="line"></div><div class="line"></div><div class="line">ani = animation.FuncAnimation(fig, update, frames=len(w_Trace), interval=<span class="number">100</span>)</div><div class="line">ani.save(<span class="string">'logisticregression.gif'</span>, writer=<span class="string">'imagemagick'</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<h2 id="2-多元分类"><a href="#2-多元分类" class="headerlink" title="2 多元分类"></a>2 多元分类</h2><p>先从三元分类开始了解，加入给了以下分布，要你求边界：</p>
<p><img src="/2018/02/16/MLLogisticRegression/multiClass3ToAll.png" alt=""></p>
<p>比起前面的来就难多了，首先前面是线性分类的而这里很明显不是，其次这里又多出了一个变量。。。别急，对比一下三幅图你就清楚了。</p>
<p><img src="/2018/02/16/MLLogisticRegression/multiClass3To1.png" alt=""><br><img src="/2018/02/16/MLLogisticRegression/multiClass3To2.png" alt=""><br><img src="/2018/02/16/MLLogisticRegression/multiClass3To3.png" alt=""></p>
<p>这里我们就解决了多一个变量的问题，通过选取一个主变量为 \( 1 \)，其它的值就为 \( 0 \)，然后就可以进行二分。还有一个问题是如何选取参数？我们既然可以转换成二分问题了，那么对于每一次二分都只要选取 \( (W, X) \) 了。\( X \) 很明显有两个基本的 \( (X1, X2) \) 但是为了能过选取更加合适的边界，如果只是这两个特征的话是不够的，所以其它的 \( X1^2，X1*X2 \ldots \) 等等，看你要拟合成什么样的边界就选取什么样的特征，同理三维就选取一个能表达弯弯曲曲的平面的特征量。下面选取权重，也就是特征前前面的系数，如果特征选取好了，那么只要在此基础上加一个 \( bias \) 就好了。</p>
<p>下面我们来举个小例子如何操作：</p>
<p><img src="/2018/02/16/MLLogisticRegression/circleGraph.png" alt=""></p>
<p>很明显这里有一个包围趋势，所以理所当然想到圆。也就是说选取的 \( X: X1,\ X2,\ X1^2,\ X2^2 \) 那么\( sigmoid \)函数就可以定义为：</p>
<p>$$<br>h_{(bias, w_1, w_2, w_3, w_4)}({x_1, x_2, x_1^2, x_2^2}) = \frac{1}{1 + e^{-(bias + w_1 * x_1 + w_2 * x_2 + w_3 * x_1^2 + w_4 * x_2^2)}}<br>$$</p>
<p>至于解法就和上面类似了，把这些都抽象成向量，那么至于什么维度，有几个变量都没差了。需要记得的是，这里要跑的次数是分类的种数的大小，也就是把每一种都当做主角处理一次得到属于它把其它元素区分的分界线，预测是也要把所有模型跑一遍，哪个模型的概率高就取哪个作为分类预测的结果。</p>
<p>这里就不详细介绍应用了，以后有机会补充代码实践，下面给出几篇参考博客：</p>
<ul>
<li><a href="http://blog.csdn.net/gamer_gyt/article/details/51236978" target="_blank" rel="external">《机器学习实战》Logistic回归算法（1）</a></li>
<li><a href="http://blog.csdn.net/lookqlp/article/details/51161640" target="_blank" rel="external">逻辑回归梯度下降法详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28775274" target="_blank" rel="external">深入机器学习系列3-逻辑回归</a></li>
</ul>
<h2 id="3-模型优化"><a href="#3-模型优化" class="headerlink" title="3 模型优化"></a>3 模型优化</h2><h3 id="3-1-算法方向"><a href="#3-1-算法方向" class="headerlink" title="3.1 算法方向"></a>3.1 算法方向</h3><p>求解最优化问题不只是只有梯度下降算法，还有其它能够收敛更快的方式。比如：共轭梯度法 <code>BFGS</code> (变尺度法) 和 <code>L-BFGS</code> (限制变尺度法) 等，不过我现在还没看过这些算法的具体实现，这里就立下 FLAG 了。不过一般我们使用这些算法都是调用现成的库，毕竟自己写的算法多多少少还是有缺陷的，</p>
<h3 id="3-2-拟合度方向"><a href="#3-2-拟合度方向" class="headerlink" title="3.2 拟合度方向"></a>3.2 拟合度方向</h3><p>在之前的例子中，你会发现个选取好特征量之后，怎么选取迭代次数很重要。举个小例子：我们拟合一个符合二次函数分布的散点图，如果你选取的模型是 \( b + w*x \) 那怎么跑都跑不出理想结果，必须把 \( x^2 \) 这个特征项加上，这样就不会欠拟合。同样，如果你选取好了一个二次模型，但是迭代次数不够，也会导致欠拟合，讲道理如果模型预测和分布一致，那么迭代次数自然是越多越好。但但是，如果你不能确定，又加一个 \( x^3 \) 的特征量进去，本来是数据是符合二次函数的特征，如果训练次数控制得比较好，也可以训练出好模型，无非就是把 \( x^3 \) 的权重置零，如果训练次数控制不好，那么必将造成欠拟合（训练次数过少）或过拟合（训练次数过多），这里不是说损失函数值越小越少，而是说那拟合函数的趋势符合整体数据的趋势，从概率论的角度讲，就是我们抽取的是总体中的一个样本，由于获得全体数据不现实，所以只能使用样本估计整体，所以这里的估计的就应该是一个趋势。</p>
<p>那么如何避免过拟合或欠拟合呢？我觉得可以从一下角度考虑：</p>
<ul>
<li>数据量（越多越好）</li>
<li>特征量（最小能描述趋势原则）</li>
<li>训练次数（朝着趋势行进方向训练次数越多越好）</li>
</ul>
<p>总之这里最最关键的是模型的准确度，如果一开始模型选错那怎么迭代也不会跑出好的效果。</p>
<p>下面给出吴恩达老师的课件图，大家体会一下：</p>
<p><img src="/2018/02/16/MLLogisticRegression/wuenda_3.png" alt=""></p>
<p><img src="/2018/02/16/MLLogisticRegression/wuenda_4.png" alt=""></p>
<p>可以看到，我们还有一种方式处理：<code>正则化</code></p>
<p>我根据我自己的理解来大胆解释一下吧，如果你有自己的理解就可以不看了。。避免误导。正则化一般就是解决过拟合，也就是我们根本不能确定趋势，所以添加了很多多余的特征量，那么对于这个模型什么样的结果是我们可以接受的呢？如果那些多余特征量的权重接近 \( 0 \) 是不是就意味着我们也能得到较好的拟合效果，甚至和没有这些干扰特征是达到一样的效果。这里就是对那些特征量的系数进行处罚达到这个目的的。</p>
<p>从上面的解释可以看出，我们不需要对 \( bias \) 进行惩罚，所以结果就是在损失函数后面加上 \( \sum_{i = 1}^{n}{w_i}^2 \)。当然这只是其中一种方式，你可以选取其可行的方式，主要目的是避免权重过大，拟合的曲线过于弯曲（因为求导之后，斜率和系数是正相关的）。这里的哲学思想就是在拟合结果能够接受的情况下，曲线越平滑，容错能力越高，样本越能够代表总体。</p>
<p>这里就直接截图举例子了：</p>
<ul>
<li><p>正则化线性回归<br><img src="/2018/02/16/MLLogisticRegression/wuenda_5.png" alt=""></p>
</li>
<li><p>正则化逻辑回归</p>
</li>
</ul>
<p><img src="/2018/02/16/MLLogisticRegression/wuenda_6.png" alt=""><br><img src="/2018/02/16/MLLogisticRegression/wuenda_7.png" alt=""></p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p>经过一些细节的阅读，对整个过程的了解又加深了不少。动手实践才是最好的方式，之前以为自己公式看懂了就行了，然后这次写代码是把一个变量写在了括号里面，死活拟合不出效果。然后花了几个小时之后决心自己再推一遍公式并且重新敲一下公式代码（因为打印数据发现很诡异），后面就一遍过了。所以如果你也是刚刚开始学习，哪怕照着敲一遍，之后也要自己将思路整理成文字/代码形式，最好是能整理成博客，方便以后自己复习。</p>
<p>还是很喜欢那就话：<code>花时间弄懂细节就是节省时间</code></p>
<p>最后：Happy Year Of Dog ^_^ ! </p>
<h2 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5 参考资料"></a>5 参考资料</h2><ul>
<li><a href="https://zhuanlan.zhihu.com/p/26614750" target="_blank" rel="external">@忆臻：一文搞懂极大似然估计</a></li>
<li><a href="http://blog.csdn.net/ligang_csdn/article/details/53838743" target="_blank" rel="external">机器学习–Logistic回归计算过程的推导</a></li>
<li><a href="http://blog.csdn.net/gamer_gyt/article/details/51236978" target="_blank" rel="external">《机器学习实战》Logistic回归算法（1）</a></li>
<li><a href="http://blog.csdn.net/lookqlp/article/details/51161640" target="_blank" rel="external">逻辑回归梯度下降法详解</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/28775274" target="_blank" rel="external">深入机器学习系列3-逻辑回归</a></li>
<li><a href="https://chenrudan.github.io/blog/2016/01/09/logisticregression.html" target="_blank" rel="external">【机器学习算法系列之二】浅析Logistic Regression</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：我将从一个入门者的视角（水平）将机器学习中的常用算法娓娓道来。自身水平确实有限，如果其中有什么错误的话希望大家指出，避免误导大家。然后这是这个系列的第二篇了，对于初学者来说，如果你没看过第一篇，推荐看看&lt;a href=&quot;http://fitzeng.org/2018/02/11/MLLinearRegression/&quot;&gt;机器学习基本算法之线性回归&lt;/a&gt;，里面提及到了很多基础的数学知识和一些的机器学习思维，对于理解这篇文章很有帮助，很多机器学习流程化的东西这里就不在具体介绍为什么这样做了，而是直接解释为什么在逻辑回归中需要使用这样的方式来实现，更加聚焦于逻辑回归实现本身，而非机器学习的流程理解。&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/mk43/machine-learning/tree/master/algorithm/logistic-regression&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;实验代码&lt;/a&gt;-&lt;a href=&quot;https://github.com/mk43/machine-learning/tree/master/reference&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;参考书籍&lt;/a&gt;-[参考博客详见最后]&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
      <category term="NumPy" scheme="http://fitzeng.org/tags/NumPy/"/>
    
      <category term="Matplotlib" scheme="http://fitzeng.org/tags/Matplotlib/"/>
    
      <category term="LogisticRegression" scheme="http://fitzeng.org/tags/LogisticRegression/"/>
    
  </entry>
  
  <entry>
    <title>leetcode 算法集锦（二）</title>
    <link href="http://fitzeng.org/2018/02/14/AlgoAndMath2/"/>
    <id>http://fitzeng.org/2018/02/14/AlgoAndMath2/</id>
    <published>2018-02-14T03:27:00.000Z</published>
    <updated>2018-02-15T03:02:44.999Z</updated>
    
    <content type="html"><![CDATA[<h2 id="leetcode-算法集锦（二）"><a href="#leetcode-算法集锦（二）" class="headerlink" title="leetcode 算法集锦（二）"></a><a href="https://www.nowcoder.com/ta/leetcode" target="_blank" rel="external">leetcode 算法集锦（二）</a></h2><blockquote>
<p>主要是牛客网上 leetcode 的算法题实践. 在 Blog 包含自己的解法和对别人优秀解法的分析.</p>
</blockquote>
<a id="more"></a>
<table>
<thead>
<tr>
<th style="text-align:center">序号</th>
<th style="text-align:center">考点</th>
<th style="text-align:center">题目</th>
<th style="text-align:center">C/C++</th>
<th style="text-align:center">Java     </th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">  11</td>
<td style="text-align:center">动态规划</td>
<td style="text-align:center">word-break-ii</td>
<td style="text-align:center">NULL</td>
<td style="text-align:center"><a href="#jump_11">题解</a>  </td>
</tr>
<tr>
<td style="text-align:center">  12</td>
<td style="text-align:center">动态规划</td>
<td style="text-align:center">word-break</td>
<td style="text-align:center">NULL</td>
<td style="text-align:center"><a href="#jump_12">题解</a> </td>
</tr>
</tbody>
</table>
<h3 id="12-word-break"><a href="#12-word-break" class="headerlink" title="12 : word-break"></a><span id="jump_12"><a href="https://www.nowcoder.com/practice/5f3b7bf611764c8ba7868f3ed40d6b2c" target="_blank" rel="external">12 : word-break</a></span></h3><h4 id="我的解法"><a href="#我的解法" class="headerlink" title="我的解法"></a>我的解法</h4><p>参考上一题，不过不用记录结果，用个标记就可以了。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Set;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">   <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">boolean</span> <span class="title">wordBreak</span><span class="params">(String s, Set&lt;String&gt; dict)</span> </span>&#123;</div><div class="line">        ArrayList&lt;String&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">        <span class="keyword">boolean</span> flag = <span class="keyword">false</span>;</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = s.length() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</div><div class="line">            String word = s.substring(i, s.length());</div><div class="line">            <span class="keyword">if</span> (dict.contains(word)) &#123;</div><div class="line">                flag = wordBreak(s.substring(<span class="number">0</span>, i), dict);</div><div class="line">                <span class="keyword">if</span> (<span class="keyword">true</span> == flag || <span class="number">0</span> == i) &#123;</div><div class="line">                    <span class="keyword">return</span> <span class="keyword">true</span>;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> flag;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="其他思路"><a href="#其他思路" class="headerlink" title="其他思路"></a>其他思路</h4><p>用动态规划，dp[i]表示字符串s[0~i]是否可分的bool值。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> len = s.length();</div><div class="line"><span class="function">vector&lt;bool&gt; <span class="title">dp</span><span class="params">(len+<span class="number">1</span>,<span class="keyword">false</span>)</span></span>;</div><div class="line">dp[<span class="number">0</span>]=<span class="keyword">true</span>;</div><div class="line"><span class="keyword">for</span>(<span class="keyword">int</span> pos=<span class="number">0</span>;pos&lt;len;++pos)</div><div class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=pos;dp[pos] &amp;&amp; i&lt;len;++i)</div><div class="line">        <span class="keyword">if</span> (dict.find(s.substr(pos,i-pos+<span class="number">1</span>))!=dict.end())</div><div class="line">            dp[i+<span class="number">1</span>]=<span class="keyword">true</span>;</div><div class="line"><span class="keyword">return</span> dp[len];</div></pre></td></tr></table></figure>
<h3 id="11-word-break-ii"><a href="#11-word-break-ii" class="headerlink" title="11 : word-break-ii"></a><span id="jump_11"><a href="https://www.nowcoder.com/practice/bd73f6b52fdc421d91b14f9c909f9104" target="_blank" rel="external">11 : word-break-ii</a></span></h3><h4 id="我的解法-1"><a href="#我的解法-1" class="headerlink" title="我的解法"></a>我的解法</h4><p>利用递归和动态规划的思想，每次截取最后面的一个然后把剩下的递归截取，接下来继续循环计算不截取该单词之后的情况。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> java.util.ArrayList;</div><div class="line"><span class="keyword">import</span> java.util.Set;</div><div class="line"></div><div class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> ArrayList&lt;String&gt; <span class="title">wordBreak</span><span class="params">(String s, Set&lt;String&gt; dict)</span> </span>&#123;</div><div class="line">        ArrayList&lt;String&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">        ArrayList&lt;String&gt; temp = <span class="keyword">new</span> ArrayList&lt;&gt;();</div><div class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = s.length() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; --i) &#123;</div><div class="line">            String word = s.substring(i, s.length());</div><div class="line">            <span class="keyword">if</span> (dict.contains(word)) &#123;</div><div class="line">                temp = wordBreak(s.substring(<span class="number">0</span>, i), dict);</div><div class="line">                <span class="keyword">if</span> (<span class="number">0</span> == temp.size() &amp;&amp; i == <span class="number">0</span>) &#123;</div><div class="line">                    result.add(word);</div><div class="line">                &#125; <span class="keyword">else</span> &#123;</div><div class="line">                    <span class="keyword">for</span> (String str : temp) &#123;</div><div class="line">                        result.add(str + <span class="string">" "</span> + word);</div><div class="line">                    &#125;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        <span class="keyword">return</span> result;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure>
<h4 id="其他思路-1"><a href="#其他思路-1" class="headerlink" title="其他思路"></a>其他思路</h4><p>很基本的动态规划思路，值得学习。DP[i][j] 表示 S 从第 i 个字母到第 j 个字母的字符串，如果在字典中，就标记为 1，直到整个遍历完成。<br>然后从 i = 0 开始找寻为 1 的下标 j 代表截取前面一个字母成功，然后从 i = j + 1 开始找下一个等于 1 的 j 直到 j = S.length() 就说明截取成功。</p>
<figure class="highlight"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div><div class="line">110</div><div class="line">111</div><div class="line">112</div><div class="line">113</div><div class="line">114</div><div class="line">115</div><div class="line">116</div><div class="line">117</div><div class="line">118</div><div class="line">119</div><div class="line">120</div><div class="line">121</div><div class="line">122</div><div class="line">123</div><div class="line">124</div><div class="line">125</div><div class="line">126</div><div class="line">127</div><div class="line">128</div><div class="line">129</div><div class="line">130</div><div class="line">131</div><div class="line">132</div><div class="line">133</div><div class="line">134</div><div class="line">135</div><div class="line">136</div><div class="line">137</div><div class="line">138</div><div class="line">139</div><div class="line">140</div><div class="line">141</div></pre></td><td class="code"><pre><div class="line">思路CSDN Allan:</div><div class="line">=================================思路Allan========================================</div><div class="line">s ="catsand",</div><div class="line">dict =["cat", "cats", "and", "sand", "dog"].</div><div class="line"> </div><div class="line">动态规划根本思想是记录状态值:</div><div class="line">DP[i][j]:</div><div class="line">           j    0       1       2       3       4       5       6  </div><div class="line">        i </div><div class="line">        0       c       ca      cat(1)  cats(1) catsa   catsan  catsand</div><div class="line">        1               a       at      ats     atsa    atsan   atsand       </div><div class="line">        2                       t       ts      tsa     tsan    tsand</div><div class="line">        3                               s       sa      san     sand(1)</div><div class="line">        4                                       a       an      and(1)</div><div class="line">        5                                               n       nd</div><div class="line">        6                                                       d</div><div class="line"> </div><div class="line">DP[i][j]里:</div><div class="line">0       c       ca      cat(1)  cats(1) catsa   catsan  catsand</div><div class="line">1       a       at      ats     atsa    atsan   atsand</div><div class="line">2       t       ts      tsa     tsan    tsand</div><div class="line">3       s       sa      san     sand(1)</div><div class="line">4       a       an      and(1)</div><div class="line">5       n       nd</div><div class="line">6       d</div><div class="line">思路:</div><div class="line">DP[i][j]存放着字符串s的所有子字符串在dict中的状态值。</div><div class="line">遍历顺序是先搜索i到串尾的子串，若子串在dict里，再搜索串头到i的子串。</div><div class="line">        c a t s a n d</div><div class="line">        j     i</div><div class="line">比如，dp[3][3]=1表明"sand"在dict里，再搜索cat......</div><div class="line">再搜索顺序为cat at t......</div><div class="line"> </div><div class="line"> </div><div class="line">output(6,s):</div><div class="line">i=6     k:0 1 2 3 4 5 6</div><div class="line">dp[k][i-k]:偏移为k，截断字符串长度i-k+1</div><div class="line">沿着次对角线遍历,相当于从头部每隔一个字符截断!!!</div><div class="line">dp[0][6]:偏移为0，截断字符串长度7      0</div><div class="line">dp[1][5]:偏移为1，截断字符串长度6      0</div><div class="line">dp[2][4]:偏移为2，截断字符串长度5      0</div><div class="line">dp[3][3]:偏移为3，截断字符串长度4      1   --&gt;output(2,s)</div><div class="line">dp[4][2]:偏移为4，截断字符串长度3      1   --&gt;output(3,s)</div><div class="line">dp[5][1]:偏移为5，截断字符串长度2      0</div><div class="line">dp[6][0]:偏移为6，截断字符串长度1      0</div><div class="line"> </div><div class="line">output(2,s):</div><div class="line">i=2     k:0 1 2</div><div class="line">dp[k][i-k]:偏移为k，截断字符串长度i-k+1</div><div class="line">沿着次对角线遍历,相当于从头部每隔一个字符截断!!!</div><div class="line">dp[0][2]:偏移为0，截断字符串长度3      1   --&gt;output(-1,s)</div><div class="line">dp[1][1]:偏移为1，截断字符串长度2      0</div><div class="line">dp[2][0]:偏移为2，截断字符串长度1      0</div><div class="line"> </div><div class="line">output(-1,s):</div><div class="line">......</div><div class="line"> </div><div class="line"> </div><div class="line">mystring.push_back(s.substr(k,i-k+1));</div><div class="line">output(k-1,s);</div><div class="line">s.substr(k,i-k+1)==&gt;递归output(k-1,s)!!!</div><div class="line">偏移为k,截断长度i-(k-1);                           ------</div><div class="line">==&gt;递归为k-1                                              |---&gt;处理字符串长度i </div><div class="line">从偏移为0，截断长度k开始以次对角线方向遍历!!!       ------</div><div class="line">================================================================================</div><div class="line"> </div><div class="line">*/</div><div class="line">#include &lt;iostream&gt;</div><div class="line">#include &lt;string&gt;</div><div class="line">#include &lt;vector&gt;</div><div class="line">#include &lt;stack&gt;</div><div class="line">#include &lt;list&gt;</div><div class="line">#include &lt;set&gt;</div><div class="line">#include &lt;unordered_set&gt;</div><div class="line">#include &lt;map&gt;</div><div class="line">#include &lt;queue&gt;</div><div class="line">#include &lt;algorithm&gt;</div><div class="line">#include &lt;numeric&gt;        //accmulate</div><div class="line">#include &lt;functional&gt;   //greater&lt;int&gt;()!!!</div><div class="line">using namespace std;</div><div class="line"> </div><div class="line">class Solution &#123;</div><div class="line">public:</div><div class="line">    vector&lt;string&gt; wordBreak(string s, unordered_set&lt;string&gt; &amp;dict) &#123;</div><div class="line">        dp = new vector&lt;bool&gt;[s.size()];</div><div class="line"> </div><div class="line">        for (int i = 0; i &lt; s.size(); i++)</div><div class="line">        &#123;</div><div class="line">            for (int j = i; j &lt; s.size(); j++)</div><div class="line">            &#123;</div><div class="line">                dp[i].push_back(match(s.substr(i, j - i + 1), dict));</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">        /*</div><div class="line">        for (int i = 0; i &lt; s.size(); i++)</div><div class="line">        &#123;</div><div class="line">            for (int j = 0; j &lt; dp[i].size(); j++)</div><div class="line">            &#123;</div><div class="line">                cout &lt;&lt; dp[i][j] &lt;&lt; " ";</div><div class="line">            &#125;</div><div class="line">            cout &lt;&lt; endl;</div><div class="line">        &#125;</div><div class="line">        */</div><div class="line">        output(s.size() - 1, s);</div><div class="line">         </div><div class="line">        return result;</div><div class="line">    &#125;</div><div class="line">    bool match(string s, unordered_set&lt;string&gt; &amp;dict)</div><div class="line">    &#123;</div><div class="line">        if (dict.find(s) != dict.end()) return true;</div><div class="line">        else return false;</div><div class="line">    &#125;</div><div class="line">    void output(int i, string s)</div><div class="line">    &#123;</div><div class="line">        if (i == -1)    //递归退出条件</div><div class="line">        &#123;</div><div class="line">            string str;</div><div class="line">            for (int j = mystring.size() - 1; j &gt;= 0; j--)</div><div class="line">            &#123;</div><div class="line">                str += mystring[j];</div><div class="line">                if (j != 0) str += " ";</div><div class="line">            &#125;</div><div class="line">            result.push_back(str);</div><div class="line">        &#125;</div><div class="line">        else</div><div class="line">        &#123;</div><div class="line">            for (int k = 0; k &lt;= i; k++) //error!!!  for (int k = 0; k &lt; i; k++)</div><div class="line">            &#123;</div><div class="line">                if (dp[k][i - k])       //dp[k][i-k]:偏移为k，截断长度i-k+1</div><div class="line">                &#123;</div><div class="line">                    mystring.push_back(s.substr(k, i - k + 1));</div><div class="line">                    output(k - 1, s);   //递归:dp[0][i]:偏移为0，截断长度i+1      i=k-1,则截断长度为k,与递归前偏移k互补</div><div class="line">                    mystring.pop_back();</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    vector&lt;string&gt; result;</div><div class="line">    vector&lt;string&gt; mystring;</div><div class="line">    vector&lt;bool&gt; *dp;</div><div class="line">&#125;;</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;leetcode-算法集锦（二）&quot;&gt;&lt;a href=&quot;#leetcode-算法集锦（二）&quot; class=&quot;headerlink&quot; title=&quot;leetcode 算法集锦（二）&quot;&gt;&lt;/a&gt;&lt;a href=&quot;https://www.nowcoder.com/ta/leetcode&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;leetcode 算法集锦（二）&lt;/a&gt;&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;主要是牛客网上 leetcode 的算法题实践. 在 Blog 包含自己的解法和对别人优秀解法的分析.&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Algo" scheme="http://fitzeng.org/tags/Algo/"/>
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Java" scheme="http://fitzeng.org/tags/Java/"/>
    
      <category term="C/C++" scheme="http://fitzeng.org/tags/C-C/"/>
    
  </entry>
  
  <entry>
    <title>机器学习基本算法之线性回归</title>
    <link href="http://fitzeng.org/2018/02/11/MLLinearRegression/"/>
    <id>http://fitzeng.org/2018/02/11/MLLinearRegression/</id>
    <published>2018-02-11T03:27:00.000Z</published>
    <updated>2018-02-12T08:50:48.613Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：我将从一个入门者的视角（水平）将机器学习中的常用算法娓娓道来。自身水平确实有限，如果其中有什么错误的话希望大家指出，避免误导大家。对于开篇有比较多的东西想说，所以另取一章介绍一下我个人对机器学习这个领域的理解和一些基本常识介绍。如果你时间比较充裕，可以看看我<code>前言</code>的文字介绍。</p>
</blockquote>
<a id="more"></a>
<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0 前言"></a>0 前言</h2><p>介绍一下个人想法</p>
<h3 id="0-1-关于机器学习"><a href="#0-1-关于机器学习" class="headerlink" title="0.1 关于机器学习"></a>0.1 关于机器学习</h3><p>毫无疑问，机器学习现在越来越热门了，需求随着以后人工智能大爆发会持续增长。不管你是不是这个编程领域的人，都有必要了解一下机器学习是怎么回事，至少有个理性的认识，而不是觉得很神秘。但是涉及到算法数学知识是可以忽略的，只要知道，我们通过某个数学手段可以达到某个目的。比如这里要介绍的线性回归在更新权重时是依据梯度下降算法的，你可以不必知道什么是梯度下降，知道知道数学上对于求解最优化是有自己的手段的，当然如果能知道更好。由于机器学习的性质，速成基本不太可能，而且现在处于高速发展的阶段，必须保持学习的热情才能学得下去。另外，这个前期的学习成本是比较高的，在自己写代码之前，要想心里有点底，至少要对 <code>微积分</code>、<code>线性代数</code> 和 <code>概率论</code> 的应用要有点自己的理解的。所以这也是一大优势，不太容易被取代。</p>
<h3 id="0-2-关于线性回归"><a href="#0-2-关于线性回归" class="headerlink" title="0.2 关于线性回归"></a>0.2 关于线性回归</h3><p>对于理性了解机器学习的捷径就是弄懂线性回归算法的整个流程，如果你通过这篇文章真的理解了线性回归，那么你应该就理解这句话了。当然，对于大多数阅读者篇文章的人来说，可能早就对线性回归有了自己的理解。但是我这要提的是，如果你没打算入门机器学习，阅读一下线性回归也是对自己知识面进行拓展的一个好方式，因为基本大多数人都是从这里开始的。</p>
<h3 id="0-3-关于基础"><a href="#0-3-关于基础" class="headerlink" title="0.3 关于基础"></a>0.3 关于基础</h3><p>首先，你需要一点 \( Python \) 基础。如果没有也简单，自学一下就行。然后就是要学习一下以下工具的使用：<code>NumPy</code>，<code>Matplotlib</code>，<code>sklearn</code>，<code>TensorFlow</code>。其中部分可以参考我以前的文章。</p>
<ul>
<li><a href="http://fitzeng.org/2018/02/03/TensorFlowIntroduction/">从 TensorFlow 入门机器学习</a></li>
<li><a href="http://fitzeng.org/2018/02/04/NumPyOfficialQuickstartTutorial/">NumPy 官方快速入门教程(译)</a></li>
<li><a href="http://fitzeng.org/2018/02/07/MatplotlibDraw/">Matplotlib 基本操作</a></li>
<li><a href="http://fitzeng.org/2018/02/08/MatplotlibGenerateGif/">Matplotlib 保存 GIF 动图</a></li>
</ul>
<p>有了这些你还需要一些数学基础。数学的话我觉得不要求全学会，也不太现实，可以在写代码过程中碰到什么算法再去寻求数学证明，前提是之前的知识量最好能达到看懂大多数数学知识的水平。这里我觉得知乎上有些数学科普其实挺不错的，毕竟不是数学系，所以理解主要逻辑之后持有一种拿来用的态度我觉得没问题。</p>
<ul>
<li><a href="http://fitzeng.org/2018/01/26/reProbabilityTheory/">机器学习之重拾概率论</a></li>
</ul>
<h3 id="0-4-关于参考"><a href="#0-4-关于参考" class="headerlink" title="0.4 关于参考"></a>0.4 关于参考</h3><p>这里很感谢像吴恩达、莫烦等老师的无私奉献，还有很多在网上写博客分享知识的人，让编程这个行业形成一个良好的学习风气。而且网上还流传很多优质的学习资源，所以这里我将我学习过程的参考资料整理进了 <a href="https://github.com/mk43" target="_blank" rel="external">GitHub</a>，我想大家奉献出来就是为了别人更好的学习吧，如果有侵权之类的话，我会立即删除的。然后这一系列的文章，我会主要以吴恩达老师的课程为基础，以初学者的视角记录并且实践。</p>
<ul>
<li><a href="https://github.com/mk43/machine-learning/tree/master/reference" target="_blank" rel="external">学习参考书籍</a></li>
</ul>
<h3 id="0-5-关于我"><a href="#0-5-关于我" class="headerlink" title="0.5 关于我"></a>0.5 关于我</h3><p>一个即将毕业的知识水平有限的本科生，所以文章肯定有很多不足之处，望大家指正。然后实验的代码在<a href="https://github.com/mk43/machine-learning/tree/master/algorithm/linear-regression" target="_blank" rel="external">这里</a>。</p>
<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1 背景"></a>1 背景</h2><p>故事就不讲了，可以参考视视频。下面我讲讲这个是要解决什么问题？生活中往往有很多现象，而我们可以从现象总结出某些结论，但是，如果我们看到了一个之前从来都没有看到过的现象，我们就无法下结论了。但但是一些有经验的人可以凭借丰富的经验进行预测。就比如，一个处于懵懂期的小孩子，看见一片乌云不能推测出会下雨，而一个大人就会知道，提醒小孩出门要带伞。。。好吧，还是在讲故事，那就继续吧😂😂😂。这个例子中的大人小孩的区别在于年龄不同，见识不同，知识水平自然不同，能做出的判断也自然不同。所以要想达到对现象进行预测，那么就必须进行学习，等小孩慢慢长大了，看到蚂蚁搬家就会带伞出门了。</p>
<p>好了，我们总结一下上面的故事：现象可以比喻成数据，结论造成的行为也是数据，唯一不同的是，是有现象得出的结论，所以可以将现象的数据看作是输入，结论的数据看成是输出。那么机器学习就是将输入数据输入进一个模型，然后将模型的输出结果和之前的“正确”结论作比对慢慢更改模型，直到这个模型对出入数据有较好的预测，那么这个模型就可以拿来用了。抽象出来的话就是找一个输入\( X \) 和输出 \( Y \)之间的映射\( f \)，使得对于绝大多数 \( X \) 都能通过映射 \( f(X) \rightarrow Y \) 较好地得到 \( Y \)。</p>
<p>房价的例子我就不举了，直接看下面的图，看看能不能解决 \( X = 4,\ Y = ? \) </p>
<p><img src="/2018/02/11/MLLinearRegression/scattergraph.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"># coding: utf-8</div><div class="line"></div><div class="line">import numpy as np</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">def draw():</div><div class="line">    X = np.linspace(0, 10, 50)</div><div class="line">    noise = np.random.normal(0, 0.5, X.shape)</div><div class="line">    Y = X * 0.5 + 3 + noise</div><div class="line"></div><div class="line">    plt.scatter(X, Y)</div><div class="line">    plt.savefig(&quot;scattergraph.png&quot;)</div><div class="line">    plt.show()</div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    draw()</div></pre></td></tr></table></figure>
<p>好了，你可能会发现，要想给出 \( Y \) 依赖的是直觉，所以每一个人的答案都是不一样的，而对于某些标准统一的领域（比如军事项目），这种依靠直觉判断的情况绝对不容许发生。想象在现代信息化战场上靠直觉怎么可能作战成功？</p>
<p>所以我们需要一个模型，你可以理解成一个黑盒，把 \( X \) 喂进去，把 \( Y \) 取出来，而且无论谁，只要那个黑盒是同一个，那么每个人的输入 \( X \) 相同的话，输出 \( Y \) 就是相同的。 那么在这里这个模型大概是什么形状的呢？目前这个还是我们人为干预进行选择的。直观来说，可以这个分布看成一条很粗的线，那么我们选择线性模型来模拟分布。那么如何确定这个线性模型呢？这里就涉及到今天的主角：<a href="https://en.wikipedia.org/w/index.php?title=Linear_regression" target="_blank" rel="external">线性回归</a></p>
<h2 id="2-一元线性回归"><a href="#2-一元线性回归" class="headerlink" title="2 一元线性回归"></a>2 一元线性回归</h2><p>先从一元线性函数开始分析，既然假设是一元线性，那么我们的拟合函数就可以定义为 \( Y = X*W + b \) 了。同样那前一个图作为例子，我们现在的目的是求出\( W, b \)，如果求出来了，给定任意一个 \( X \) 就能求出 \( Y \) 来了。</p>
<h3 id="2-1-定义评估函数-损失函数"><a href="#2-1-定义评估函数-损失函数" class="headerlink" title="2.1 定义评估函数-损失函数"></a>2.1 定义评估函数-损失函数</h3><p>这里讲解如何求出 \( W, b \) 的整个思考过程。</p>
<p>我们选取一个点 \( (x_0, y_0) \) 因为误差的存在，所以 \( y_0 = x_0 <em> W + b + \delta_0 \)。同理如果有很多点，那么就有以下结果，为了方便，我们取评估值 \( h(x) = x </em> W + b \)。</p>
<p>$$<br>\delta_0 = y_0 - h(x_0) \\<br>\delta_1 = y_1 - h(x_1) \\<br>\ldots \\<br>\delta_n = y_n - h(x_n) \\\<br>$$</p>
<p>我们的目的自然是误差越小越好，所以采用平方和的形式表达误差：</p>
<p>$$<br>loss(W, b) = \sum_{i = 0}^{n}\delta_i^2 = \sum_{i = 0}^{n} [y_i - h(x_i)]^2<br>$$</p>
<p>到这里，我们来看看图像是怎么样的？</p>
<p><img src="/2018/02/11/MLLinearRegression/lossgraph.png" alt=""></p>
<p>PS：图有点丑，实在是没办法了，设置了噪点，使得函数值跨度很大，稍微改一下就会呼脸。只能设置散点图凑合看了，望知道的小伙伴告知。好了，就不在这浪费过多时间了，直接看代码。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</div><div class="line"></div><div class="line">fig = plt.figure()</div><div class="line">ax = Axes3D(fig)</div><div class="line">ax.set_xlabel(<span class="string">r'$Weight$'</span>)</div><div class="line">ax.set_ylabel(<span class="string">r'$bias$'</span>)</div><div class="line">ax.set_zlabel(<span class="string">r'$loss\ value$'</span>)</div><div class="line">N = <span class="number">200</span></div><div class="line"></div><div class="line">X = np.linspace(<span class="number">0</span>, <span class="number">10</span>, N)</div><div class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, X.shape)</div><div class="line">Y = X * <span class="number">0.5</span> + <span class="number">3</span> + noise</div><div class="line"></div><div class="line">W = np.linspace(<span class="number">-10</span>, <span class="number">10</span>, N)</div><div class="line">b = np.linspace(<span class="number">-2</span>, <span class="number">8</span>, N)</div><div class="line"></div><div class="line">p = <span class="number">0.6</span></div><div class="line">plt.xlim(W.min() * p, W.max() * p)</div><div class="line">plt.ylim(b.min() * p, b.max() * p)</div><div class="line">ax.set_zlim(<span class="number">0</span>, <span class="number">1000</span>)</div><div class="line"></div><div class="line">h = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> W:</div><div class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> b:</div><div class="line">        h.append(np.sum(np.square(Y - (X * i + j))))</div><div class="line"></div><div class="line">print(np.min(h))</div><div class="line">h = np.asarray(h).reshape(N, N)</div><div class="line">W, b = np.meshgrid(W, b)</div><div class="line"></div><div class="line">ax.scatter(W, b, h, c=<span class="string">'b'</span>)</div><div class="line">plt.savefig(<span class="string">"lossgraph.png"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>现在我们通过可视化的手段知道了大概什么位置取最小值，其实我们还是没有达到目的，我们要的是 \( W, b \) 具体值。</p>
<h3 id="2-2-求解最小值-梯度下降"><a href="#2-2-求解最小值-梯度下降" class="headerlink" title="2.2 求解最小值-梯度下降"></a>2.2 求解最小值-梯度下降</h3><p>问题被进一步细化，就是求解下面二元函数在什么时候取极值：</p>
<p>$$<br>loss(W, b) = \sum_{i = 0}^{n}\delta_i^2 = \sum_{i = 0}^{n} [y_i - h(x_i)]^2<br>$$</p>
<p>这时候高数就登场了，找了一张图</p>
<p><img src="/2018/02/11/MLLinearRegression/findextremum.png" alt=""></p>
<p>那问题就很简单了，就是直接求对 \( W, b \) 的偏导等于 \( 0 \) 的方程组。但是这是数学方法，不符合计算机思维，就好像我们解一个一元一次方程，普通方法是直接拿定义域内的值一个一个试，直到结果符合预期。当然，高级一点点的方法是可以写一个解释器，把人类计算方法程序化。显然这里不太现实，而且方程类型一变，解释器很可能就不适用了。所以我们还是采用那种机器认可的“笨”办法，想起开学时计算机导论老师评价计算机的一句话：<code>fast but stupid</code>。说明在这种情况下，我们可以接受用快来弥补其它劣势，只要计算的值一步一步靠近最终结果就能满足需求。其实说到这里，计算机变快的这些特点和现在机器学习、人工智能领域的兴起有一定关系。</p>
<p>好了，下面开始想一个办法如何让结果慢慢趋近与极值。</p>
<p>我们可以这样想，随机放一个球在这个平面上，等到球禁止时，它所在的位置就是极值的位置。那我们如何模仿这一过程呢？</p>
<blockquote>
<p>第一步：放球</p>
</blockquote>
<p>这个好做，就是直接随机一个在定义域内的任意位置就好了。</p>
<blockquote>
<p>第二步：滚动</p>
</blockquote>
<p>滚动的话如何去模拟是一个比较难的点，但是仔细分析就很好理解了。你可以把自己想象成那个球，开始站在某个地方，这个地方很不稳，那么你自然反应肯定是往平整的地方去，前提是每个位置的海拔差不多。那这样就说明那些斜率越大的地方有更大的几率更快跌落谷底。这个例子有个不恰当的地方，可以思考下二次函数，这里主要是体会这种思维，而我们是可以变通的。当你有了这种斜率的思维，那就好办了，下面给出总结：<code>我们总是希望在给定步长的情况下，往水平最低的地方行进</code>。而其中要知道那个是最低，又是一个难题，计算机可没有直觉，如果数值设置不当，可能导致函数收敛过慢或者直接发散，而这些都是机器学习应该极力避免的。</p>
<p>为了好分析问题，我们采用控制变量方法。可以看到，在损失函数中有两个变量 \( W, b \)，当 \( W \) 取某一值得时候，\( loss(n, b) \ n\ is\ a\ constant \)是一个二次函数。</p>
<p><img src="/2018/02/11/MLLinearRegression/quadraticFunction.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">fig = plt.figure()</div><div class="line"></div><div class="line">N = <span class="number">200</span></div><div class="line"></div><div class="line">X = np.linspace(<span class="number">0</span>, <span class="number">10</span>, N * <span class="number">2</span>)</div><div class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, X.shape)</div><div class="line">Y = X * <span class="number">0.5</span> + <span class="number">3</span> + noise</div><div class="line"></div><div class="line">W = <span class="number">1</span></div><div class="line">b = np.linspace(<span class="number">-5</span>, <span class="number">6</span>, N)</div><div class="line"></div><div class="line">h = []</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> b:</div><div class="line">    h.append(np.sum(np.square(Y - (X * W + i))))</div><div class="line"></div><div class="line">plt.plot(b, h)</div><div class="line"></div><div class="line">plt.savefig(<span class="string">"quadraticFunction.png"</span>)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>我们来直接看吴恩达老师的课件吧，有点不想做图了。。。</p>
<p><img src="/2018/02/11/MLLinearRegression/wuenda_1.png" alt=""></p>
<p>现在我们可以引出更新规则了，在这里求解最小值，对于一元函数来说就是求导，对于多元函数就是求偏导了。很明显，当斜率为正的时候，我们要往负反向走，反之往正方向走。所以我们可以加上一个关于斜率呈反比的函数来跟新值，至于更新力度也就是前面说的一次走多远就是学习率的设定了。而且离极值点越远，斜率绝对值越大，步子迈得越大，符合更新逻辑。</p>
<p>$$<br>\begin{aligned}<br>&amp;W:=W-\frac{\alpha}{2n}\frac{\partial\,loss(W)}{\partial\,W} = W + \frac{ \alpha}{n}\sum_{i = 1}^{n}{x_i*(y_i-h(x_i))}\\<br>&amp;b:=b-\frac{\alpha}{2n}\frac{\partial\,loss(b)}{\partial\,b} = b + \frac{ \alpha}{n}\sum_{i = 0}^{n}{y_i-h(x_i)}<br>\end{aligned}<br>$$</p>
<p>这里可能有点思维跳跃的是除了一个 \( 2n \)，可以从数据量的角度思考，也可以从更新斜率的角度思考，因为在更新权重中，我们不仅仅只是计算一个点的斜率，如果直接求和必定会导致权重过大从而最终结果是发散的，这点大家可以自己改改代码试试。好了，既然基本思路都出来了，那就写代码实现吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">N = <span class="number">200</span></div><div class="line"></div><div class="line">X = np.linspace(<span class="number">0</span>, <span class="number">10</span>, N * <span class="number">2</span>)</div><div class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.5</span>, X.shape)</div><div class="line">Y = X * <span class="number">0.5</span> + <span class="number">3</span> + noise</div><div class="line"></div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcLoss</span><span class="params">(train_X, train_Y, W, b)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.sum(np.square(train_Y - (train_X * W + b)))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradientDescent</span><span class="params">(train_X, train_Y, W, b, learningrate=<span class="number">0.001</span>, trainingtimes=<span class="number">500</span>)</span>:</span></div><div class="line">    <span class="keyword">global</span> loss</div><div class="line">    <span class="keyword">global</span> W_trace</div><div class="line">    <span class="keyword">global</span> b_trace</div><div class="line">    size = train_Y.size</div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(trainingtimes):</div><div class="line">        prediction = W * train_X + b</div><div class="line">        tempW = W + learningrate * np.sum(train_X * (train_Y - prediction)) / size</div><div class="line">        tempb = b + learningrate * np.sum(train_Y - prediction) / size</div><div class="line">        W = tempW</div><div class="line">        b = tempb</div><div class="line">        loss.append(calcLoss(train_X, train_Y, W, b))</div><div class="line">        W_trace.append(W)</div><div class="line">        b_trace.append(b)</div><div class="line"></div><div class="line"></div><div class="line">Training_Times = <span class="number">100</span></div><div class="line">Learning_Rate = <span class="number">0.002</span></div><div class="line"></div><div class="line">loss = []</div><div class="line">W_trace = [<span class="number">-1</span>]</div><div class="line">b_trace = [<span class="number">1</span>]</div><div class="line">gradientDescent(X, Y, W_trace[<span class="number">0</span>], b_trace[<span class="number">0</span>], learningrate=Learning_Rate, trainingtimes=Training_Times)</div><div class="line">print(W_trace[<span class="number">-1</span>], b_trace[<span class="number">-1</span>])</div><div class="line"></div><div class="line">fig = plt.figure()</div><div class="line">plt.title(<span class="string">r'$loss\ function\ change\ tendency$'</span>)</div><div class="line">plt.xlabel(<span class="string">r'$learning\ times$'</span>)</div><div class="line">plt.ylabel(<span class="string">r'$loss\ value$'</span>)</div><div class="line">plt.plot(np.linspace(<span class="number">1</span>, Training_Times, Training_Times), loss)</div><div class="line">plt.savefig(<span class="string">"gradientDescentLR.png"</span>)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/11/MLLinearRegression/gradientDescentLR.png" alt=""></p>
<p>到这里，我们基本手写实现了一元变量的线性回归，这里为了方便训练次数取得比较少，只有 \( 100 \) 次，这远远不够，我测试了一下，大概达到 \( 10000 \) 次训练效果就比较好了。当然也可以调整学习率，初始值……读者可以自行尝试。</p>
<h2 id="3-多元线性回归"><a href="#3-多元线性回归" class="headerlink" title="3 多元线性回归"></a>3 多元线性回归</h2><p>这里我们先拆分理解多元代表多个自变量，线性代表自变量之间满足齐次性和可加性，回归就是一种方法的表述。一元好办，我们可以通过各种工具实现可视化，但是多元的表述就有了一些困难，特别是在高维线性空间我们想象不出来。所以我们就必须抽象出来，使用数学符号代替那些复杂的变量。</p>
<p>可能你已经想到了，对，没错，我们要使用线性代数了。如果之前没有一点基础的话，可以稍微花点时间在知乎上了解下线性代数是做什么的就好了，这里只涉及线性代数的基础认识应用。</p>
<p>假如当我们有三个自变量 \( X_1, X_2, X_3 \) 和一个因变量 \( Y \)，且符合线性关系。那么就会有以下方程成立：</p>
<p>$$<br>w_1X_1 + w_2X_2 + w_3X_3 + b = Y<br>$$</p>
<p>再假设我们有四组数据\((x_1, x_2, x_3, y): \{(1,1,1,1), (1,1,2,3), (1,3,4,1), (3,2,4,2) \} \)，那么有：</p>
<p>$$<br>1w_1 + 1w_2 + 1w_3 + b = 1 \\<br>1w_1 + 1w_2 + 2w_3 + b = 3 \\<br>1w_1 + 3w_2 + 4w_3 + b = 1 \\<br>3w_1 + 2w_2 + 4w_3 + b = 2<br>$$</p>
<p>于是我们可以抽象以下，写成矩阵的形式：</p>
<p>$$<br>\begin{bmatrix}<br>1 &amp; 1 &amp; 1 &amp; 1 \\<br>1 &amp; 1 &amp; 2 &amp; 1 \\<br>1 &amp; 3 &amp; 4 &amp; 1 \\<br>3 &amp; 2 &amp; 4 &amp; 1<br>\end{bmatrix} * \begin{bmatrix}<br>w_1 \\<br>w_2 \\<br>w_3 \\ b \end{bmatrix} = \begin{bmatrix}<br>1 \\<br>3 \\<br>1 \\<br>2<br>\end{bmatrix}<br>$$</p>
<p>再抽象：</p>
<p>$$<br>XW = Y<br>$$</p>
<p>其中</p>
<p>$$<br>X =<br>\begin{bmatrix}<br>X_1 &amp; X_2 &amp; X_3 &amp; 1<br>\end{bmatrix}<br>and\ X_i =<br>\begin{bmatrix}<br>x_{1i} \\<br>x_{2i} \\<br>\ldots \\<br>x_{ni}<br>\end{bmatrix} \\<br>W =<br>\begin{bmatrix}<br>w_1 \\<br>w_2 \\<br>w_3 \\<br>b<br>\end{bmatrix}<br>$$</p>
<p>我们的目的是求解 \( W \)，也就是把方程 \(XW = Y \) 中的 \( W \) 解出来。直接解个方程就能得到我们想要的效果，太好了，马上开始：</p>
<p>$$<br>\begin{aligned}<br>&amp;XW = Y \\<br>\Longrightarrow &amp;X^TXW = X^TY \\<br>\Longrightarrow &amp;(X^TX)^{-1}X^TXW = (X^TX)^{-1}X^TY \\<br>\Longrightarrow &amp;W = (X^TX)^{-1}X^TY<br>\end{aligned}<br>$$</p>
<p>如果这里不理解的话，我简单说一下，求逆运算必须是形如 \( \mathbb{R}^{n*n} \) 的形式，也就是行和列要相等。为什么这样呢？我的理解是矩阵是对一个线性空间到另一个线性空间的映射，所以如果一个矩阵的秩小于行数，也就是里面有线性相关的向量，在对线性空间进行变换时，可能进行降维影响，也就是造成某个维度塌缩，这种影响是不可逆的，所以这种矩阵是没有逆矩阵进行恢复的。如果要从一个向量映射到另一个向量，还有从另一个向量映射回来的话，这个矩阵必须有逆矩阵也就是满秩的。还有个东西叫奇异矩阵，感兴趣的可以了解一下。</p>
<p>回到主线，直接求解方程吧。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">X1 = np.asarray([<span class="number">2104</span>, <span class="number">1416</span>, <span class="number">1534</span>, <span class="number">852</span>]).reshape(<span class="number">4</span>, <span class="number">1</span>)</div><div class="line">X2 = np.asarray([<span class="number">5</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>]).reshape(<span class="number">4</span>, <span class="number">1</span>)</div><div class="line">X3 = np.asarray([<span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>]).reshape(<span class="number">4</span>, <span class="number">1</span>)</div><div class="line"></div><div class="line">X = np.mat(np.column_stack((X1, X2, X3, np.ones(shape=(<span class="number">4</span>, <span class="number">1</span>)))))</div><div class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.1</span>, X1.shape)</div><div class="line">Y = np.mat(<span class="number">2.5</span> * X1 - X2 + <span class="number">2</span> * X3 + <span class="number">4</span> + noise)</div><div class="line">YTwin = np.mat(<span class="number">2.5</span> * X1 - X2 + <span class="number">2</span> * X3 + <span class="number">4</span>)</div><div class="line"></div><div class="line">W = (X.T * X).I * X.T * Y</div><div class="line">WTWin = (X.T * X).I * X.T * YTwin</div><div class="line">print(W, <span class="string">"\n"</span>, WTWin)</div><div class="line"></div><div class="line"><span class="comment"># output:</span></div><div class="line"><span class="comment"># [[ 2.50043958]</span></div><div class="line"><span class="comment">#  [-1.16868808]</span></div><div class="line"><span class="comment">#  [ 1.79213736]</span></div><div class="line"><span class="comment">#  [ 4.27637958]] </span></div><div class="line"><span class="comment">#  [[ 2.5]</span></div><div class="line"><span class="comment">#  [-1. ]</span></div><div class="line"><span class="comment">#  [ 2. ]</span></div><div class="line"><span class="comment">#  [ 4. ]]</span></div></pre></td></tr></table></figure>
<p><img src="/2018/02/11/MLLinearRegression/wuenda_2.png" alt=""></p>
<p>这里我们采用吴恩达老师的房子数据，自己生成的数据之间有太大的相似性，算出的结果误差太大。</p>
<p>我们基本可以计算多元线性回归，但是一点也体现不出机器学习中学习这个词，当然我们可以自己利用前面给出的例子利用 \( loss \) 函数求解。这里我们借助 \( TensorFlow \) 帮我们完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">N = <span class="number">1000</span></div><div class="line">train_X1 = np.linspace(<span class="number">1</span>, <span class="number">10</span>, N).reshape(N, <span class="number">1</span>)</div><div class="line">train_X2 = np.linspace(<span class="number">1</span>, <span class="number">10</span>, N).reshape(N, <span class="number">1</span>)</div><div class="line">train_X3 = np.linspace(<span class="number">1</span>, <span class="number">10</span>, N).reshape(N, <span class="number">1</span>)</div><div class="line">train_X4 = np.linspace(<span class="number">1</span>, <span class="number">10</span>, N).reshape(N, <span class="number">1</span>)</div><div class="line"></div><div class="line"><span class="comment"># train_X = np.column_stack((train_X1, np.ones(shape=(N, 1))))</span></div><div class="line">train_X = np.column_stack((train_X1, train_X2, train_X3, train_X4, np.ones(shape=(N, <span class="number">1</span>))))</div><div class="line"></div><div class="line">noise = np.random.normal(<span class="number">0</span>, <span class="number">0.5</span>, train_X1.shape)</div><div class="line"><span class="comment"># train_Y = 3 * train_X1 + 4</span></div><div class="line">train_Y = train_X1 + train_X2 + train_X3 + train_X4 + <span class="number">4</span> + noise</div><div class="line"></div><div class="line">length = len(train_X[<span class="number">0</span>])</div><div class="line"></div><div class="line">X = tf.placeholder(tf.float32, [<span class="keyword">None</span>, length], name=<span class="string">"X"</span>)</div><div class="line">Y = tf.placeholder(tf.float32, [<span class="keyword">None</span>, <span class="number">1</span>], name=<span class="string">"Y"</span>)</div><div class="line"></div><div class="line">W = tf.Variable(np.random.random(size=length).reshape(length, <span class="number">1</span>), dtype=tf.float32, name=<span class="string">"weight"</span>)</div><div class="line"></div><div class="line">activation = tf.matmul(X, W)</div><div class="line">learning_rate = <span class="number">0.006</span></div><div class="line"></div><div class="line">loss = tf.reduce_mean(tf.reduce_sum(tf.pow(activation - Y, <span class="number">2</span>), reduction_indices=[<span class="number">1</span>]))</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)</div><div class="line"></div><div class="line">training_epochs = <span class="number">2000</span></div><div class="line">display_step = <span class="number">100</span></div><div class="line"></div><div class="line">loss_trace = []</div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</div><div class="line">        sess.run(optimizer, feed_dict=&#123;X: train_X, Y: train_Y&#125;)</div><div class="line">        temp_loss = sess.run(loss, feed_dict=&#123;X: train_X, Y: train_Y&#125;)</div><div class="line">        loss_trace.append(temp_loss)</div><div class="line">        <span class="keyword">if</span> <span class="number">1</span> == epoch % display_step:</div><div class="line">            print(<span class="string">'epoch: %4s'</span>%epoch, <span class="string">'\tloss: %s'</span>%temp_loss)</div><div class="line">    print(<span class="string">"\nOptimization Finished!"</span>)</div><div class="line">    print(<span class="string">"\nloss = "</span>, loss_trace[<span class="number">-1</span>], <span class="string">"\nWeight =\n"</span>, sess.run(W, feed_dict=&#123;X: train_X, Y: train_Y&#125;))</div><div class="line"></div><div class="line"></div><div class="line">plt.plot(np.linspace(<span class="number">0</span>, <span class="number">100</span>, <span class="number">100</span>), loss_trace[:<span class="number">100</span>])</div><div class="line">plt.savefig(<span class="string">"tensorflowLR.png"</span>)</div><div class="line">plt.show()</div><div class="line"></div><div class="line"><span class="comment"># output:</span></div><div class="line"><span class="comment"># epoch:    1 	loss: 118.413925</span></div><div class="line"><span class="comment"># epoch:  101 	loss: 1.4500043</span></div><div class="line"><span class="comment"># epoch:  201 	loss: 1.0270562</span></div><div class="line"><span class="comment"># epoch:  301 	loss: 0.75373846</span></div><div class="line"><span class="comment"># epoch:  401 	loss: 0.5771168</span></div><div class="line"><span class="comment"># epoch:  501 	loss: 0.46298113</span></div><div class="line"><span class="comment"># epoch:  601 	loss: 0.38922414</span></div><div class="line"><span class="comment"># epoch:  701 	loss: 0.34156123</span></div><div class="line"><span class="comment"># epoch:  801 	loss: 0.31076077</span></div><div class="line"><span class="comment"># epoch:  901 	loss: 0.29085675</span></div><div class="line"><span class="comment"># epoch: 1001 	loss: 0.27799463</span></div><div class="line"><span class="comment"># epoch: 1101 	loss: 0.26968285</span></div><div class="line"><span class="comment"># epoch: 1201 	loss: 0.2643118</span></div><div class="line"><span class="comment"># epoch: 1301 	loss: 0.26084095</span></div><div class="line"><span class="comment"># epoch: 1401 	loss: 0.2585978</span></div><div class="line"><span class="comment"># epoch: 1501 	loss: 0.25714833</span></div><div class="line"><span class="comment"># epoch: 1601 	loss: 0.25621164</span></div><div class="line"><span class="comment"># epoch: 1701 	loss: 0.2556064</span></div><div class="line"><span class="comment"># epoch: 1801 	loss: 0.2552152</span></div><div class="line"><span class="comment"># epoch: 1901 	loss: 0.2549625</span></div><div class="line"><span class="comment"># Optimization Finished!</span></div><div class="line"><span class="comment"># loss =  0.25480175 </span></div><div class="line"><span class="comment"># Weight =</span></div><div class="line"><span class="comment">#  [[1.0982682 ]</span></div><div class="line"><span class="comment">#  [0.9760315 ]</span></div><div class="line"><span class="comment">#  [1.0619627 ]</span></div><div class="line"><span class="comment">#  [0.87049955]</span></div><div class="line"><span class="comment">#  [3.9700394 ]]</span></div></pre></td></tr></table></figure>
<p>这里的拟合效果不太好，不知道是不是数据的问题，因为数据增长的相似性太高，感觉可能过拟合了，如果有知道的小伙伴欢迎告知。下图可以看到 \( loss \) 早早就收敛了。</p>
<p><img src="/2018/02/11/MLLinearRegression/tensorflowLR.png" alt=""></p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4 总结"></a>4 总结</h2><p>线性回归的求解过程很直观，符合我们的理解。对于这个问题有一个先入为主的观点就是数据一定是拟合成线性的，因为这里讲的是线性回归。当然，数据分布不是线性的这个方法就不适用了，同样，如果我们看不到数据的分布，不能对模型进行预估，那只能一步一步去试探，所以机器学习开始就是选取适用于当前数据的模型，可以理解为找一个输入输出的合理映射关系，然后导入数据，构造一个能够正确评估拟合效果的损失函数，接着就是对损失函数进行最优化，这里有一个问题前面没有提及的是如果只是随机地找，其实我们不能保证找到的就是全局最优，当然一般情况下，局部最优得到的模型已经能很好的预测输出了。这只是我的个人理解啦，欢迎大家一起讨论，然后实验的代码在我的 <a href="https://github.com/mk43/machine-learning" target="_blank" rel="external">GitHub</a> 上，需要的可以自取。然后这个系列可能会继续更新，不过离下一次更新可能要点时间，因为新年快乐^_^，对了参考资料前面又给出，就不一一列举感谢了。</p>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：我将从一个入门者的视角（水平）将机器学习中的常用算法娓娓道来。自身水平确实有限，如果其中有什么错误的话希望大家指出，避免误导大家。对于开篇有比较多的东西想说，所以另取一章介绍一下我个人对机器学习这个领域的理解和一些基本常识介绍。如果你时间比较充裕，可以看看我&lt;code&gt;前言&lt;/code&gt;的文字介绍。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
      <category term="NumPy" scheme="http://fitzeng.org/tags/NumPy/"/>
    
      <category term="Matplotlib" scheme="http://fitzeng.org/tags/Matplotlib/"/>
    
      <category term="LinearRegression" scheme="http://fitzeng.org/tags/LinearRegression/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib 保存 GIF 动图</title>
    <link href="http://fitzeng.org/2018/02/08/MatplotlibGenerateGif/"/>
    <id>http://fitzeng.org/2018/02/08/MatplotlibGenerateGif/</id>
    <published>2018-02-08T03:27:00.000Z</published>
    <updated>2018-02-08T12:55:13.776Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：为了可视化机器学习过程，并且保存下来，所以想直接利用 \( Matplotlib.animation \) 保存动图，期间参考了好多资料，过程比较艰辛，所以想记录下来。当然，此文还参考了好多网上的其它文章，再此一并感谢那些热爱分享的 \( coder \)，并且参考资料中给出链接。所有代码整理到<a href="https://github.com/mk43/python-practice/tree/master/matplotlib" target="_blank" rel="external">GitHub</a>。</p>
</blockquote>
<a id="more"></a>
<h2 id="0-前期准备"><a href="#0-前期准备" class="headerlink" title="0. 前期准备"></a>0. 前期准备</h2><p>安装 \( NumPy \) 和 \( Matplotlib \)。具体安装直接上官网便可，遇到什么问题在网上基本可以搜到答案的，这里就不介绍了。这里要简单的使用 \( NumPy \) 生成一些测试数据，如果对 \( NumPy \) 不熟悉，参考 <a href="http://fitzeng.org/2018/02/04/NumPyOfficialQuickstartTutorial/">NumPy 官方快速入门教程(译)</a>，如果对 \( Matplotlib \) 不熟悉，参考<a href="http://fitzeng.org/2018/02/07/MatplotlibDraw/">Matplotlib 基本操作</a>。当然如果要能够保存 \( GIF \) 还需要一个工具 <a href="http://www.imagemagick.org/script/download.php#macosx" target="_blank" rel="external">ImageMagick</a>，按照官方指导安装就好，确保在命令中输入 <code>magick</code> 有响应。如果使用 \( PyCharm \) 之类的 \( IDE \) 请将环境变量配置到全局，避免在 \( IDE \) 中找不到命令。</p>
<h2 id="1-绘制基本动图"><a href="#1-绘制基本动图" class="headerlink" title="1. 绘制基本动图"></a>1. 绘制基本动图</h2><p>请确保已经安装了 <a href="http://www.imagemagick.org/script/download.php#macosx" target="_blank" rel="external">ImageMagick</a> 并且可用的情况下再继续，不然代码跑步起来。</p>
<p>这里采用两种方式绘制动图</p>
<h3 id="1-1-重置重绘"><a href="#1-1-重置重绘" class="headerlink" title="1.1 重置重绘"></a>1.1 重置重绘</h3><p>重置重绘主要是每次更新原来图形的值来达到绘制动图的效果。</p>
<ul>
<li>导入基本库</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">from matplotlib import animation</div></pre></td></tr></table></figure>
<ul>
<li>生成数据，画出原始图</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">fig, ax = plt.subplots()</div><div class="line"></div><div class="line">x = np.arange(0, 2 * np.pi, 0.01)</div><div class="line">line0 = ax.plot(x, np.cos(x))</div><div class="line">line, = ax.plot(x, np.sin(x))</div></pre></td></tr></table></figure>
<p>注意，这里申明的 \( line, \) 中 <code>,</code> 不能少，好像是为了更新值时类型匹配。没深究，希望知道的可以指点一下。</p>
<ul>
<li>定义初始函数和跟新函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">def init():</div><div class="line">    line.set_ydata(np.sin(x))</div><div class="line">    return line,</div><div class="line"></div><div class="line">def animate(i):</div><div class="line">    line.set_ydata(np.sin(x + i / 10.0))</div><div class="line">    return line,</div></pre></td></tr></table></figure>
<p>其实就是更新一下 \( Y \) 坐标的值。</p>
<ul>
<li>执行动画</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">animation = animation.FuncAnimation(fig=fig, func=animate, frames=100, init_func=init, interval=20, blit=False)</div></pre></td></tr></table></figure>
<p>这个函数的参数可以看源码，以及官网的介绍，这里就是每个 \( 20\ ms \) 绘制一帧，总共有 \( 100 \) 帧。</p>
<ul>
<li>保存 \( GIF \)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">animation.save(&apos;resetvalue.gif&apos;, writer=&apos;imagemagick&apos;)</div></pre></td></tr></table></figure>
<p>这里就是直接保存成 \( GIF \) 格式就好了。</p>
<ul>
<li>显示动图</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>生成图片的效果如图所示：</p>
<p><img src="/2018/02/08/MatplotlibGenerateGif/resetvalue.gif" alt=""></p>
<h3 id="1-2-擦除重绘"><a href="#1-2-擦除重绘" class="headerlink" title="1.2 擦除重绘"></a>1.2 擦除重绘</h3><p>与上一种方法比较，这种就是不利用上一次的任何坐标，直接擦除，然后再 \( plot \) 图形上去。</p>
<ul>
<li>导入基本库</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">from matplotlib import animation</div></pre></td></tr></table></figure>
<ul>
<li>生成数据，画出原始图</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">fig, ax = plt.subplots()</div><div class="line"></div><div class="line">x = np.arange(0, 2 * np.pi, 0.01)</div><div class="line">ax.plot(x, np.cos(x))</div></pre></td></tr></table></figure>
<p>这里就没有那个要求了，因为这种方式不依赖于前面的图形。</p>
<ul>
<li>定义初始函数和跟新函数</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">def init():</div><div class="line">    return ax.plot(x, np.sin(x))</div><div class="line"></div><div class="line"></div><div class="line">def animate(i):</div><div class="line">    try:</div><div class="line">        ax.lines.pop(1)</div><div class="line">    except Exception:</div><div class="line">        pass</div><div class="line">    line = ax.plot(x, np.sin(x + i / 10.0), &apos;r&apos;)</div><div class="line">    return line,</div></pre></td></tr></table></figure>
<p>初始化没什么好说的，其实也可以不初始化，时间间隔太短效果基本是看不出来的。下面介绍一下 <code>ax.lines.pop(1)</code> 这句“擦除”函数。这里的 \( lines \)可以理解为存储 \( plot \) 上来的图像栈，前面 \( plot \) 了一个余弦函数，在初始化的时候绘制了第二条，所以索引是 \( 1 \) 的正弦函数被 \( pop \) 了然后进行下一条绘制。于是执行 <code>line = ax.plot(x, np.sin(x + i / 10.0), &#39;r&#39;)</code></p>
<ul>
<li>后续</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">animation = animation.FuncAnimation(fig=fig, func=animate, frames=100, init_func=init, interval=20, blit=False)</div><div class="line">animation.save(&apos;redraw.gif&apos;, writer=&apos;imagemagick&apos;)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>没区别</p>
<p>生成图片的效果如图所示：</p>
<p><img src="/2018/02/08/MatplotlibGenerateGif/redraw.gif" alt=""></p>
<h2 id="2-机器学习过程可视化"><a href="#2-机器学习过程可视化" class="headerlink" title="2. 机器学习过程可视化"></a>2. 机器学习过程可视化</h2><p>前面介绍了的知识基本够用了，但终究不是实操。如果你不是学习机器学习的，其实有上面的基础就可以了，这一节可以跳过。但是，如果你想学机器学习的话，这里提供一个小例子让你更加清晰的理解机器学习的过程中的数据变化。不过这里只专注于绘制，机器学习的部分参考<a href="http://fitzeng.org/2018/02/03/TensorFlowIntroduction/">从 TensorFlow 入门机器学习</a></p>
<p>同样还是拿线性回归作为例子。</p>
<p>原始代码</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line"># coding: utf-8</div><div class="line">from __future__ import print_function</div><div class="line">import tensorflow as tf</div><div class="line">import numpy as np</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">import matplotlib.animation as animation</div><div class="line">from scipy.interpolate import spline</div><div class="line"></div><div class="line">train_X = np.linspace(0, 10, 50)</div><div class="line">noise = np.random.normal(0, 1, train_X.shape)</div><div class="line">train_Y = train_X * 1 - 2 + noise</div><div class="line"></div><div class="line">X = tf.placeholder(tf.float32)</div><div class="line">Y = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">W = tf.Variable(-1., name=&quot;weight&quot;)</div><div class="line">b = tf.Variable(1., name=&quot;bias&quot;)</div><div class="line"></div><div class="line">activation = tf.add(tf.multiply(X, W), b)</div><div class="line"></div><div class="line">learning_rate = 0.0001</div><div class="line"></div><div class="line">cost = tf.reduce_sum(tf.pow(activation - Y, 2))</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</div><div class="line"></div><div class="line">training_epochs = 20</div><div class="line">display_step = 10</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    for epoch in range(training_epochs):</div><div class="line">        for (x, y) in zip(train_X, train_Y):</div><div class="line">            sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;)</div><div class="line">        if epoch &lt; 10 or epoch % display_step == 0:</div><div class="line">            c_tmp = sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;)</div><div class="line">            W_tmp = sess.run(W)</div><div class="line">            b_tmp = sess.run(b)</div><div class="line">            activation_tmp = sess.run(activation, feed_dict=&#123;X: train_X&#125;)</div><div class="line">            print(&quot;Epoch: %04d&quot; % (epoch + 1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(c_tmp), &quot;W=&quot;, W_tmp, &quot;b=&quot;, b_tmp)</div><div class="line">    print(&quot;Optimization Finished!&quot;)</div><div class="line">    print(&quot;cost=&quot;, sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;), &quot;W=&quot;, sess.run(W), &quot;b=&quot;, sess.run(b))</div></pre></td></tr></table></figure>
<p>上面的代码就不解释了，为了方便测试，把迭代次数调的比较小。接下来我们在上面的基础上进行扩充。</p>
<p>首先进行可视化，首先把我们觉得有用的数据提取出来吧。因为经过测试，前面的变化幅度比较大，为了图示明显，刻意进行非均匀采样。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">c_trace = []</div><div class="line">W_trace = []</div><div class="line">b_trace = []</div><div class="line">activation_trace = []</div><div class="line"></div><div class="line">with tf.Session() as sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    for epoch in range(training_epochs):</div><div class="line">        for (x, y) in zip(train_X, train_Y):</div><div class="line">            sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;)</div><div class="line">        if epoch &lt; 10 or epoch % display_step == 0:</div><div class="line">            c_tmp = sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;)</div><div class="line">            W_tmp = sess.run(W)</div><div class="line">            b_tmp = sess.run(b)</div><div class="line">            activation_tmp = sess.run(activation, feed_dict=&#123;X: train_X&#125;)</div><div class="line">            print(&quot;Epoch: %04d&quot; % (epoch + 1), &quot;cost=&quot;, &quot;&#123;:.9f&#125;&quot;.format(c_tmp), &quot;W=&quot;, W_tmp, &quot;b=&quot;, b_tmp)</div><div class="line">            c_trace.append(c_tmp)</div><div class="line">            W_trace.append(W_tmp)</div><div class="line">            b_trace.append(b_tmp)</div><div class="line">            activation_trace.append(activation_tmp)</div><div class="line">    print(&quot;Optimization Finished!&quot;)</div><div class="line">    print(&quot;cost=&quot;, sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;), &quot;W=&quot;, sess.run(W), &quot;b=&quot;, sess.run(b))</div></pre></td></tr></table></figure>
<p>参考前面的小例子，把数据填进去，做出动图来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">fig, ax = plt.subplots()</div><div class="line">l1 = ax.scatter(train_X, train_Y, color=&apos;red&apos;, label=r&apos;$Original\ data$&apos;)</div><div class="line">ax.set_xlabel(r&apos;$X\ data$&apos;)</div><div class="line">ax.set_ylabel(r&apos;$Y\ data$&apos;)</div><div class="line"></div><div class="line"></div><div class="line">def update(i):</div><div class="line">    try:</div><div class="line">        ax.lines.pop(0)</div><div class="line">    except Exception:</div><div class="line">        pass</div><div class="line">    line, = ax.plot(train_X, activation_trace[i], &apos;g--&apos;, label=r&apos;$Fitting\ line$&apos;, lw=2)</div><div class="line">    return line,</div><div class="line"></div><div class="line"></div><div class="line">ani = animation.FuncAnimation(fig, update, frames=len(activation_trace), interval=100)</div><div class="line">ani.save(&apos;linearregression.gif&apos;, writer=&apos;imagemagick&apos;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>效果如下图所示：</p>
<p><img src="/2018/02/08/MatplotlibGenerateGif/linearregression1.gif" alt=""></p>
<p>接着把 \( Cost \) 函数也加上来并且在最后显示。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">def update(i):</div><div class="line">    try:</div><div class="line">        ax.lines.pop(0)</div><div class="line">    except Exception:</div><div class="line">        pass</div><div class="line">    line, = ax.plot(train_X, activation_trace[i], &apos;g--&apos;, label=r&apos;$Fitting\ line$&apos;, lw=2)</div><div class="line">    if i == len(activation_trace) - 1:</div><div class="line">        twinax = ax.twinx()</div><div class="line">        twinax.plot(np.linspace(0, 10, np.size(c_trace)), c_trace, &apos;b&apos;, label=&apos;Cost line&apos;, lw=2)</div><div class="line">    return line,</div></pre></td></tr></table></figure>
<p><img src="/2018/02/08/MatplotlibGenerateGif/linearregression2.gif" alt=""><br><img src="/2018/02/08/MatplotlibGenerateGif/linearregression2.png" alt=""></p>
<p>可以看到，线条十分锋利，这时就可以使用 \( spline \)平滑过渡一下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">def update(i):</div><div class="line">    try:</div><div class="line">        ax.lines.pop(0)</div><div class="line">    except Exception:</div><div class="line">        pass</div><div class="line">    line, = ax.plot(train_X, activation_trace[i], &apos;g--&apos;, label=r&apos;$Fitting\ line$&apos;, lw=2)</div><div class="line">    if i == len(activation_trace) - 1:</div><div class="line">        xnew = np.linspace(0, 10, np.max(c_trace) - np.min(c_trace))</div><div class="line">        smooth = spline(np.linspace(0, 10, np.size(c_trace)), c_trace, xnew)</div><div class="line">        twinax = ax.twinx()</div><div class="line">        twinax.set_ylabel(r&apos;Cost&apos;)</div><div class="line">        twinax.plot(xnew, smooth, &apos;b&apos;, label=r&apos;$Cost\ line$&apos;, lw=2)</div><div class="line">    return line,</div></pre></td></tr></table></figure>
<p>其实就是对 \( [0, 10] \) 这个区间进行采样。添加 <code>np.max(c_trace) - np.min(c_trace)</code> 个点来绘制这线条。</p>
<p><img src="/2018/02/08/MatplotlibGenerateGif/linearregression3.png" alt=""></p>
<p>加上图例。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">def update(i):</div><div class="line">    try:</div><div class="line">        ax.lines.pop(0)</div><div class="line">    except Exception:</div><div class="line">        pass</div><div class="line">    line, = ax.plot(train_X, activation_trace[i], &apos;g--&apos;, label=r&apos;$Fitting\ line$&apos;, lw=2)</div><div class="line">    plt.legend(handles=[l1, line], loc=&apos;upper center&apos;)</div><div class="line">    if i == len(activation_trace) - 1:</div><div class="line">        ax.text(6, -2, &apos;Cost: %s&apos; % c_trace[i], fontdict=&#123;&apos;size&apos;: 16, &apos;color&apos;: &apos;r&apos;&#125;)</div><div class="line">        xnew = np.linspace(0, 10, np.max(c_trace) - np.min(c_trace))</div><div class="line">        smooth = spline(np.linspace(0, 10, np.size(c_trace)), c_trace, xnew)</div><div class="line">        twinax = ax.twinx()</div><div class="line">        twinax.set_ylabel(r&apos;Cost&apos;)</div><div class="line">        costline, = twinax.plot(xnew, smooth, &apos;b&apos;, label=r&apos;$Cost\ line$&apos;, lw=2)</div><div class="line">        plt.legend(handles=[l1, line, costline], loc=&apos;upper center&apos;)</div><div class="line">    return line,</div></pre></td></tr></table></figure>
<p><img src="/2018/02/08/MatplotlibGenerateGif/linearregression3.gif" alt=""><br><img src="/2018/02/08/MatplotlibGenerateGif/linearregression4.png" alt=""></p>
<p>下面把数据细节处理下。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">learning_rate = 0.001</div><div class="line"></div><div class="line">training_epochs = 500</div><div class="line">display_step = 40</div></pre></td></tr></table></figure>
<p><img src="/2018/02/08/MatplotlibGenerateGif/linearregression5.png" alt=""></p>
<p>可以看到，\( Cost \) 函数并非严格递减的，我们采用的是梯度下降算法求最优，所以问题出在学习率，具体为什么也是一个机器学习中应该注意的问题。另外大家还可以试试继续把学习率调大看看会发生什么有趣的事情？</p>
<p>我们把学习率调整到 <code>0.0001</code> 将会得到以下结果：</p>
<p><img src="/2018/02/08/MatplotlibGenerateGif/linearregression4.gif" alt=""><br><img src="/2018/02/08/MatplotlibGenerateGif/linearregression6.png" alt=""></p>
<p>其实你观察输出可能并不怎么符合原始函数。而且在不断调整训练参数的时候会发现拟合程度似乎也没法每次后很好。原因其实在于加的干扰，至于为什么干扰会造成这样，就不在本文的讨论范围了。好了，到这里你应该可以绘制自己的 \( GIF \)了吧。</p>
<p>源码详见 <a href="https://github.com/mk43/python-practice/tree/master/matplotlib" target="_blank" rel="external">GitHub</a></p>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h2><p>花时间把小细节搞懂就是节省时间，其实眼前的问题可以引申出其它更多值得思考的问题。多联系，多思考。</p>
<h2 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4. 参考资料"></a>4. 参考资料</h2><ul>
<li><a href="https://morvanzhou.github.io/tutorials/data-manipulation/plt/" target="_blank" rel="external">morvan：markdown + 视频</a></li>
<li><a href="https://matplotlib.org/tutorials/index.html" target="_blank" rel="external">官方文档</a></li>
<li><a href="https://www.jianshu.com/p/aa4150cf6c7f" target="_blank" rel="external">Matplotlib 入门教程</a></li>
<li><a href="https://www.jianshu.com/p/aa4150cf6c7f" target="_blank" rel="external">Matplotlib 教程</a></li>
<li><a href="https://www.tuicool.com/articles/Z7BzY3V" target="_blank" rel="external">如何用 Matplotlib 画 GIF 动图</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：为了可视化机器学习过程，并且保存下来，所以想直接利用 \( Matplotlib.animation \) 保存动图，期间参考了好多资料，过程比较艰辛，所以想记录下来。当然，此文还参考了好多网上的其它文章，再此一并感谢那些热爱分享的 \( coder \)，并且参考资料中给出链接。所有代码整理到&lt;a href=&quot;https://github.com/mk43/python-practice/tree/master/matplotlib&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GitHub&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
      <category term="NumPy" scheme="http://fitzeng.org/tags/NumPy/"/>
    
      <category term="Matplotlib" scheme="http://fitzeng.org/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>Matplotlib 基本操作</title>
    <link href="http://fitzeng.org/2018/02/07/MatplotlibDraw/"/>
    <id>http://fitzeng.org/2018/02/07/MatplotlibDraw/</id>
    <published>2018-02-07T03:27:00.000Z</published>
    <updated>2018-02-13T02:43:09.248Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：这篇主要为了自己的学习，先理一下基础概念，然后记录下来，下次使用起来就方便了。学这个的主要目的是为了可视化机器学习的学习过程。网络上的资料太多了，每次写的时候去查算起来也是挺花费时间的。所以还是花点时间直接整理记录好了。这里推荐<a href="https://morvanzhou.github.io/tutorials/data-manipulation/plt/" target="_blank" rel="external">morvan：markdown + 视频</a> 和 <a href="https://matplotlib.org/tutorials/index.html" target="_blank" rel="external">官方文档</a>。当然，此文还参考了好多网上的其它文章，再此一并感谢，并且参考资料中给出链接。所有代码整理到<a href="https://github.com/mk43/python-practice/tree/master/matplotlib" target="_blank" rel="external">GitHub</a>。</p>
</blockquote>
<a id="more"></a>
<h2 id="0-前期准备"><a href="#0-前期准备" class="headerlink" title="0. 前期准备"></a>0. 前期准备</h2><p>安装 \( NumPy \) 和 \( Matplotlib \)。具体安装直接上官网便可，遇到什么问题在网上基本可以搜到答案的，这里就不介绍了。这里要简单的使用 \( NumPy \) 生成一些测试数据，如果对 \( NumPy \) 不熟悉的话可以参考我之前记录的 <a href="http://fitzeng.org/2018/02/04/NumPyOfficialQuickstartTutorial/">NumPy 官方快速入门教程(译)</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">PyCharm: 2017.3.3</div><div class="line">Python: Python 3.6.4 :: Anaconda, Inc.</div></pre></td></tr></table></figure>
<h2 id="1-Matplotlib-快速入门"><a href="#1-Matplotlib-快速入门" class="headerlink" title="1. Matplotlib 快速入门"></a>1. Matplotlib 快速入门</h2><p>主要内容是把图画出来，然后认识图中的基本元素。</p>
<h3 id="1-1-显示图像"><a href="#1-1-显示图像" class="headerlink" title="1.1 显示图像"></a>1.1 显示图像</h3><p>直接上代码吧。整个过程就是<code>导库-准备数据-绘制-显示</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">x = np.linspace(0, 10, 20)</div><div class="line">y = x + 1</div><div class="line"></div><div class="line">plt.plot(x, y)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_1_01.png" alt=""></p>
<p>那么显示多条线怎么办？</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">x = np.linspace(0, 10, 20)</div><div class="line">y1 = x + 1</div><div class="line">y2 = -x + 1</div><div class="line"></div><div class="line">plt.plot(x, y1)</div><div class="line">plt.plot(x, y2)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_1_02.png" alt=""></p>
<p>看到这里，我们就很清楚了，\( plot \) 函数就是把数据绘制在画框里。</p>
<h3 id="1-2-认识-figure"><a href="#1-2-认识-figure" class="headerlink" title="1.2 认识 figure"></a>1.2 认识 figure</h3><p>\( figure \) 可以理解为一个画框，往里面 \( plot \) 图形。所以以下代码会显示出两个窗体。一个窗体比例是 \( 9 : 6 \)。更多参数可以参考源码注释，讲的很清楚。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">x = np.linspace(0, 10, 20)</div><div class="line">y1 = x + 1</div><div class="line">y2 = -x - 10</div><div class="line">plt.figure()</div><div class="line">plt.plot(x, y1)</div><div class="line">plt.figure(figsize=(9, 6))</div><div class="line">plt.plot(x, y2)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_2_01.png" alt=""><br><img src="/2018/02/07/MatplotlibDraw/1_2_02.png" alt=""></p>
<h3 id="1-3-plot-参数设置"><a href="#1-3-plot-参数设置" class="headerlink" title="1.3 plot 参数设置"></a>1.3 plot 参数设置</h3><p>先看下一些效果吧。<br><img src="/2018/02/07/MatplotlibDraw/1_3_01.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">x = np.linspace(0, 10, 20)</div><div class="line">y1 = x + 1</div><div class="line">y2 = x + 2</div><div class="line">y3 = x + 3</div><div class="line">y4 = x + 4</div><div class="line">y5 = x + 5</div><div class="line">y6 = x + 6</div><div class="line">y7 = x + 7</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.plot(x, y1, &apos;bo&apos;)</div><div class="line">plt.plot(x, y2, &apos;r-&apos;)</div><div class="line">plt.plot(x, y3, &apos;g--&apos;)</div><div class="line">plt.plot(x, y4, &apos;y.-&apos;)</div><div class="line">plt.plot(x, y5, &apos;m^&apos;, x, y6, &apos;m-&apos;)</div><div class="line">plt.plot(x, y7, &apos;c-&apos;, linewidth=6)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>这里可供实验的东西实在太多，只要知道这个函数大概可以给我提供什么效果就可以了，其它的交给<a href="https://matplotlib.org/api/_as_gen/matplotlib.pyplot.plot.html#matplotlib.pyplot.plot" target="_blank" rel="external">API: matplotlib.pyplot.plot</a></p>
<h3 id="1-4-坐标轴设置"><a href="#1-4-坐标轴设置" class="headerlink" title="1.4 坐标轴设置"></a>1.4 坐标轴设置</h3><p>设置范围和标签</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">x = np.linspace(-10, 10, 40)</div><div class="line">y1 = 10 * x + 50</div><div class="line">y2 = x**2</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.plot(x, y1, &apos;b-&apos;)</div><div class="line">plt.plot(x, y2, &apos;b--&apos;)</div><div class="line">plt.xlim((-20, 20))</div><div class="line">plt.ylim((-60, 160))</div><div class="line">plt.xlabel(&apos;I am x&apos;)</div><div class="line">plt.ylabel(&apos;I am y&apos;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_4_01.png" alt=""></p>
<p>设置刻度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">plt.xticks(np.linspace(-20, 20, 5))</div><div class="line">plt.yticks([0, 50, 100], [r&apos;$bad$&apos;, r&apos;$normal$&apos;, r&apos;$good$&apos;])</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_4_02.png" alt=""></p>
<p>设置边框<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">boderparameter = plt.gca()</div><div class="line">boderparameter.spines[&apos;right&apos;].set_color(&apos;none&apos;)</div><div class="line">boderparameter.spines[&apos;top&apos;].set_color(&apos;none&apos;)</div></pre></td></tr></table></figure></p>
<p><img src="/2018/02/07/MatplotlibDraw/1_4_03.png" alt=""></p>
<p>设置刻度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">boderparameter.xaxis.set_ticks_position(&apos;top&apos;)</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_4_04.png" alt=""></p>
<p>通过边框参数设置属性</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">boderparameter.spines[&apos;left&apos;].set_position((&apos;data&apos;,0))</div><div class="line">boderparameter.spines[&apos;bottom&apos;].set_position((&apos;data&apos;,0))</div><div class="line">boderparameter.xaxis.set_ticks_position(&apos;bottom&apos;)</div><div class="line">boderparameter.set_xlabel(&apos;&apos;)</div><div class="line">boderparameter.set_ylabel(&apos;&apos;)</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_4_05.png" alt=""></p>
<h3 id="1-5-图例设置"><a href="#1-5-图例设置" class="headerlink" title="1.5 图例设置"></a>1.5 图例设置</h3><p>这里很简单，设置 \( label \) 和 位置就可以了。其它可以参考文档。需要注意的一点是 \( l1\ l2 \)后面要加一个 <code>,</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">x = np.linspace(-10, 10, 40)</div><div class="line">y1 = 10 * x + 50</div><div class="line">y2 = x**2</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">l1, = plt.plot(x, y1, &apos;b-&apos;)</div><div class="line">l2, = plt.plot(x, y2, &apos;b--&apos;)</div><div class="line"></div><div class="line">plt.legend(handles=[l1, l2], labels=[r&apos;$line\ 1$&apos;, r&apos;$line\ 2$&apos;],  loc=&apos;best&apos;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_5_01.png" alt=""></p>
<h3 id="1-6-添加注释"><a href="#1-6-添加注释" class="headerlink" title="1.6 添加注释"></a>1.6 添加注释</h3><p>直接看代码，就不一一介绍了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">x = np.linspace(-10, 10, 40)</div><div class="line">y = x + 5</div><div class="line"></div><div class="line">plt.figure()</div><div class="line">plt.plot(x, y, &apos;b-&apos;)</div><div class="line"></div><div class="line">plt.xticks(np.linspace(-10, 10, 6))</div><div class="line">plt.yticks(np.linspace(-6, 14, 6))</div><div class="line"></div><div class="line">ax = plt.gca()</div><div class="line">ax.spines[&apos;right&apos;].set_color(&apos;none&apos;)</div><div class="line">ax.spines[&apos;top&apos;].set_color(&apos;none&apos;)</div><div class="line"></div><div class="line">ax.xaxis.set_ticks_position(&apos;bottom&apos;)</div><div class="line">ax.spines[&apos;bottom&apos;].set_position(&apos;zero&apos;)</div><div class="line"></div><div class="line">ax.yaxis.set_ticks_position(&apos;left&apos;)</div><div class="line">ax.spines[&apos;left&apos;].set_position(&apos;zero&apos;)</div><div class="line"></div><div class="line">x0 = 4</div><div class="line">y0 = x0 + 5</div><div class="line">plt.plot([x0, x0], [0, y0], &apos;k--&apos;, linewidth=2.5)</div><div class="line">plt.scatter([x0], [y0], color=&apos;k&apos;)</div><div class="line"></div><div class="line">plt.annotate(r&apos;$(%s,\ %s)$&apos; % (x0, y0), xy=(x0, y0),</div><div class="line">             xycoords=&apos;data&apos;, xytext=(+30, -30),</div><div class="line">             textcoords=&apos;offset points&apos;, fontsize=16,</div><div class="line">             arrowprops=dict(arrowstyle=&apos;-&gt;&apos;, connectionstyle=&quot;arc3,rad=.1&quot;))</div><div class="line"></div><div class="line">plt.text(2, 14, r&apos;$y\ =\ x\ +\ 5$&apos;, fontdict=&#123;&apos;size&apos;: 16, &apos;color&apos;: &apos;r&apos;&#125;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p>主要是观察倒数第二三行代码进行设置。</p>
<p><img src="/2018/02/07/MatplotlibDraw/1_6_01.png" alt=""></p>
<h3 id="1-7-散点图"><a href="#1-7-散点图" class="headerlink" title="1.7 散点图"></a>1.7 散点图</h3><p>散点图在上面标点的时候就已经用过了，这里回顾一下。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">n = 1024</div><div class="line">X = np.random.normal(0, 1, n)</div><div class="line">Y = np.random.normal(0, 1, n)</div><div class="line">T = np.arctan2(Y, X)</div><div class="line"></div><div class="line">plt.scatter(X, Y, s=50, c=T, alpha=.5)</div><div class="line"></div><div class="line">plt.xlim(-2.5, 2.5)</div><div class="line">plt.xticks(())</div><div class="line">plt.ylim(-2.5, 2.5)</div><div class="line">plt.yticks(())</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure></p>
<p><img src="/2018/02/07/MatplotlibDraw/1_7_01.png" alt=""></p>
<h3 id="1-8-柱状图"><a href="#1-8-柱状图" class="headerlink" title="1.8 柱状图"></a>1.8 柱状图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">n = 12</div><div class="line">X = np.arange(n)</div><div class="line">Y1 = np.random.uniform(0.1, 1.0, n)</div><div class="line">Y2 = np.random.uniform(0.1, 1.0, n)</div><div class="line"></div><div class="line">plt.bar(X, +Y1)</div><div class="line">plt.bar(X, -Y2)</div><div class="line"></div><div class="line">plt.xlim(-.5, n)</div><div class="line">plt.xticks(())</div><div class="line">plt.ylim(-1.25, 1.25)</div><div class="line">plt.yticks(())</div><div class="line"></div><div class="line">for x, y in zip(X, Y1):</div><div class="line">    plt.text(x, y + 0.02, &apos;%.2f&apos; % y, ha=&apos;center&apos;, va=&apos;bottom&apos;)</div><div class="line"></div><div class="line">for x, y in zip(X, Y2):</div><div class="line">    plt.text(x, -y - 0.02, &apos;%.2f&apos; % y, ha=&apos;center&apos;, va=&apos;top&apos;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_8_01.png" alt=""></p>
<h3 id="1-9-饼状图"><a href="#1-9-饼状图" class="headerlink" title="1.9 饼状图"></a>1.9 饼状图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">labels = &apos;Frogs&apos;, &apos;Hogs&apos;, &apos;Dogs&apos;, &apos;Logs&apos;</div><div class="line">sizes = [15, 30, 45, 10]</div><div class="line">explode = (0, 0.1, 0, 0)</div><div class="line">colors = [&apos;y&apos;, &apos;g&apos;, &apos;c&apos;, &apos;m&apos;]</div><div class="line"></div><div class="line">fig1, ax1 = plt.subplots()</div><div class="line">ax1.pie(sizes, colors=colors, explode=explode, labels=labels, autopct=&apos;%1.f%%&apos;, shadow=True, startangle=90)</div><div class="line">ax1.axis(&apos;equal&apos;)  # Equal aspect ratio ensures that pie is drawn as a circle.</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_9_01.png" alt=""></p>
<h3 id="1-10-等高线"><a href="#1-10-等高线" class="headerlink" title="1.10 等高线"></a>1.10 等高线</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line"></div><div class="line">def f(x, y):</div><div class="line">    return (1 - x / 2 + x ** 5 + y ** 3) * np.exp(-x ** 2 - y ** 2)</div><div class="line"></div><div class="line"></div><div class="line">n = 256</div><div class="line">x = np.linspace(-3, 3, n)</div><div class="line">y = np.linspace(-3, 3, n)</div><div class="line">X, Y = np.meshgrid(x, y)</div><div class="line"></div><div class="line">plt.contourf(X, Y, f(X, Y), 12, alpha=.9, cmap=plt.cm.coolwarm)</div><div class="line"></div><div class="line">C = plt.contour(X, Y, f(X, Y), 12, colors=&apos;black&apos;)</div><div class="line"></div><div class="line">plt.clabel(C, inline=True, fontsize=10)</div><div class="line">plt.xticks(())</div><div class="line">plt.yticks(())</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_10_01.png" alt=""></p>
<h3 id="1-11-图像"><a href="#1-11-图像" class="headerlink" title="1.11 图像"></a>1.11 图像</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">a = np.random.normal(2, 1, 16).reshape(4, 4)</div><div class="line"></div><div class="line">plt.imshow(a, interpolation=&apos;nearest&apos;, cmap=&apos;coolwarm&apos;, origin=&apos;lower&apos;)</div><div class="line"></div><div class="line">plt.colorbar(shrink=.98)</div><div class="line"></div><div class="line">plt.xticks(())</div><div class="line">plt.yticks(())</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_11_01.png" alt=""></p>
<h3 id="1-12-3D"><a href="#1-12-3D" class="headerlink" title="1.12 3D"></a>1.12 3D</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import matplotlib.pyplot as plt</div><div class="line">from mpl_toolkits.mplot3d import Axes3D</div><div class="line"></div><div class="line">fig = plt.figure()</div><div class="line">ax = Axes3D(fig)</div><div class="line"></div><div class="line">X = np.arange(-4, 4, 0.25)</div><div class="line">Y = np.arange(-4, 4, 0.25)</div><div class="line">X, Y = np.meshgrid(X, Y)</div><div class="line">R = np.sqrt(X ** 2 + Y ** 2)</div><div class="line">Z = np.sin(R)</div><div class="line"></div><div class="line">ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.get_cmap(&apos;rainbow&apos;))</div><div class="line"></div><div class="line">ax.contourf(X, Y, Z, zdir=&apos;x&apos;, offset=-5, cmap=plt.get_cmap(&apos;rainbow&apos;))</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_12_01.png" alt=""></p>
<h3 id="1-13-子图"><a href="#1-13-子图" class="headerlink" title="1.13 子图"></a>1.13 子图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line"># example 1:</div><div class="line">###############################</div><div class="line">plt.figure(figsize=(6, 4))</div><div class="line"># plt.subplot(n_rows, n_cols, plot_num)</div><div class="line">plt.subplot(2, 2, 1)</div><div class="line">plt.plot([0, 1], [0, 1])</div><div class="line"></div><div class="line">plt.subplot(222)</div><div class="line">plt.plot([0, 1], [0, 2])</div><div class="line"></div><div class="line">plt.subplot(223)</div><div class="line">plt.plot([0, 1], [0, 3])</div><div class="line"></div><div class="line">plt.subplot(224)</div><div class="line">plt.plot([0, 1], [0, 4])</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line"></div><div class="line"># example 2:</div><div class="line">###############################</div><div class="line">plt.figure(figsize=(6, 4))</div><div class="line"># plt.subplot(n_rows, n_cols, plot_num)</div><div class="line">plt.subplot(2, 1, 1)</div><div class="line"># figure splits into 2 rows, 1 col, plot to the 1st sub-fig</div><div class="line">plt.plot([0, 1], [0, 1])</div><div class="line"></div><div class="line">plt.subplot(234)</div><div class="line"># figure splits into 2 rows, 3 col, plot to the 4th sub-fig</div><div class="line">plt.plot([0, 1], [0, 2])</div><div class="line"></div><div class="line">plt.subplot(235)</div><div class="line"># figure splits into 2 rows, 3 col, plot to the 5th sub-fig</div><div class="line">plt.plot([0, 1], [0, 3])</div><div class="line"></div><div class="line">plt.subplot(236)</div><div class="line"># figure splits into 2 rows, 3 col, plot to the 6th sub-fig</div><div class="line">plt.plot([0, 1], [0, 4])</div><div class="line"></div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_13_01.png" alt=""><br><img src="/2018/02/07/MatplotlibDraw/1_13_02.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import matplotlib.gridspec as gridspec</div><div class="line"></div><div class="line"># method 1: subplot2grid</div><div class="line">##########################</div><div class="line">plt.figure()</div><div class="line">ax1 = plt.subplot2grid((3, 3), (0, 0), colspan=3)  # stands for axes</div><div class="line">ax1.plot([1, 2], [1, 2])</div><div class="line">ax1.set_title(&apos;ax1_title&apos;)</div><div class="line">ax2 = plt.subplot2grid((3, 3), (1, 0), colspan=2)</div><div class="line">ax3 = plt.subplot2grid((3, 3), (1, 2), rowspan=2)</div><div class="line">ax4 = plt.subplot2grid((3, 3), (2, 0))</div><div class="line">ax4.scatter([1, 2], [2, 2])</div><div class="line">ax4.set_xlabel(&apos;ax4_x&apos;)</div><div class="line">ax4.set_ylabel(&apos;ax4_y&apos;)</div><div class="line">ax5 = plt.subplot2grid((3, 3), (2, 1))</div><div class="line"></div><div class="line"># method 2: gridspec</div><div class="line">#########################</div><div class="line">plt.figure()</div><div class="line">gs = gridspec.GridSpec(3, 3)</div><div class="line"># use index from 0</div><div class="line">ax6 = plt.subplot(gs[0, :])</div><div class="line">ax7 = plt.subplot(gs[1, :2])</div><div class="line">ax8 = plt.subplot(gs[1:, 2])</div><div class="line">ax9 = plt.subplot(gs[-1, 0])</div><div class="line">ax10 = plt.subplot(gs[-1, -2])</div><div class="line"></div><div class="line"># method 3: easy to define structure</div><div class="line">####################################</div><div class="line">f, ((ax11, ax12), (ax13, ax14)) = plt.subplots(2, 2, sharex=True, sharey=True)</div><div class="line">ax11.scatter([1,2], [1,2])</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_13_03.png" alt=""><br><img src="/2018/02/07/MatplotlibDraw/1_13_04.png" alt=""><br><img src="/2018/02/07/MatplotlibDraw/1_13_05.png" alt=""></p>
<h3 id="1-14-图中图"><a href="#1-14-图中图" class="headerlink" title="1.14 图中图"></a>1.14 图中图</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line">fig = plt.figure()</div><div class="line">x = [1, 2, 3, 4, 5, 6, 7]</div><div class="line">y = [1, 3, 4, 2, 5, 8, 6]</div><div class="line"></div><div class="line"># below are all percentage</div><div class="line">left, bottom, width, height = 0.1, 0.1, 0.8, 0.8</div><div class="line">ax1 = fig.add_axes([left, bottom, width, height])  # main axes</div><div class="line">ax1.plot(x, y, &apos;r&apos;)</div><div class="line">ax1.set_xlabel(&apos;x&apos;)</div><div class="line">ax1.set_ylabel(&apos;y&apos;)</div><div class="line">ax1.set_title(&apos;title&apos;)</div><div class="line"></div><div class="line">ax2 = fig.add_axes([0.2, 0.6, 0.25, 0.25])  # inside axes</div><div class="line">ax2.plot(y, x, &apos;b&apos;)</div><div class="line">ax2.set_xlabel(&apos;x&apos;)</div><div class="line">ax2.set_ylabel(&apos;y&apos;)</div><div class="line">ax2.set_title(&apos;title inside 1&apos;)</div><div class="line"></div><div class="line"></div><div class="line"># different method to add axes</div><div class="line">####################################</div><div class="line">plt.axes([0.6, 0.2, 0.25, 0.25])</div><div class="line">plt.plot(y[::-1], x, &apos;g&apos;)</div><div class="line">plt.xlabel(&apos;x&apos;)</div><div class="line">plt.ylabel(&apos;y&apos;)</div><div class="line">plt.title(&apos;title inside 2&apos;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_14_01.png" alt=""></p>
<h3 id="1-15-第二-Y-坐标"><a href="#1-15-第二-Y-坐标" class="headerlink" title="1.15 第二 Y 坐标"></a>1.15 第二 Y 坐标</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">import matplotlib.pyplot as plt</div><div class="line">import numpy as np</div><div class="line"></div><div class="line">x = np.arange(0, 10, 0.1)</div><div class="line">y1 = 0.05 * x**2</div><div class="line">y2 = -1 *y1</div><div class="line"></div><div class="line">fig, ax1 = plt.subplots()</div><div class="line"></div><div class="line">ax2 = ax1.twinx()    # mirror the ax1</div><div class="line">ax1.plot(x, y1, &apos;g-&apos;)</div><div class="line">ax2.plot(x, y2, &apos;b-&apos;)</div><div class="line"></div><div class="line">ax1.set_xlabel(&apos;X data&apos;)</div><div class="line">ax1.set_ylabel(&apos;Y1 data&apos;, color=&apos;g&apos;)</div><div class="line">ax2.set_ylabel(&apos;Y2 data&apos;, color=&apos;b&apos;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_15_01.png" alt=""></p>
<h3 id="1-16-动画"><a href="#1-16-动画" class="headerlink" title="1.16 动画"></a>1.16 动画</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">from matplotlib import pyplot as plt</div><div class="line">from matplotlib import animation</div><div class="line"></div><div class="line">fig, ax = plt.subplots()</div><div class="line"></div><div class="line">x = np.arange(0, 2*np.pi, 0.01)</div><div class="line">line, = ax.plot(x, np.sin(x))</div><div class="line"></div><div class="line"></div><div class="line">def animate(i):</div><div class="line">    line.set_ydata(np.sin(x + i/10.0))  # update the data</div><div class="line">    return line,</div><div class="line"></div><div class="line"></div><div class="line"># Init only required for blitting to give a clean slate.</div><div class="line">def init():</div><div class="line">    line.set_ydata(np.sin(x))</div><div class="line">    return line,</div><div class="line"></div><div class="line"># call the animator.  blit=True means only re-draw the parts that have changed.</div><div class="line"># blit=True dose not work on Mac, set blit=False</div><div class="line"># interval= update frequency</div><div class="line">ani = animation.FuncAnimation(fig=fig, func=animate, frames=100, init_func=init,</div><div class="line">                              interval=20, blit=False)</div><div class="line"></div><div class="line"># save the animation as an mp4.  This requires ffmpeg or mencoder to be</div><div class="line"># installed.  The extra_args ensure that the x264 codec is used, so that</div><div class="line"># the video can be embedded in html5.  You may need to adjust this for</div><div class="line"># your system: for more information, see</div><div class="line"># http://matplotlib.sourceforge.net/api/animation_api.html</div><div class="line"># anim.save(&apos;basic_animation.mp4&apos;, fps=30, extra_args=[&apos;-vcodec&apos;, &apos;libx264&apos;])</div><div class="line"></div><div class="line">ani.save(&apos;myanimation.gif&apos;, writer=&apos;imagemagick&apos;)</div><div class="line"></div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/07/MatplotlibDraw/1_16_01.gif" alt=""></p>
<h2 id="2-总结"><a href="#2-总结" class="headerlink" title="2. 总结"></a>2. 总结</h2><p>花时间把小细节搞懂就是节省时间。</p>
<h2 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3. 参考资料"></a>3. 参考资料</h2><ul>
<li><a href="https://morvanzhou.github.io/tutorials/data-manipulation/plt/" target="_blank" rel="external">morvan：markdown + 视频</a></li>
<li><a href="https://matplotlib.org/tutorials/index.html" target="_blank" rel="external">官方文档</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：这篇主要为了自己的学习，先理一下基础概念，然后记录下来，下次使用起来就方便了。学这个的主要目的是为了可视化机器学习的学习过程。网络上的资料太多了，每次写的时候去查算起来也是挺花费时间的。所以还是花点时间直接整理记录好了。这里推荐&lt;a href=&quot;https://morvanzhou.github.io/tutorials/data-manipulation/plt/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;morvan：markdown + 视频&lt;/a&gt; 和 &lt;a href=&quot;https://matplotlib.org/tutorials/index.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方文档&lt;/a&gt;。当然，此文还参考了好多网上的其它文章，再此一并感谢，并且参考资料中给出链接。所有代码整理到&lt;a href=&quot;https://github.com/mk43/python-practice/tree/master/matplotlib&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;GitHub&lt;/a&gt;。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
      <category term="NumPy" scheme="http://fitzeng.org/tags/NumPy/"/>
    
      <category term="Matplotlib" scheme="http://fitzeng.org/tags/Matplotlib/"/>
    
  </entry>
  
  <entry>
    <title>NumPy 官方快速入门教程(译)</title>
    <link href="http://fitzeng.org/2018/02/04/NumPyOfficialQuickstartTutorial/"/>
    <id>http://fitzeng.org/2018/02/04/NumPyOfficialQuickstartTutorial/</id>
    <published>2018-02-04T03:27:00.000Z</published>
    <updated>2018-02-04T09:26:08.312Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：本来是学习下 \( NumPy \)，看到官网的<a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">入门教程</a>想跟着实验一下，怕不常用，而我这人健忘，所以记录下来。索性就照着翻译一下，同样可以提升自己的阅读和写作能力，需要的可以存一下。当然，本人水平有限，有错误的地方欢迎大家指正。这里是基于 <a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html" target="_blank" rel="external">\( NumPy\ v1.13.dev0\ Manual \)</a> 翻译的。截止时间\( 2018/02/04 \)</p>
</blockquote>
<a id="more"></a>
<h2 id="快速入门教程"><a href="#快速入门教程" class="headerlink" title="快速入门教程"></a>快速入门教程</h2><h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1 准备工作"></a>1 准备工作</h3><p>在你浏览这个指导之前，你应该懂一点 \( Python \) 。如果你想回顾一下可以看<a href="https://docs.python.org/3/tutorial/" target="_blank" rel="external">Python tutorial</a>。如果你想把教程的代码跑起来，必须安装一些软件，请参考<a href="http://scipy.org/install.html" target="_blank" rel="external">http://scipy.org/install.html</a></p>
<h3 id="2-基础知识"><a href="#2-基础知识" class="headerlink" title="2 基础知识"></a>2 基础知识</h3><p>\( NumPy \) 的主要操作对象是同类型的多维数组。它是一个由正整数元组索引，元素类型相同的表（通常元素是数字）。在 \( NumPy \) 维度被称为 <code>axes</code>, <code>axes</code> 的数量称为 <code>rank</code>。</p>
<p>例如，在 \( 3D \) 空间的一个点 \( [1, 2, 1] \) 是一个 <code>rank = 1</code> 的数组，因为它只有一个 <code>axes</code>。这个 <code>axes</code> 的长度是 \( 3 \)。在下面这个例子中，数组 <code>rank = 2</code> （它是 \( 2  \)维的）。第一维（<code>axes</code>）长度是 \( 2 \)，第二位长度是 \( 3 \)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">[[ 1., 0., 0. ],</div><div class="line"> [ 0., 1., 2. ]]</div></pre></td></tr></table></figure>
<p>\( NumPy \) 的数组类是 <code>ndarray</code>。也可以叫做 <code>array</code>。说到这里，<code>numpy.array</code> 和标准 ( Python \) 库中的 <code>array.array</code> 是不一样的，它只能处理一维的数组和提供更少的功能。<code>ndarray</code> 对象的一些重要属性如下：</p>
<h5 id="ndarray-ndim"><a href="#ndarray-ndim" class="headerlink" title="ndarray.ndim"></a>ndarray.ndim</h5><blockquote>
<p>数组的 <code>axes</code> （维数）数值大小。在 \( Python \) 中维数的大小可以参考 <code>rank</code></p>
</blockquote>
<h5 id="ndarray-shape"><a href="#ndarray-shape" class="headerlink" title="ndarray.shape"></a>ndarray.shape</h5><blockquote>
<p>数组的维数，这是由每个维度的大小组成的一个元组。对于一个 \( n \) 行 \( m \) 列的矩阵。<code>shape</code> 是 <code>(n, m)</code>。由 <code>shape</code> 元组的长度得出 <code>rank</code> 或者维数 <code>ndim</code>。</p>
</blockquote>
<h5 id="ndarray-size"><a href="#ndarray-size" class="headerlink" title="ndarray.size"></a>ndarray.size</h5><blockquote>
<p>数组元素的个数总和，这等于 <code>shape</code> 元组数字的乘积。</p>
</blockquote>
<h5 id="ndarray-dtype"><a href="#ndarray-dtype" class="headerlink" title="ndarray.dtype"></a>ndarray.dtype</h5><blockquote>
<p>在数组中描述元素类型的一个对象。它是一种可以用标准的 \( Python \) 类型创建和指定的类型。另外，\( NumPy \)也提供了它自己的类型：<code>numpy.int32</code>，<code>numpy.int16</code>，<code>numpy.float64</code>……</p>
</blockquote>
<h5 id="ndarray-itemsize"><a href="#ndarray-itemsize" class="headerlink" title="ndarray.itemsize"></a>ndarray.itemsize</h5><blockquote>
<p>数组中每个元素所占字节数。例如，一个 <code>float64</code> 的 <code>itemsize</code> 是 \( 8 ( = 64/8bit) \)，<code>complex32</code> 的 <code>itemsize</code> 是 \( 4 ( = 32/8bit) \)。它和 <code>ndarray.dtype.itemsize</code> 是相等的。</p>
</blockquote>
<h5 id="ndarray-data"><a href="#ndarray-data" class="headerlink" title="ndarray.data"></a>ndarray.data</h5><blockquote>
<p>数组实际元素的缓存区。通常来说，我们不需要使用这个属性，因为我们会使用索引的方式访问数据。</p>
</blockquote>
<h3 id="2-1-例子"><a href="#2-1-例子" class="headerlink" title="2.1 例子"></a>2.1 例子</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import numpy as np</div><div class="line">&gt;&gt;&gt; a = np.arange(15).reshape(3, 5)</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[ 0,  1,  2,  3,  4],</div><div class="line">       [ 5,  6,  7,  8,  9],</div><div class="line">       [10, 11, 12, 13, 14]])</div><div class="line">&gt;&gt;&gt; a.shape</div><div class="line">(3, 5)</div><div class="line">&gt;&gt;&gt; a.ndim</div><div class="line">2</div><div class="line">&gt;&gt;&gt; a.dtype.name</div><div class="line">&apos;int64&apos;</div><div class="line">&gt;&gt;&gt; a.itemsize</div><div class="line">8</div><div class="line">&gt;&gt;&gt; a.size</div><div class="line">15</div><div class="line">&gt;&gt;&gt; type(a)</div><div class="line">&lt;type &apos;numpy.ndarray&apos;&gt;</div><div class="line">&gt;&gt;&gt; b = np.array([6, 7, 8])</div><div class="line">&gt;&gt;&gt; b</div><div class="line">array([6, 7, 8])</div><div class="line">&gt;&gt;&gt; type(b)</div><div class="line">&lt;type &apos;numpy.ndarray&apos;&gt;</div></pre></td></tr></table></figure>
<h3 id="2-2-创建数组"><a href="#2-2-创建数组" class="headerlink" title="2.2 创建数组"></a>2.2 创建数组</h3><p>这里有几种方法创建数组。</p>
<p>例如，你可以使用 <code>array</code> 函数从一个常规的 \( Python \) 列表或元组创建一个数组。创建的数组类型是从原始序列中的元素推断出来的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import numpy as np</div><div class="line">&gt;&gt;&gt; a = np.array([2,3,4])</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([2, 3, 4])</div><div class="line">&gt;&gt;&gt; a.dtype</div><div class="line">dtype(&apos;int64&apos;)</div><div class="line">&gt;&gt;&gt; b = np.array([1.2, 3.5, 5.1])</div><div class="line">&gt;&gt;&gt; b.dtype</div><div class="line">dtype(&apos;float64&apos;)</div></pre></td></tr></table></figure>
<p>一个常见错误是在调用 <code>array</code> 函数时，传递的参数是多个数值而不是一个单独的数字列表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.array(1,2,3,4)    # WRONG</div><div class="line">&gt;&gt;&gt; a = np.array([1,2,3,4])  # RIGHT</div></pre></td></tr></table></figure>
<p><code>array</code> 将序列转化成高维数组</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; b = np.array([(1.5,2,3), (4,5,6)])</div><div class="line">&gt;&gt;&gt; b</div><div class="line">array([[ 1.5,  2. ,  3. ],</div><div class="line">       [ 4. ,  5. ,  6. ]])</div></pre></td></tr></table></figure>
<p>数组的类型也能够在创建时具体指定：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; c = np.array( [ [1,2], [3,4] ], dtype=complex )</div><div class="line">&gt;&gt;&gt; c</div><div class="line">array([[ 1.+0.j,  2.+0.j],</div><div class="line">       [ 3.+0.j,  4.+0.j]])</div></pre></td></tr></table></figure>
<p>通常，我们都是知道数组的大小而不知道其中的原始数据。因此 \( NumPy \) 提供了几个用占位符的函数去创建数组。这样可以最小化增加数组的成本，增加数组是一项很耗费资源的操作。</p>
<p><code>zeros</code> 函数创建一个全是 \( 0 \) 的数组，<code>ones</code> 函数创建全是 \( 1 \) 的数组，<code>empty</code> 创建一个随机的数组。默认创建数组的类型是 <code>float64</code>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; np.zeros( (3,4) )</div><div class="line">array([[ 0.,  0.,  0.,  0.],</div><div class="line">       [ 0.,  0.,  0.,  0.],</div><div class="line">       [ 0.,  0.,  0.,  0.]])</div><div class="line">&gt;&gt;&gt; np.ones( (2,3,4), dtype=np.int16 )                # dtype can also be specified</div><div class="line">array([[[ 1, 1, 1, 1],</div><div class="line">        [ 1, 1, 1, 1],</div><div class="line">        [ 1, 1, 1, 1]],</div><div class="line">       [[ 1, 1, 1, 1],</div><div class="line">        [ 1, 1, 1, 1],</div><div class="line">        [ 1, 1, 1, 1]]], dtype=int16)</div><div class="line">&gt;&gt;&gt; np.empty( (2,3) )                                 # uninitialized, output may vary</div><div class="line">array([[  3.73603959e-262,   6.02658058e-154,   6.55490914e-260],</div><div class="line">       [  5.30498948e-313,   3.14673309e-307,   1.00000000e+000]])</div></pre></td></tr></table></figure>
<p>为了创建数字序列，\( NumPy \) 提供了一个和 <code>range</code> 相似的函数，可以返回一个数组而不是列表。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; np.arange( 10, 30, 5 )</div><div class="line">array([10, 15, 20, 25])</div><div class="line">&gt;&gt;&gt; np.arange( 0, 2, 0.3 )                 # it accepts float arguments</div><div class="line">array([ 0. ,  0.3,  0.6,  0.9,  1.2,  1.5,  1.8])</div></pre></td></tr></table></figure>
<p>当 <code>arange</code> 的参数是浮点型的，由于有限的浮点精度，通常不太可能去预测获得元素的数量。出于这个原因，通常选择更好的函数 <code>linspace</code>，他接收我们想要的元素数量而不是步长作为参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from numpy import pi</div><div class="line">&gt;&gt;&gt; np.linspace( 0, 2, 9 )                 # 9 numbers from 0 to 2</div><div class="line">array([ 0.  ,  0.25,  0.5 ,  0.75,  1.  ,  1.25,  1.5 ,  1.75,  2.  ])</div><div class="line">&gt;&gt;&gt; x = np.linspace( 0, 2*pi, 100 )        # useful to evaluate function at lots of points</div><div class="line">&gt;&gt;&gt; f = np.sin(x)</div></pre></td></tr></table></figure>
<h5 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h5><p><a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.array.html#numpy.array" target="_blank" rel="external">array</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.zeros.html#numpy.zeros" target="_blank" rel="external">zeros</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.zeros_like.html#numpy.zeros_like" target="_blank" rel="external">zeros_like</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.ones.html#numpy.ones" target="_blank" rel="external">ones</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.ones_like.html#numpy.ones_like" target="_blank" rel="external">ones_like</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.empty.html#numpy.empty" target="_blank" rel="external">empty</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.empty_like.html#numpy.empty_like" target="_blank" rel="external">empty_like</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.arange.html#numpy.arange" target="_blank" rel="external">arange</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.linspace.html#numpy.linspace" target="_blank" rel="external">linspace</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.rand.html#numpy.random.rand" target="_blank" rel="external">numpy.random.rand</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.random.randn.html#numpy.random.randn" target="_blank" rel="external">numpy.random.randn</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.fromfunction.html#numpy.fromfunction" target="_blank" rel="external">fromfunction</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.fromfile.html#numpy.fromfile" target="_blank" rel="external">fromfile</a></p>
<h3 id="2-3-打印数组"><a href="#2-3-打印数组" class="headerlink" title="2.3 打印数组"></a>2.3 打印数组</h3><p>当你打印数组时，\( NumPy \) 显示出来和嵌套的列表相似，但是具有以下布局：</p>
<ul>
<li>最后一个 <code>axis</code> 从左到右打印，</li>
<li>第二到最后一个从上到下打印，</li>
<li>剩余的也是从上到下打印，每一片通过一个空行隔开。</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(6)                         # 1d array</div><div class="line">&gt;&gt;&gt; print(a)</div><div class="line">[0 1 2 3 4 5]</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; b = np.arange(12).reshape(4,3)           # 2d array</div><div class="line">&gt;&gt;&gt; print(b)</div><div class="line">[[ 0  1  2]</div><div class="line"> [ 3  4  5]</div><div class="line"> [ 6  7  8]</div><div class="line"> [ 9 10 11]]</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; c = np.arange(24).reshape(2,3,4)         # 3d array</div><div class="line">&gt;&gt;&gt; print(c)</div><div class="line">[[[ 0  1  2  3]</div><div class="line">  [ 4  5  6  7]</div><div class="line">  [ 8  9 10 11]]</div><div class="line"> [[12 13 14 15]</div><div class="line">  [16 17 18 19]</div><div class="line">  [20 21 22 23]]]</div></pre></td></tr></table></figure>
<p>参考<a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#quickstart-shape-manipulation" target="_blank" rel="external">下文</a>来获取更多 <code>reshape</code> 的细节。</p>
<p>如果一个数组太大而不能被打印，那么 \( NumPy \) 会自动忽略中间的只打印角上的数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; print(np.arange(10000))</div><div class="line">[   0    1    2 ..., 9997 9998 9999]</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; print(np.arange(10000).reshape(100,100))</div><div class="line">[[   0    1    2 ...,   97   98   99]</div><div class="line"> [ 100  101  102 ...,  197  198  199]</div><div class="line"> [ 200  201  202 ...,  297  298  299]</div><div class="line"> ...,</div><div class="line"> [9700 9701 9702 ..., 9797 9798 9799]</div><div class="line"> [9800 9801 9802 ..., 9897 9898 9899]</div><div class="line"> [9900 9901 9902 ..., 9997 9998 9999]]</div></pre></td></tr></table></figure>
<p>为了取消这种行为，强制 \( NumPy \) 去打印整个数组，你可以通过 <code>set_printoptions</code> 改变打印选项。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; np.set_printoptions(threshold=&apos;nan&apos;)</div></pre></td></tr></table></figure>
<h3 id="2-4-基本操作"><a href="#2-4-基本操作" class="headerlink" title="2.4 基本操作"></a>2.4 基本操作</h3><p>在数组上的算数运算应用于每个元素。并创建一个用结果填充的新的数组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.array( [20,30,40,50] )</div><div class="line">&gt;&gt;&gt; b = np.arange( 4 )</div><div class="line">&gt;&gt;&gt; b</div><div class="line">array([0, 1, 2, 3])</div><div class="line">&gt;&gt;&gt; c = a-b</div><div class="line">&gt;&gt;&gt; c</div><div class="line">array([20, 29, 38, 47])</div><div class="line">&gt;&gt;&gt; b**2</div><div class="line">array([0, 1, 4, 9])</div><div class="line">&gt;&gt;&gt; 10*np.sin(a)</div><div class="line">array([ 9.12945251, -9.88031624,  7.4511316 , -2.62374854])</div><div class="line">&gt;&gt;&gt; a&lt;35</div><div class="line">array([ True, True, False, False], dtype=bool)</div></pre></td></tr></table></figure>
<p>在 \( NumPy \) 数组的 <code>*</code> 操作不像其他的矩阵语言。矩阵乘法通过 <code>dot</code> 函数进行模拟。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; A = np.array( [[1,1],</div><div class="line">...             [0,1]] )</div><div class="line">&gt;&gt;&gt; B = np.array( [[2,0],</div><div class="line">...             [3,4]] )</div><div class="line">&gt;&gt;&gt; A*B                         # elementwise product</div><div class="line">array([[2, 0],</div><div class="line">       [0, 4]])</div><div class="line">&gt;&gt;&gt; A.dot(B)                    # matrix product</div><div class="line">array([[5, 4],</div><div class="line">       [3, 4]])</div><div class="line">&gt;&gt;&gt; np.dot(A, B)                # another matrix product</div><div class="line">array([[5, 4],</div><div class="line">       [3, 4]])</div></pre></td></tr></table></figure>
<p>想 <code>+=</code> 和 <code>*=</code> 操作之类的，直接在原数组上做修改，不会创建新数组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.ones((2,3), dtype=int)</div><div class="line">&gt;&gt;&gt; b = np.random.random((2,3))</div><div class="line">&gt;&gt;&gt; a *= 3</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[3, 3, 3],</div><div class="line">       [3, 3, 3]])</div><div class="line">&gt;&gt;&gt; b += a</div><div class="line">&gt;&gt;&gt; b</div><div class="line">array([[ 3.417022  ,  3.72032449,  3.00011437],</div><div class="line">       [ 3.30233257,  3.14675589,  3.09233859]])</div><div class="line">&gt;&gt;&gt; a += b                  # b is not automatically converted to integer type</div><div class="line">Traceback (most recent call last):</div><div class="line">  ...</div><div class="line">TypeError: Cannot cast ufunc add output from dtype(&apos;float64&apos;) to dtype(&apos;int64&apos;) with casting rule &apos;same_kind&apos;</div></pre></td></tr></table></figure>
<p>在不同数组类型之间的操作，结果数组的类型趋于更普通或者更精确的一种（称为向上转型）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.ones(3, dtype=np.int32)</div><div class="line">&gt;&gt;&gt; b = np.linspace(0,pi,3)</div><div class="line">&gt;&gt;&gt; b.dtype.name</div><div class="line">&apos;float64&apos;</div><div class="line">&gt;&gt;&gt; c = a+b</div><div class="line">&gt;&gt;&gt; c</div><div class="line">array([ 1.        ,  2.57079633,  4.14159265])</div><div class="line">&gt;&gt;&gt; c.dtype.name</div><div class="line">&apos;float64&apos;</div><div class="line">&gt;&gt;&gt; d = np.exp(c*1j)</div><div class="line">&gt;&gt;&gt; d</div><div class="line">array([ 0.54030231+0.84147098j, -0.84147098+0.54030231j,</div><div class="line">       -0.54030231-0.84147098j])</div><div class="line">&gt;&gt;&gt; d.dtype.name</div><div class="line">&apos;complex128&apos;</div></pre></td></tr></table></figure>
<p>许多类似于求数组所有元素的和的一元操作都是作为 <code>ndarray</code> 类的方法实现的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.random.random((2,3))</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[ 0.18626021,  0.34556073,  0.39676747],</div><div class="line">       [ 0.53881673,  0.41919451,  0.6852195 ]])</div><div class="line">&gt;&gt;&gt; a.sum()</div><div class="line">2.5718191614547998</div><div class="line">&gt;&gt;&gt; a.min()</div><div class="line">0.1862602113776709</div><div class="line">&gt;&gt;&gt; a.max()</div><div class="line">0.6852195003967595</div></pre></td></tr></table></figure>
<p>默认情况下，尽管这些操作是应用于一个数字列表，可以无视它的形状。当时，通过指定 <code>axis</code> 参数可以将操作应用于数组的某一具体 <code>axis</code> 。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; b = np.arange(12).reshape(3,4)</div><div class="line">&gt;&gt;&gt; b</div><div class="line">array([[ 0,  1,  2,  3],</div><div class="line">       [ 4,  5,  6,  7],</div><div class="line">       [ 8,  9, 10, 11]])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; b.sum(axis=0)                            # sum of each column</div><div class="line">array([12, 15, 18, 21])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; b.min(axis=1)                            # min of each row</div><div class="line">array([0, 4, 8])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; b.cumsum(axis=1)                         # cumulative sum along each row</div><div class="line">array([[ 0,  1,  3,  6],</div><div class="line">       [ 4,  9, 15, 22],</div><div class="line">       [ 8, 17, 27, 38]])</div></pre></td></tr></table></figure>
<h3 id="2-5-通用功能"><a href="#2-5-通用功能" class="headerlink" title="2.5 通用功能"></a>2.5 通用功能</h3><p>\( NumPy \) 提供了很多数学上的函数，例如 <code>sin</code>、<code>cos</code>、<code>exp</code>。这些被叫做 “universal functions” (<code>ufunc</code>)。在 \( NumPy \）中这些函数是操作数组数字，产生一个数组作为输出。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; B = np.arange(3)</div><div class="line">&gt;&gt;&gt; B</div><div class="line">array([0, 1, 2])</div><div class="line">&gt;&gt;&gt; np.exp(B)</div><div class="line">array([ 1.        ,  2.71828183,  7.3890561 ])</div><div class="line">&gt;&gt;&gt; np.sqrt(B)</div><div class="line">array([ 0.        ,  1.        ,  1.41421356])</div><div class="line">&gt;&gt;&gt; C = np.array([2., -1., 4.])</div><div class="line">&gt;&gt;&gt; np.add(B, C)</div><div class="line">array([ 2.,  0.,  6.])</div></pre></td></tr></table></figure>
<h5 id="参考-1"><a href="#参考-1" class="headerlink" title="参考"></a>参考</h5><p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#universal-functions" target="_blank" rel="external">all, any, apply_along_axis, argmax, argmin, argsort, average, bincount, ceil, clip, conj, corrcoef, cov, cross, cumprod, cumsum, diff, dot, floor, inner, inv, lexsort, max, maximum, mean, median, min, minimum, nonzero, outer, prod, re, round, sort, std, sum, trace, transpose, var, vdot, vectorize, where</a></p>
<h3 id="2-6-索引，切片和迭代"><a href="#2-6-索引，切片和迭代" class="headerlink" title="2.6 索引，切片和迭代"></a>2.6 索引，切片和迭代</h3><p>一维数组可以被索引，切片和迭代，就像<a href="https://docs.python.org/tutorial/introduction.html#lists" target="_blank" rel="external">列表</a>和其他Python序列一样。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(10)**3</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([  0,   1,   8,  27,  64, 125, 216, 343, 512, 729])</div><div class="line">&gt;&gt;&gt; a[2]</div><div class="line">8</div><div class="line">&gt;&gt;&gt; a[2:5]</div><div class="line">array([ 8, 27, 64])</div><div class="line">&gt;&gt;&gt; a[:6:2] = -1000    # equivalent to a[0:6:2] = -1000; from start to position 6, exclusive, set every 2nd element to -1000</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([-1000,     1, -1000,    27, -1000,   125,   216,   343,   512,   729])</div><div class="line">&gt;&gt;&gt; a[ : :-1]                                 # reversed a</div><div class="line">array([  729,   512,   343,   216,   125, -1000,    27, -1000,     1, -1000])</div><div class="line">&gt;&gt;&gt; for i in a:</div><div class="line">...     print(i**(1/3.))</div><div class="line">...</div><div class="line">nan</div><div class="line">1.0</div><div class="line">nan</div><div class="line">3.0</div><div class="line">nan</div><div class="line">5.0</div><div class="line">6.0</div><div class="line">7.0</div><div class="line">8.0</div><div class="line">9.0</div></pre></td></tr></table></figure>
<p>多维数组对于每个 <code>axis</code> 都有一个索引，这些索引用逗号分隔。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; def f(x,y):</div><div class="line">...     return 10*x+y</div><div class="line">...</div><div class="line">&gt;&gt;&gt; b = np.fromfunction(f,(5,4),dtype=int)</div><div class="line">&gt;&gt;&gt; b</div><div class="line">array([[ 0,  1,  2,  3],</div><div class="line">       [10, 11, 12, 13],</div><div class="line">       [20, 21, 22, 23],</div><div class="line">       [30, 31, 32, 33],</div><div class="line">       [40, 41, 42, 43]])</div><div class="line">&gt;&gt;&gt; b[2,3]</div><div class="line">23</div><div class="line">&gt;&gt;&gt; b[0:5, 1]                       # each row in the second column of b</div><div class="line">array([ 1, 11, 21, 31, 41])</div><div class="line">&gt;&gt;&gt; b[ : ,1]                        # equivalent to the previous example</div><div class="line">array([ 1, 11, 21, 31, 41])</div><div class="line">&gt;&gt;&gt; b[1:3, : ]                      # each column in the second and third row of b</div><div class="line">array([[10, 11, 12, 13],</div><div class="line">       [20, 21, 22, 23]])</div></pre></td></tr></table></figure>
<p>当提供的索引少于 <code>axis</code> 的数量时，缺失的索引按完全切片考虑。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; b[-1]                                  # the last row. Equivalent to b[-1,:]</div><div class="line">array([40, 41, 42, 43])</div></pre></td></tr></table></figure>
<p><code>b[i]</code> 这种表达中括号中的 <code>i</code> 后面可以跟很多用 <code>:</code> 表示其它 <code>axis</code> 的实例。\( NumPy \) 也允许使用三个点代替 <code>b[i, ...]</code></p>
<p>这三个点(<code>...</code>)表示很多完整索引元组中的冒号。例如，<code>x</code> 的 <code>rank = 5</code> 有：</p>
<ul>
<li><code>x[1, 2, ...]</code> = <code>x[1, 2, :, :, :]</code></li>
<li><code>x[..., 3]</code> = <code>x[:, :, :, :, 3]</code></li>
<li><code>x[4, ..., 5, :]</code> = <code>x[4, :, :, 5, :]</code></li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; c = np.array( [[[  0,  1,  2],               # a 3D array (two stacked 2D arrays)</div><div class="line">...                 [ 10, 12, 13]],</div><div class="line">...                [[100,101,102],</div><div class="line">...                 [110,112,113]]])</div><div class="line">&gt;&gt;&gt; c.shape</div><div class="line">(2, 2, 3)</div><div class="line">&gt;&gt;&gt; c[1,...]                                   # same as c[1,:,:] or c[1]</div><div class="line">array([[100, 101, 102],</div><div class="line">       [110, 112, 113]])</div><div class="line">&gt;&gt;&gt; c[...,2]                                   # same as c[:,:,2]</div><div class="line">array([[  2,  13],</div><div class="line">       [102, 113]])</div></pre></td></tr></table></figure>
<p>迭代多维数组是对第一 <code>axis</code> 进行的。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for row in b:</div><div class="line">...     print(row)</div><div class="line">...</div><div class="line">[0 1 2 3]</div><div class="line">[10 11 12 13]</div><div class="line">[20 21 22 23]</div><div class="line">[30 31 32 33]</div><div class="line">[40 41 42 43]</div></pre></td></tr></table></figure>
<p>然而，如果你想模拟对数组中每一个元素的操作，你可以使用 <code>flat</code> 属性，它是一个 <a href="https://docs.python.org/2/tutorial/classes.html#iterators" target="_blank" rel="external">iterator</a>，能够遍历数组中每一个元素。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; for element in b.flat:</div><div class="line">...     print(element)</div><div class="line">...</div><div class="line">0</div><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td></tr></table></figure>
<h5 id="参考-2"><a href="#参考-2" class="headerlink" title="参考"></a>参考</h5><p><a href="https://docs.scipy.org/doc/numpy-dev/user/basics.indexing.html#basics-indexing" target="_blank" rel="external">Indexing</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/arrays.indexing.html#arrays-indexing" target="_blank" rel="external">Indexing</a> (reference), <a href="https://docs.scipy.org/doc/numpy-dev/reference/arrays.indexing.html#numpy.newaxis" target="_blank" rel="external">newaxis</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.ndenumerate.html#numpy.ndenumerate" target="_blank" rel="external">ndenumerate</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.indices.html#numpy.indices" target="_blank" rel="external">indices</a></p>
<h2 id="3-操控形状"><a href="#3-操控形状" class="headerlink" title="3 操控形状"></a>3 操控形状</h2><h3 id="3-1-改变数组的形状"><a href="#3-1-改变数组的形状" class="headerlink" title="3.1 改变数组的形状"></a>3.1 改变数组的形状</h3><p>每一个数组的形状通过每一个 <code>axis</code> 中的元素数量。（其实就是每一个维度的元素多少确定）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.floor(10*np.random.random((3,4)))</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[ 2.,  8.,  0.,  6.],</div><div class="line">       [ 4.,  5.,  1.,  1.],</div><div class="line">       [ 8.,  9.,  3.,  6.]])</div><div class="line">&gt;&gt;&gt; a.shape</div><div class="line">(3, 4)</div></pre></td></tr></table></figure>
<p>数组的形状可以通过很多命令来改变，提到这里，接下来的三个例子放回一个被修改的数组，原数组不会改变。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a.ravel()  # returns the array, flattened</div><div class="line">array([ 2.,  8.,  0.,  6.,  4.,  5.,  1.,  1.,  8.,  9.,  3.,  6.])</div><div class="line">&gt;&gt;&gt; a.reshape(6,2)  # returns the array with a modified shape</div><div class="line">array([[ 2.,  8.],</div><div class="line">       [ 0.,  6.],</div><div class="line">       [ 4.,  5.],</div><div class="line">       [ 1.,  1.],</div><div class="line">       [ 8.,  9.],</div><div class="line">       [ 3.,  6.]])</div><div class="line">&gt;&gt;&gt; a.T  # returns the array, transposed</div><div class="line">array([[ 2.,  4.,  8.],</div><div class="line">       [ 8.,  5.,  9.],</div><div class="line">       [ 0.,  1.,  3.],</div><div class="line">       [ 6.,  1.,  6.]])</div><div class="line">&gt;&gt;&gt; a.T.shape</div><div class="line">(4, 3)</div><div class="line">&gt;&gt;&gt; a.shape</div><div class="line">(3, 4)</div></pre></td></tr></table></figure>
<p><code>ravel()</code> 函数中每个元素的位置通常是一种 “C-style” 的，也就是说，最右边的索引改变起来最快。所以元素 <code>a[0, 0]</code> 后面的元素是 <code>a[0, 1]</code>。如果这个数组被塑造成其它形状，这个数组也是作为 “C-style” 对待。\( NumPy \) 通常也是按照这个创建的数组，所以使用 <code>ravel()</code> 函数时不需要复制，但是如果这个数组是通过从另一个数组切片或者其它不同寻常的方式而来的话，它就需要进行复制了。函数 <code>ravel()</code> 和 <code>reshape()</code> 也可以通过可选参数被指定去用 <code>FORTRAN-style</code> 的数组，这这种风格中，最左的索引改变最快。</p>
<p><a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.reshape.html#numpy.reshape" target="_blank" rel="external">reshape</a> 函数返回修改的形状，而 <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.ndarray.resize.html#numpy.ndarray.resize" target="_blank" rel="external">ndarray.resize</a> 方法直接修改数组本身。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[ 2.,  8.,  0.,  6.],</div><div class="line">       [ 4.,  5.,  1.,  1.],</div><div class="line">       [ 8.,  9.,  3.,  6.]])</div><div class="line">&gt;&gt;&gt; a.resize((2,6))</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[ 2.,  8.,  0.,  6.,  4.,  5.],</div><div class="line">       [ 1.,  1.,  8.,  9.,  3.,  6.]])</div></pre></td></tr></table></figure>
<p>如果一个维度给一个 \( -1 \) 作为参数，那么其他它维度将自动计算。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a.reshape(3,-1)</div><div class="line">array([[ 2.,  8.,  0.,  6.],</div><div class="line">       [ 4.,  5.,  1.,  1.],</div><div class="line">       [ 8.,  9.,  3.,  6.]])</div></pre></td></tr></table></figure>
<h5 id="参考-3"><a href="#参考-3" class="headerlink" title="参考"></a>参考</h5><p><a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.ndarray.shape.html#numpy.ndarray.shape" target="_blank" rel="external">ndarray.shape</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.reshape.html#numpy.reshape" target="_blank" rel="external">reshape</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.resize.html#numpy.resize" target="_blank" rel="external">resize</a>, <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.ravel.html#numpy.ravel" target="_blank" rel="external">ravel</a></p>
<h3 id="3-2-不同数组的组合"><a href="#3-2-不同数组的组合" class="headerlink" title="3.2 不同数组的组合"></a>3.2 不同数组的组合</h3><p>数组可以通过不同的 <code>axes</code> 组合起来。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.floor(10*np.random.random((2,2)))</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[ 8.,  8.],</div><div class="line">       [ 0.,  0.]])</div><div class="line">&gt;&gt;&gt; b = np.floor(10*np.random.random((2,2)))</div><div class="line">&gt;&gt;&gt; b</div><div class="line">array([[ 1.,  8.],</div><div class="line">       [ 0.,  4.]])</div><div class="line">&gt;&gt;&gt; np.vstack((a,b))</div><div class="line">array([[ 8.,  8.],</div><div class="line">       [ 0.,  0.],</div><div class="line">       [ 1.,  8.],</div><div class="line">       [ 0.,  4.]])</div><div class="line">&gt;&gt;&gt; np.hstack((a,b))</div><div class="line">array([[ 8.,  8.,  1.,  8.],</div><div class="line">       [ 0.,  0.,  0.,  4.]])</div></pre></td></tr></table></figure>
<p><a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.column_stack.html#numpy.column_stack" target="_blank" rel="external">column_stack</a> 函数可以将 \( 1D \) 数组作为 \( 2D \) 数组的列。当且仅当数组是 \( 1D \) 的时候它等于 <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.vstack.html#numpy.vstack" target="_blank" rel="external">vstack</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; from numpy import newaxis</div><div class="line">&gt;&gt;&gt; np.column_stack((a,b))   # With 2D arrays</div><div class="line">array([[ 8.,  8.,  1.,  8.],</div><div class="line">       [ 0.,  0.,  0.,  4.]])</div><div class="line">&gt;&gt;&gt; a = np.array([4.,2.])</div><div class="line">&gt;&gt;&gt; b = np.array([2.,8.])</div><div class="line">&gt;&gt;&gt; a[:,newaxis]  # This allows to have a 2D columns vector</div><div class="line">array([[ 4.],</div><div class="line">       [ 2.]])</div><div class="line">&gt;&gt;&gt; np.column_stack((a[:,newaxis],b[:,newaxis]))</div><div class="line">array([[ 4.,  2.],</div><div class="line">       [ 2.,  8.]])</div><div class="line">&gt;&gt;&gt; np.vstack((a[:,newaxis],b[:,newaxis])) # The behavior of vstack is different</div><div class="line">array([[ 4.],</div><div class="line">       [ 2.],</div><div class="line">       [ 2.],</div><div class="line">       [ 8.]])</div></pre></td></tr></table></figure>
<p>对于超过两个维度的数组，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.hstack.html#numpy.hstack" target="_blank" rel="external">hstack</a> 会沿着第二个 <code>axis</code> 堆积，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.vstack.html#numpy.vstack" target="_blank" rel="external">vstack</a> 沿着第一个 <code>axes</code> 堆积，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.concatenate.html#numpy.concatenate" target="_blank" rel="external">concatenate</a> 允许一个可选参数选择哪一个 <code>axis</code> 发生连接操作。</p>
<h5 id="提示"><a href="#提示" class="headerlink" title="提示"></a>提示</h5><p>在复杂情况下，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.r_.html#numpy.r_" target="_blank" rel="external">r_</a> 和 <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.c_.html#numpy.c_" target="_blank" rel="external">c_</a> 对于通过沿一个 <code>axis</code> 堆积数字来创建数组很有用。它们允许使用范围表示符号（“:”）</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; np.r_[1:4,0,4]</div><div class="line">array([1, 2, 3, 0, 4])</div></pre></td></tr></table></figure>
<p>当使用数组作为参数时，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.r_.html#numpy.r_" target="_blank" rel="external">r_</a> 与 <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.c_.html#numpy.c_" target="_blank" rel="external">c_</a> 在默认行为是和 <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.vstack.html#numpy.vstack" target="_blank" rel="external">vstack</a> 与 <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.hstack.html#numpy.hstack" target="_blank" rel="external">hstack</a> 相似的，但是它们允许可选参数给出 <code>axis</code> 来连接。</p>
<h5 id="参考-4"><a href="#参考-4" class="headerlink" title="参考"></a>参考</h5><p><a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.hstack.html#numpy.hstack" target="_blank" rel="external">hstack</a>，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.vstack.html#numpy.vstack" target="_blank" rel="external">vstack</a>，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.column_stack.html#numpy.column_stack" target="_blank" rel="external">column_stack</a>，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.concatenate.html#numpy.concatenate" target="_blank" rel="external">concatenate</a>，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.c_.html#numpy.c_" target="_blank" rel="external">c_</a>，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.r_.html#numpy.r_" target="_blank" rel="external">r_</a></p>
<h3 id="3-3-将数组分割成几个小数组"><a href="#3-3-将数组分割成几个小数组" class="headerlink" title="3.3 将数组分割成几个小数组"></a>3.3 将数组分割成几个小数组</h3><p>使用 <a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.hsplit.html#numpy.hsplit" target="_blank" rel="external">hsplit</a>，你能沿着它的水平 <code>axis</code> 分割，可以通过指定数组形状来返回，也可以指定哪个列应该拆分：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.floor(10*np.random.random((2,12)))</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[ 9.,  5.,  6.,  3.,  6.,  8.,  0.,  7.,  9.,  7.,  2.,  7.],</div><div class="line">       [ 1.,  4.,  9.,  2.,  2.,  1.,  0.,  6.,  2.,  2.,  4.,  0.]])</div><div class="line">&gt;&gt;&gt; np.hsplit(a,3)   # Split a into 3</div><div class="line">[array([[ 9.,  5.,  6.,  3.],</div><div class="line">       [ 1.,  4.,  9.,  2.]]), array([[ 6.,  8.,  0.,  7.],</div><div class="line">       [ 2.,  1.,  0.,  6.]]), array([[ 9.,  7.,  2.,  7.],</div><div class="line">       [ 2.,  2.,  4.,  0.]])]</div><div class="line">&gt;&gt;&gt; np.hsplit(a,(3,4))   # Split a after the third and the fourth column</div><div class="line">[array([[ 9.,  5.,  6.],</div><div class="line">       [ 1.,  4.,  9.]]), array([[ 3.],</div><div class="line">       [ 2.]]), array([[ 6.,  8.,  0.,  7.,  9.,  7.,  2.,  7.],</div><div class="line">       [ 2.,  1.,  0.,  6.,  2.,  2.,  4.,  0.]])]</div></pre></td></tr></table></figure>
<p><a href="">vplit</a> 沿着竖直的 <code>axis</code> 分割，<a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.array_split.html#numpy.array_split" target="_blank" rel="external">array_split</a> 允许通过指定哪个 <code>axis</code> 去分割。</p>
<h2 id="4-拷贝和-Views"><a href="#4-拷贝和-Views" class="headerlink" title="4 拷贝和 Views"></a>4 拷贝和 Views</h2><p>在操作数组的时候，它们的数据有时候拷贝进一个新的数组，有时候又不是。这经常是初学者感到困惑。下面有三种情况：</p>
<h3 id="4-1-不拷贝"><a href="#4-1-不拷贝" class="headerlink" title="4.1 不拷贝"></a>4.1 不拷贝</h3><p>简单的赋值不会拷贝任何数组对象和它们的数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(12)</div><div class="line">&gt;&gt;&gt; b = a            # no new object is created</div><div class="line">&gt;&gt;&gt; b is a           # a and b are two names for the same ndarray object</div><div class="line">True</div><div class="line">&gt;&gt;&gt; b.shape = 3,4    # changes the shape of a</div><div class="line">&gt;&gt;&gt; a.shape</div><div class="line">(3, 4)</div></pre></td></tr></table></figure>
<p>\( Python \) 将可变对象作为引用传递，所以函数调用不会产生拷贝。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; def f(x):</div><div class="line">...     print(id(x))</div><div class="line">...</div><div class="line">&gt;&gt;&gt; id(a)                           # id is a unique identifier of an object</div><div class="line">148293216</div><div class="line">&gt;&gt;&gt; f(a)</div><div class="line">148293216</div></pre></td></tr></table></figure>
<h3 id="4-2-View-或者浅拷贝"><a href="#4-2-View-或者浅拷贝" class="headerlink" title="4.2 View 或者浅拷贝"></a>4.2 View 或者浅拷贝</h3><p>不同的数组对象可以分享相同的数据。<code>view</code> 方法创建了一个相同数据的新数组对象。<br>PS：这里 View（视图？） 不知道如何理解好，所以保留。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; c = a.view()</div><div class="line">&gt;&gt;&gt; c is a</div><div class="line">False</div><div class="line">&gt;&gt;&gt; c.base is a                        # c is a view of the data owned by a</div><div class="line">True</div><div class="line">&gt;&gt;&gt; c.flags.owndata</div><div class="line">False</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; c.shape = 2,6                      # a&apos;s shape doesn&apos;t change</div><div class="line">&gt;&gt;&gt; a.shape</div><div class="line">(3, 4)</div><div class="line">&gt;&gt;&gt; c[0,4] = 1234                      # a&apos;s data changes</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[   0,    1,    2,    3],</div><div class="line">       [1234,    5,    6,    7],</div><div class="line">       [   8,    9,   10,   11]])</div></pre></td></tr></table></figure>
<p>切片数组返回一个 <code>view</code>：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; s = a[ : , 1:3]     # spaces added for clarity; could also be written &quot;s = a[:,1:3]&quot;</div><div class="line">&gt;&gt;&gt; s[:] = 10           # s[:] is a view of s. Note the difference between s=10 and s[:]=10</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[   0,   10,   10,    3],</div><div class="line">       [1234,   10,   10,    7],</div><div class="line">       [   8,   10,   10,   11]])</div></pre></td></tr></table></figure>
<h3 id="4-3-深拷贝"><a href="#4-3-深拷贝" class="headerlink" title="4.3 深拷贝"></a>4.3 深拷贝</h3><p><code>copy</code> 方法完全拷贝数组。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; d = a.copy()                          # a new array object with new data is created</div><div class="line">&gt;&gt;&gt; d is a</div><div class="line">False</div><div class="line">&gt;&gt;&gt; d.base is a                           # d doesn&apos;t share anything with a</div><div class="line">False</div><div class="line">&gt;&gt;&gt; d[0,0] = 9999</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[   0,   10,   10,    3],</div><div class="line">       [1234,   10,   10,    7],</div><div class="line">       [   8,   10,   10,   11]])</div></pre></td></tr></table></figure>
<h3 id="4-4-函数和方法综述"><a href="#4-4-函数和方法综述" class="headerlink" title="4.4 函数和方法综述"></a>4.4 函数和方法综述</h3><p>这里通过类别排序列举一些有用的 \( NumPy \) 函数和方法。拆看完整列表点击<a href="https://docs.scipy.org/doc/numpy-dev/reference/routines.html#routines" target="_blank" rel="external">Routines</a></p>
<h5 id="数组收集"><a href="#数组收集" class="headerlink" title="数组收集"></a>数组收集</h5><blockquote>
<p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#functions-and-methods-overview" target="_blank" rel="external">arange, array, copy, empty, empty_like, eye, fromfile, fromfunction, identity, linspace, logspace, mgrid, ogrid, ones, ones_like, r, zeros, zeros_like</a></p>
</blockquote>
<h5 id="转化"><a href="#转化" class="headerlink" title="转化"></a>转化</h5><blockquote>
<p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#functions-and-methods-overview" target="_blank" rel="external">ndarray.astype, atleast_1d, atleast_2d, atleast_3d, mat</a></p>
</blockquote>
<h5 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h5><blockquote>
<p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#functions-and-methods-overview" target="_blank" rel="external">array_split, column_stack, concatenate, diagonal, dsplit, dstack, hsplit, hstack, ndarray.item, newaxis, ravel, repeat, reshape, resize, squeeze, swapaxes, take, transpose, vsplit, vstack</a></p>
</blockquote>
<h5 id="疑问"><a href="#疑问" class="headerlink" title="疑问"></a>疑问</h5><blockquote>
<p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#functions-and-methods-overview" target="_blank" rel="external">all, any, nonzero, where</a></p>
</blockquote>
<h5 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h5><blockquote>
<p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#functions-and-methods-overview" target="_blank" rel="external">argmax, argmin, argsort, max, min, ptp, searchsorted, sort</a></p>
</blockquote>
<h5 id="运算"><a href="#运算" class="headerlink" title="运算"></a>运算</h5><blockquote>
<p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#functions-and-methods-overview" target="_blank" rel="external">choose, compress, cumprod, cumsum, inner, ndarray.fill, imag, prod, put, putmask, real, sum</a></p>
</blockquote>
<h5 id="基本统计"><a href="#基本统计" class="headerlink" title="基本统计"></a>基本统计</h5><blockquote>
<p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#functions-and-methods-overview" target="_blank" rel="external">cov, mean, std, var</a></p>
</blockquote>
<h5 id="基本线性代数"><a href="#基本线性代数" class="headerlink" title="基本线性代数"></a>基本线性代数</h5><blockquote>
<p><a href="https://docs.scipy.org/doc/numpy-dev/user/quickstart.html#functions-and-methods-overview" target="_blank" rel="external">cross, dot, outer, linalg.svd, vdot</a></p>
</blockquote>
<h2 id="5-Less-Basic"><a href="#5-Less-Basic" class="headerlink" title="5 Less Basic"></a>5 Less Basic</h2><h3 id="5-1-广播规则"><a href="#5-1-广播规则" class="headerlink" title="5.1 广播规则"></a>5.1 广播规则</h3><p>广播允许通用功能用一种有意义的方式去处理不完全相同的形状输入。<br>第一条广播规则是如果所有输入的数组都没有相同的维度数字，那么将会重复地用 \( 1 \) 去加在较小的数组形状上直到所有的数组有相同的维度数字。<br>第二条广播规则是确保沿着特定维度大小为 \( 1 \) 的数组就像沿着这个维度最大维数大小一样的，假设数组元素的值在广播数组的维度是相同的。<br>应用广播规则后，所有数组大小不必须匹配。更多细节可以在<a href="https://docs.scipy.org/doc/numpy-dev/user/basics.broadcasting.html" target="_blank" rel="external">Broadcasting</a>。</p>
<ul>
<li><a href="http://blog.csdn.net/yangnanhai93/article/details/50127747" target="_blank" rel="external">Numpy中的广播(Broadcasting)</a></li>
</ul>
<h2 id="6-花式索引和索引技巧"><a href="#6-花式索引和索引技巧" class="headerlink" title="6 花式索引和索引技巧"></a>6 花式索引和索引技巧</h2><p>\( NumPy \) 提供了比 \(  Python \) 序列更多的索引功能。除了我们之前看到的通过整数和切片索引之外，数组可以通过整数数组和布尔数组索引。</p>
<h3 id="6-1-用索引数组索引"><a href="#6-1-用索引数组索引" class="headerlink" title="6.1 用索引数组索引"></a>6.1 用索引数组索引</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(12)**2                       # the first 12 square numbers</div><div class="line">&gt;&gt;&gt; i = np.array( [ 1,1,3,8,5 ] )              # an array of indices</div><div class="line">&gt;&gt;&gt; a[i]                                       # the elements of a at the positions i</div><div class="line">array([ 1,  1,  9, 64, 25])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; j = np.array( [ [ 3, 4], [ 9, 7 ] ] )      # a bidimensional array of indices</div><div class="line">&gt;&gt;&gt; a[j]                                       # the same shape as j</div><div class="line">array([[ 9, 16],</div><div class="line">       [81, 49]])</div></pre></td></tr></table></figure>
<p>当数组 <code>a</code> 是多维的，单个数组指向数组 <code>a</code> 的第一维。以下示例通过使用调色板将标签图像转换为彩色图像来显示此行为。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; palette = np.array( [ [0,0,0],                # black</div><div class="line">...                       [255,0,0],              # red</div><div class="line">...                       [0,255,0],              # green</div><div class="line">...                       [0,0,255],              # blue</div><div class="line">...                       [255,255,255] ] )       # white</div><div class="line">&gt;&gt;&gt; image = np.array( [ [ 0, 1, 2, 0 ],           # each value corresponds to a color in the palette</div><div class="line">...                     [ 0, 3, 4, 0 ]  ] )</div><div class="line">&gt;&gt;&gt; palette[image]                            # the (2,4,3) color image</div><div class="line">array([[[  0,   0,   0],</div><div class="line">        [255,   0,   0],</div><div class="line">        [  0, 255,   0],</div><div class="line">        [  0,   0,   0]],</div><div class="line">       [[  0,   0,   0],</div><div class="line">        [  0,   0, 255],</div><div class="line">        [255, 255, 255],</div><div class="line">        [  0,   0,   0]]])</div></pre></td></tr></table></figure>
<p>我们可以给超过一维的索引。数组每个维度的索引形状必须一样。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(12).reshape(3,4)</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[ 0,  1,  2,  3],</div><div class="line">       [ 4,  5,  6,  7],</div><div class="line">       [ 8,  9, 10, 11]])</div><div class="line">&gt;&gt;&gt; i = np.array( [ [0,1],                        # indices for the first dim of a</div><div class="line">...                 [1,2] ] )</div><div class="line">&gt;&gt;&gt; j = np.array( [ [2,1],                        # indices for the second dim</div><div class="line">...                 [3,3] ] )</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; a[i,j]                                     # i and j must have equal shape</div><div class="line">array([[ 2,  5],</div><div class="line">       [ 7, 11]])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; a[i,2]</div><div class="line">array([[ 2,  6],</div><div class="line">       [ 6, 10]])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; a[:,j]                                     # i.e., a[ : , j]</div><div class="line">array([[[ 2,  1],</div><div class="line">        [ 3,  3]],</div><div class="line">       [[ 6,  5],</div><div class="line">        [ 7,  7]],</div><div class="line">       [[10,  9],</div><div class="line">        [11, 11]]])</div></pre></td></tr></table></figure>
<p>当然，我们可以把 <code>i</code>, <code>j</code> 放进一个序列然后对这个列表进行索引。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; l = [i,j]</div><div class="line">&gt;&gt;&gt; a[l]                                       # equivalent to a[i,j]</div><div class="line">array([[ 2,  5],</div><div class="line">       [ 7, 11]])</div></pre></td></tr></table></figure>
<p>然而，我们可以直接把 <code>i</code>，<code>j</code> 放进数组中，因为这个数组将会被解释成 <code>a</code> 第一维的索引。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; s = np.array( [i,j] )</div><div class="line">&gt;&gt;&gt; a[s]                                       # not what we want</div><div class="line">Traceback (most recent call last):</div><div class="line">  File &quot;&lt;stdin&gt;&quot;, line 1, in ?</div><div class="line">IndexError: index (3) out of range (0&lt;=index&lt;=2) in dimension 0</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; a[tuple(s)]                                # same as a[i,j]</div><div class="line">array([[ 2,  5],</div><div class="line">       [ 7, 11]])</div></pre></td></tr></table></figure>
<p>另一个常用数组索引是查询时间相关系列的最大值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; time = np.linspace(20, 145, 5)                 # time scale</div><div class="line">&gt;&gt;&gt; data = np.sin(np.arange(20)).reshape(5,4)      # 4 time-dependent series</div><div class="line">&gt;&gt;&gt; time</div><div class="line">array([  20.  ,   51.25,   82.5 ,  113.75,  145.  ])</div><div class="line">&gt;&gt;&gt; data</div><div class="line">array([[ 0.        ,  0.84147098,  0.90929743,  0.14112001],</div><div class="line">       [-0.7568025 , -0.95892427, -0.2794155 ,  0.6569866 ],</div><div class="line">       [ 0.98935825,  0.41211849, -0.54402111, -0.99999021],</div><div class="line">       [-0.53657292,  0.42016704,  0.99060736,  0.65028784],</div><div class="line">       [-0.28790332, -0.96139749, -0.75098725,  0.14987721]])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; ind = data.argmax(axis=0)                   # index of the maxima for each series</div><div class="line">&gt;&gt;&gt; ind</div><div class="line">array([2, 0, 3, 1])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; time_max = time[ ind]                       # times corresponding to the maxima</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; data_max = data[ind, xrange(data.shape[1])] # =&gt; data[ind[0],0], data[ind[1],1]...</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; time_max</div><div class="line">array([  82.5 ,   20.  ,  113.75,   51.25])</div><div class="line">&gt;&gt;&gt; data_max</div><div class="line">array([ 0.98935825,  0.84147098,  0.99060736,  0.6569866 ])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; np.all(data_max == data.max(axis=0))</div><div class="line">True</div></pre></td></tr></table></figure>
<p>你也可以使用数组索引对数组进行赋值：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(5)</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([0, 1, 2, 3, 4])</div><div class="line">&gt;&gt;&gt; a[[1,3,4]] = 0</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([0, 0, 2, 0, 0])</div></pre></td></tr></table></figure>
<p>然而，当你的列表索引包含重复，这个赋值会发生几次，保留最后一个数值。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(5)</div><div class="line">&gt;&gt;&gt; a[[0,0,2]]=[1,2,3]</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([2, 1, 3, 3, 4])</div></pre></td></tr></table></figure>
<p>这足够合理，但是如果你想使用 \( Python \) 的 <code>+=</code> 结构时要小心，它可能不像你期待的一样：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(5)</div><div class="line">&gt;&gt;&gt; a[[0,0,2]]+=1</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([1, 1, 3, 3, 4])</div></pre></td></tr></table></figure>
<p>即使 \( 0 \) 在列表中出现了两次，这第 \( 0 \) 个元素也只增加一次。这是因为  \( Python \) 把 “a+=1” 等价于 “a=a+1”。</p>
<h3 id="6-2-用布尔数组索引"><a href="#6-2-用布尔数组索引" class="headerlink" title="6.2 用布尔数组索引"></a>6.2 用布尔数组索引</h3><p>当我们用整数数组去索引数组时，我们提供了索引列表去挑选。用布尔索引的方法是不用的；我们明确的在数组中选择哪个我们想要哪个我们不想要。<br>最自然能想到的方法是用和原数组一样形状的布尔数组进行布尔索引。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(12).reshape(3,4)</div><div class="line">&gt;&gt;&gt; b = a &gt; 4</div><div class="line">&gt;&gt;&gt; b                                          # b is a boolean with a&apos;s shape</div><div class="line">array([[False, False, False, False],</div><div class="line">       [False,  True,  True,  True],</div><div class="line">       [ True,  True,  True,  True]], dtype=bool)</div><div class="line">&gt;&gt;&gt; a[b]                                       # 1d array with the selected elements</div><div class="line">array([ 5,  6,  7,  8,  9, 10, 11])</div></pre></td></tr></table></figure>
<p>这个属性在复制时非常有用。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a[b] = 0                                   # All elements of &apos;a&apos; higher than 4 become 0</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[0, 1, 2, 3],</div><div class="line">       [4, 0, 0, 0],</div><div class="line">       [0, 0, 0, 0]])</div></pre></td></tr></table></figure>
<p>你可以看接下来的例子去了解如何使用布尔索引去生成 <a href="http://en.wikipedia.org/wiki/Mandelbrot_set" target="_blank" rel="external">Mandelbrot set</a> 图像。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import numpy as np</div><div class="line">&gt;&gt;&gt; import matplotlib.pyplot as plt</div><div class="line">&gt;&gt;&gt; def mandelbrot( h,w, maxit=20 ):</div><div class="line">...     &quot;&quot;&quot;Returns an image of the Mandelbrot fractal of size (h,w).&quot;&quot;&quot;</div><div class="line">...     y,x = np.ogrid[ -1.4:1.4:h*1j, -2:0.8:w*1j ]</div><div class="line">...     c = x+y*1j</div><div class="line">...     z = c</div><div class="line">...     divtime = maxit + np.zeros(z.shape, dtype=int)</div><div class="line">...</div><div class="line">...     for i in range(maxit):</div><div class="line">...         z = z**2 + c</div><div class="line">...         diverge = z*np.conj(z) &gt; 2**2            # who is diverging</div><div class="line">...         div_now = diverge &amp; (divtime==maxit)  # who is diverging now</div><div class="line">...         divtime[div_now] = i                  # note when</div><div class="line">...         z[diverge] = 2                        # avoid diverging too much</div><div class="line">...</div><div class="line">...     return divtime</div><div class="line">&gt;&gt;&gt; plt.imshow(mandelbrot(400,400))</div><div class="line">&gt;&gt;&gt; plt.show()</div></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">mandelbrot</span><span class="params">(h, w, maxit=<span class="number">20</span>)</span>:</span></div><div class="line">    y, x = np.ogrid[<span class="number">-1.4</span>:<span class="number">1.4</span>:h * <span class="number">1j</span>, <span class="number">-2</span>:<span class="number">0.8</span>:w * <span class="number">1j</span>]</div><div class="line">    c = x + y * <span class="number">1j</span></div><div class="line">    z = c</div><div class="line">    divtime = maxit + np.zeros(z.shape, dtype=int)</div><div class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(maxit):</div><div class="line">        z = z ** <span class="number">2</span> + c</div><div class="line">        diverge = z * np.conj(z) &gt; <span class="number">2</span> ** <span class="number">2</span>  <span class="comment"># who is diverging</span></div><div class="line">        div_now = diverge &amp; (divtime == maxit)  <span class="comment"># who is diverging now</span></div><div class="line">        divtime[div_now] = i  <span class="comment"># note when</span></div><div class="line">        z[diverge] = <span class="number">2</span>  <span class="comment"># avoid diverging too much</span></div><div class="line">    <span class="keyword">return</span> divtime</div><div class="line"></div><div class="line">plt.imshow(mandelbrot(<span class="number">400</span>, <span class="number">400</span>))</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/04/NumPyOfficialQuickstartTutorial/6_2_01.png" alt=""></p>
<p>第二种用布尔索引方法更像是整数索引，对于每个数组的维度，我们给一个 \( 1D \) 的布尔数组去选择我们想要的切片。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(12).reshape(3,4)</div><div class="line">&gt;&gt;&gt; b1 = np.array([False,True,True])             # first dim selection</div><div class="line">&gt;&gt;&gt; b2 = np.array([True,False,True,False])       # second dim selection</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; a[b1,:]                                   # selecting rows</div><div class="line">array([[ 4,  5,  6,  7],</div><div class="line">       [ 8,  9, 10, 11]])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; a[b1]                                     # same thing</div><div class="line">array([[ 4,  5,  6,  7],</div><div class="line">       [ 8,  9, 10, 11]])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; a[:,b2]                                   # selecting columns</div><div class="line">array([[ 0,  2],</div><div class="line">       [ 4,  6],</div><div class="line">       [ 8, 10]])</div><div class="line">&gt;&gt;&gt;</div><div class="line">&gt;&gt;&gt; a[b1,b2]                                  # a weird thing to do</div><div class="line">array([ 4, 10])</div></pre></td></tr></table></figure>
<p>请注意，\( 1D \) 布尔数组的长度必须与你要切片的维度（或<code>axis</code>）的长度一致。在之前的例子中，<code>b1</code> 是一个长度为 \( 3 \)（<code>a</code> 的行数） 的 <code>1-rank</code> 数组，<code>b2</code> （长度为 \( 4 \)）是一个适合去索引 <code>a</code> 的第二 <code>rank</code> (列)。</p>
<h3 id="6-3-ix-函数"><a href="#6-3-ix-函数" class="headerlink" title="6.3 ix_() 函数"></a>6.3 ix_() 函数</h3><p><a href="https://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.ix_.html#numpy.ix_" target="_blank" rel="external">ix_</a> 可以组合不同向量去获得对于每一个 <code>n-uplet</code> 的结果。例如，如果你想从每个 \( a, b, c \) 向量中取得三元组去计算所有的 \( a+b*c \) ：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.array([2,3,4,5])</div><div class="line">&gt;&gt;&gt; b = np.array([8,5,4])</div><div class="line">&gt;&gt;&gt; c = np.array([5,4,6,8,3])</div><div class="line">&gt;&gt;&gt; ax,bx,cx = np.ix_(a,b,c)</div><div class="line">&gt;&gt;&gt; ax</div><div class="line">array([[[2]],</div><div class="line">       [[3]],</div><div class="line">       [[4]],</div><div class="line">       [[5]]])</div><div class="line">&gt;&gt;&gt; bx</div><div class="line">array([[[8],</div><div class="line">        [5],</div><div class="line">        [4]]])</div><div class="line">&gt;&gt;&gt; cx</div><div class="line">array([[[5, 4, 6, 8, 3]]])</div><div class="line">&gt;&gt;&gt; ax.shape, bx.shape, cx.shape</div><div class="line">((4, 1, 1), (1, 3, 1), (1, 1, 5))</div><div class="line">&gt;&gt;&gt; result = ax+bx*cx</div><div class="line">&gt;&gt;&gt; result</div><div class="line">array([[[42, 34, 50, 66, 26],</div><div class="line">        [27, 22, 32, 42, 17],</div><div class="line">        [22, 18, 26, 34, 14]],</div><div class="line">       [[43, 35, 51, 67, 27],</div><div class="line">        [28, 23, 33, 43, 18],</div><div class="line">        [23, 19, 27, 35, 15]],</div><div class="line">       [[44, 36, 52, 68, 28],</div><div class="line">        [29, 24, 34, 44, 19],</div><div class="line">        [24, 20, 28, 36, 16]],</div><div class="line">       [[45, 37, 53, 69, 29],</div><div class="line">        [30, 25, 35, 45, 20],</div><div class="line">        [25, 21, 29, 37, 17]]])</div><div class="line">&gt;&gt;&gt; result[3,2,4]</div><div class="line">17</div><div class="line">&gt;&gt;&gt; a[3]+b[2]*c[4]</div><div class="line">17</div></pre></td></tr></table></figure>
<p>你也可以按以下方式实现：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; def ufunc_reduce(ufct, *vectors):</div><div class="line">...    vs = np.ix_(*vectors)</div><div class="line">...    r = ufct.identity</div><div class="line">...    for v in vs:</div><div class="line">...        r = ufct(r,v)</div><div class="line">...    return r</div></pre></td></tr></table></figure>
<p>然后这样使用：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; ufunc_reduce(np.add,a,b,c)</div><div class="line">array([[[15, 14, 16, 18, 13],</div><div class="line">        [12, 11, 13, 15, 10],</div><div class="line">        [11, 10, 12, 14,  9]],</div><div class="line">       [[16, 15, 17, 19, 14],</div><div class="line">        [13, 12, 14, 16, 11],</div><div class="line">        [12, 11, 13, 15, 10]],</div><div class="line">       [[17, 16, 18, 20, 15],</div><div class="line">        [14, 13, 15, 17, 12],</div><div class="line">        [13, 12, 14, 16, 11]],</div><div class="line">       [[18, 17, 19, 21, 16],</div><div class="line">        [15, 14, 16, 18, 13],</div><div class="line">        [14, 13, 15, 17, 12]]])</div></pre></td></tr></table></figure>
<p>这个版本的比通常的 <code>ufunc.reduce</code> 好在它使用了<a href="https://docs.scipy.org/doc/numpy-dev/user/Tentative_NumPy_Tutorial.html#head-c43f3f81719d84f09ae2b33a22eaf50b26333db8" target="_blank" rel="external">Broadcasting Rules</a> 规则去避免创建一个大小是输出乘以矢量数量数组。</p>
<h3 id="6-4-使用字符串索引"><a href="#6-4-使用字符串索引" class="headerlink" title="6.4 使用字符串索引"></a>6.4 使用字符串索引</h3><p>参考<a href="https://docs.scipy.org/doc/numpy-dev/user/basics.rec.html#structured-arrays" target="_blank" rel="external">Structured arrays</a></p>
<h2 id="7-线性代数"><a href="#7-线性代数" class="headerlink" title="7 线性代数"></a>7 线性代数</h2><p>工作进行中，基本的线性代数包含在其中。</p>
<h3 id="7-1-简单的数组操作"><a href="#7-1-简单的数组操作" class="headerlink" title="7.1 简单的数组操作"></a>7.1 简单的数组操作</h3><p>看\( NumPy \) 文件夹下的 \( linalg.py \) 文件了解更多。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import numpy as np</div><div class="line">&gt;&gt;&gt; a = np.array([[1.0, 2.0], [3.0, 4.0]])</div><div class="line">&gt;&gt;&gt; print(a)</div><div class="line">[[ 1.  2.]</div><div class="line"> [ 3.  4.]]</div><div class="line"></div><div class="line">&gt;&gt;&gt; a.transpose()</div><div class="line">array([[ 1.,  3.],</div><div class="line">       [ 2.,  4.]])</div><div class="line"></div><div class="line">&gt;&gt;&gt; np.linalg.inv(a)</div><div class="line">array([[-2. ,  1. ],</div><div class="line">       [ 1.5, -0.5]])</div><div class="line"></div><div class="line">&gt;&gt;&gt; u = np.eye(2) # unit 2x2 matrix; &quot;eye&quot; represents &quot;I&quot;</div><div class="line">&gt;&gt;&gt; u</div><div class="line">array([[ 1.,  0.],</div><div class="line">       [ 0.,  1.]])</div><div class="line">&gt;&gt;&gt; j = np.array([[0.0, -1.0], [1.0, 0.0]])</div><div class="line"></div><div class="line">&gt;&gt;&gt; np.dot (j, j) # matrix product</div><div class="line">array([[-1.,  0.],</div><div class="line">       [ 0., -1.]])</div><div class="line"></div><div class="line">&gt;&gt;&gt; np.trace(u)  # trace</div><div class="line">2.0</div><div class="line"></div><div class="line">&gt;&gt;&gt; y = np.array([[5.], [7.]])</div><div class="line">&gt;&gt;&gt; np.linalg.solve(a, y)</div><div class="line">array([[-3.],</div><div class="line">       [ 4.]])</div><div class="line"></div><div class="line">&gt;&gt;&gt; np.linalg.eig(j)</div><div class="line">(array([ 0.+1.j,  0.-1.j]), array([[ 0.70710678+0.j        ,  0.70710678-0.j        ],</div><div class="line">       [ 0.00000000-0.70710678j,  0.00000000+0.70710678j]]))</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">Parameters:</div><div class="line">    square matrix</div><div class="line">Returns</div><div class="line">    The eigenvalues, each repeated according to its multiplicity.</div><div class="line">    The normalized (unit &quot;length&quot;) eigenvectors, such that the</div><div class="line">    column ``v[:,i]`` is the eigenvector corresponding to the</div><div class="line">    eigenvalue ``w[i]`` .</div></pre></td></tr></table></figure>
<h2 id="8-技巧和提示"><a href="#8-技巧和提示" class="headerlink" title="8 技巧和提示"></a>8 技巧和提示</h2><p>这里我们给出一些有用的小技巧。</p>
<h3 id="8-1-“自动塑形”"><a href="#8-1-“自动塑形”" class="headerlink" title="8.1 “自动塑形”"></a>8.1 “自动塑形”</h3><p>为了改变数组的维度，你可以省略一个可以自动被推算出来的大小的参数。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; a = np.arange(30)</div><div class="line">&gt;&gt;&gt; a.shape = 2,-1,3  # -1 means &quot;whatever is needed&quot;</div><div class="line">&gt;&gt;&gt; a.shape</div><div class="line">(2, 5, 3)</div><div class="line">&gt;&gt;&gt; a</div><div class="line">array([[[ 0,  1,  2],</div><div class="line">        [ 3,  4,  5],</div><div class="line">        [ 6,  7,  8],</div><div class="line">        [ 9, 10, 11],</div><div class="line">        [12, 13, 14]],</div><div class="line">       [[15, 16, 17],</div><div class="line">        [18, 19, 20],</div><div class="line">        [21, 22, 23],</div><div class="line">        [24, 25, 26],</div><div class="line">        [27, 28, 29]]])</div></pre></td></tr></table></figure>
<h3 id="8-2-矢量叠加"><a href="#8-2-矢量叠加" class="headerlink" title="8.2 矢量叠加"></a>8.2 矢量叠加</h3><p>我们怎么从一个相同大小的行向量构造出一个 \( 2D \) 数组？在 \( MATLAB \)中是相当简单的：如果 <code>x</code> 和 <code>y</code> 是两个相同长度的向量，你只需要把 <code>m=[x;y]</code>。在 \( NumPy \) 中，通过函数 <code>column_stack</code>，<code>dstack</code>，<code>hstack</code> 和 <code>vstack</code> 实现，这取决于所要叠加的维度。例如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">x = np.arange(0,10,2)                     # x=([0,2,4,6,8])</div><div class="line">y = np.arange(5)                          # y=([0,1,2,3,4])</div><div class="line">m = np.vstack([x,y])                      # m=([[0,2,4,6,8],</div><div class="line">                                          #     [0,1,2,3,4]])</div><div class="line">xy = np.hstack([x,y])                     # xy =([0,2,4,6,8,0,1,2,3,4])</div></pre></td></tr></table></figure>
<p>在超过两个维度时这些函数背后的逻辑是奇怪的。</p>
<h5 id="参考-5"><a href="#参考-5" class="headerlink" title="参考"></a>参考</h5><p><a href="https://docs.scipy.org/doc/numpy-dev/user/numpy-for-matlab-users.html" target="_blank" rel="external">NumPy for Matlab users</a></p>
<h3 id="8-3-柱状图"><a href="#8-3-柱状图" class="headerlink" title="8.3 柱状图"></a>8.3 柱状图</h3><p>\( NumPy \) 的 <code>histogram</code> 函数应用于数组，返回一对矢量：数组的柱状图和 <code>bins</code> 矢量。当心：<code>matplotlib</code> 也有一个函数去构建柱状图（叫做 <code>hist</code>，同样在 \( Matlab \) 中），这个和 \( NumPy \) 还是不一样的。主要的区别是 <code>pylab.hist</code> 自动绘制柱状图而 <code>matplotlib</code> 只是生成数据。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; import numpy as np</div><div class="line">&gt;&gt;&gt; import matplotlib.pyplot as plt</div><div class="line">&gt;&gt;&gt; # Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2</div><div class="line">&gt;&gt;&gt; mu, sigma = 2, 0.5</div><div class="line">&gt;&gt;&gt; v = np.random.normal(mu,sigma,10000)</div><div class="line">&gt;&gt;&gt; # Plot a normalized histogram with 50 bins</div><div class="line">&gt;&gt;&gt; plt.hist(v, bins=50, normed=1)       # matplotlib version (plot)</div><div class="line">&gt;&gt;&gt; plt.show()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line"># Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2</div><div class="line">mu, sigma = 2, 0.5</div><div class="line">v = np.random.normal(mu, sigma, 10000)</div><div class="line"># Plot a normalized histogram with 50 bins</div><div class="line">plt.hist(v, bins=50, normed=1)  # matplotlib version (plot)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/04/NumPyOfficialQuickstartTutorial/8_3_01.png" alt=""></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">&gt;&gt;&gt; # Compute the histogram with numpy and then plot it</div><div class="line">&gt;&gt;&gt; (n, bins) = np.histogram(v, bins=50, normed=True)  # NumPy version (no plot)</div><div class="line">&gt;&gt;&gt; plt.plot(.5*(bins[1:]+bins[:-1]), n)</div><div class="line">&gt;&gt;&gt; plt.show()</div></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">import numpy as np</div><div class="line">import matplotlib.pyplot as plt</div><div class="line"></div><div class="line"># Build a vector of 10000 normal deviates with variance 0.5^2 and mean 2</div><div class="line">mu, sigma = 2, 0.5</div><div class="line">v = np.random.normal(mu, sigma, 10000)</div><div class="line"># Compute the histogram with numpy and then plot it</div><div class="line">(n, bins) = np.histogram(v, bins=50, normed=True)  # NumPy version (no plot)</div><div class="line">plt.plot(.5 * (bins[1:] + bins[:-1]), n)</div><div class="line">plt.show()</div></pre></td></tr></table></figure>
<p><img src="/2018/02/04/NumPyOfficialQuickstartTutorial/8_3_02.png" alt=""></p>
<h2 id="9-更多阅读"><a href="#9-更多阅读" class="headerlink" title="9 更多阅读"></a>9 更多阅读</h2><ul>
<li>The <a href="http://docs.python.org/tutorial/" target="_blank" rel="external">Python tutorial</a></li>
<li><a href="https://docs.scipy.org/doc/numpy-dev/reference/index.html#reference" target="_blank" rel="external">NumPy Reference</a></li>
<li><a href="https://docs.scipy.org/doc/scipy/reference/tutorial/index.html" target="_blank" rel="external">SciPy Tutorial</a></li>
<li><a href="http://www.scipy-lectures.org/" target="_blank" rel="external">SciPy Lecture Notes</a></li>
<li>A <a href="http://mathesaurus.sf.net/" target="_blank" rel="external">matlab, R, IDL, NumPy/SciPy dictionary</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：本来是学习下 \( NumPy \)，看到官网的&lt;a href=&quot;https://docs.scipy.org/doc/numpy-dev/user/quickstart.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;入门教程&lt;/a&gt;想跟着实验一下，怕不常用，而我这人健忘，所以记录下来。索性就照着翻译一下，同样可以提升自己的阅读和写作能力，需要的可以存一下。当然，本人水平有限，有错误的地方欢迎大家指正。这里是基于 &lt;a href=&quot;https://docs.scipy.org/doc/numpy-dev/user/quickstart.html&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;\( NumPy\ v1.13.dev0\ Manual \)&lt;/a&gt; 翻译的。截止时间\( 2018/02/04 \)&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
      <category term="NumPy" scheme="http://fitzeng.org/tags/NumPy/"/>
    
  </entry>
  
  <entry>
    <title>从 TensorFlow 入门机器学习</title>
    <link href="http://fitzeng.org/2018/02/03/TensorFlowIntroduction/"/>
    <id>http://fitzeng.org/2018/02/03/TensorFlowIntroduction/</id>
    <published>2018-02-03T03:27:00.000Z</published>
    <updated>2018-02-03T02:16:06.149Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<blockquote>
<p>写在前面：紧跟时代步伐，开始学习机器学习，抱着争取在毕业之前多看看各个方向是什么样子的心态，发现这是一个很有潜力也很有趣的领域（keng）。// 然后就开始补数学了……</p>
</blockquote>
<a id="more"></a>
<h2 id="0-TensorFlow-介绍"><a href="#0-TensorFlow-介绍" class="headerlink" title="0 TensorFlow 介绍"></a>0 TensorFlow 介绍</h2><p>刚刚入门的小白，理解不深，直接看官方的介绍吧</p>
<blockquote>
<p>GitHub Description: Computation using data flow graphs for scalable machine learning</p>
<p><a href="https://www.tensorflow.org/" target="_blank" rel="external">官网</a>: \( TensorFlow^{^{TM}} \)是一个使用数据流图进行数值计算的开源软件库。图中的节点代表数学运算， 而图中的边则代表在这些节点之间传递的多维数组（张量）。</p>
</blockquote>
<h3 id="0-1-什么是-TensorFlow"><a href="#0-1-什么是-TensorFlow" class="headerlink" title="0.1 什么是 TensorFlow ?"></a>0.1 什么是 TensorFlow ?</h3><p>\( Tensor \) 是张量的意思，\( Flow \) 是流的意思。所以可以直接看做利用张量组成的数据流图进行计算的一个开源库。</p>
<h3 id="0-2-TensorFlow-可以做什么"><a href="#0-2-TensorFlow-可以做什么" class="headerlink" title="0.2 TensorFlow 可以做什么 ?"></a>0.2 TensorFlow 可以做什么 ?</h3><p>目前主要是用于机器学习，这样说有点不亲民，笔者理解是可以将数据转化为向量描述并且构建相应的计算流图都是可以使用的。<br>举个例子吧，虽然不知道恰不恰当。<br>比如我们在计算 \( (1 + 2)*3-4 \) 时，可以构建一个二叉树</p>
<p><img src="/2018/02/03/TensorFlowIntroduction/0_2_01.png" alt=""></p>
<p>这棵二叉树的中序遍历就是上面的表达式，也就是说这个表达式可以转化成一个形如二叉树的图，而 \( TensorFlow \) 正好可以计算这个图。下面给出代码，看不懂没关系，只要理解代码流程是对图(二叉树)的计算就可以了，下一章会介绍如何使用\( TensorFlow \)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">a, b, c, d = tf.constant(<span class="number">1</span>), tf.constant(<span class="number">2</span>), tf.constant(<span class="number">3</span>),tf.constant(<span class="number">4</span>)</div><div class="line">add = tf.add(a,b)</div><div class="line">mul = tf.multiply(add, c)</div><div class="line">sub = tf.subtract(mul, d)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    print(sess.run(sub))</div><div class="line"><span class="comment"># output: </span></div><div class="line"><span class="comment"># 5</span></div></pre></td></tr></table></figure>
<h3 id="0-3-TensorFlow-安装"><a href="#0-3-TensorFlow-安装" class="headerlink" title="0.3 TensorFlow 安装"></a>0.3 TensorFlow 安装</h3><p>这里就不做详细介绍了，相信点开这篇文章的你应该有了运行环境。如果没有这里推荐两个网站<a href="https://www.tensorflow.org/install/" target="_blank" rel="external">英文:官网</a> 和 <a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/os_setup.html" target="_blank" rel="external">中文:**学院翻译</a><br>然后介绍一下我的环境：\( Anaconda + PyCharm \)</p>
<p>注意 \( PyCharm:\ Project\ Interpreter \) 设置为 \( Conda\ Environment \) 才能跑 \( TensorFlow \)。如果不会可以多看看网上的教程，能对虚拟环境加深了解。</p>
<h2 id="1-初识-TensorFlow"><a href="#1-初识-TensorFlow" class="headerlink" title="1 初识 TensorFlow"></a>1 初识 TensorFlow</h2><p>好了，有前面的介绍，你应该有能够使用 \( TensorFlow \) 的环境了，下面开始介绍如何编码。</p>
<h3 id="1-1-基础语法"><a href="#1-1-基础语法" class="headerlink" title="1.1 基础语法"></a>1.1 基础语法</h3><blockquote>
<p>其实说语法是不准确的，语法就是 \( Python \) 的语法(这里使用 Python)，主要是介绍调用这个计算库来实现这个特殊的计算。同样摆上<a href="https://www.tensorflow.org/get_started/" target="_blank" rel="external">官网教程</a></p>
</blockquote>
<h4 id="1-1-1-计算单元介绍"><a href="#1-1-1-计算单元介绍" class="headerlink" title="1.1.1 计算单元介绍"></a>1.1.1 计算单元介绍</h4><p>可以看到在计算图中，有两个主要的内容是点(叶子和非叶子节点)和线。我的理解是点代表数据，线代表操作。不知道对不对，不过下面就按照这样思路介绍了。<br>下面开始介绍有哪些常用的“点”：</p>
<p><code>常量</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">c = tf.constant(<span class="number">2</span>)</div></pre></td></tr></table></figure>
<p><code>变量</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">v = tf.Variable(<span class="number">2</span>)</div></pre></td></tr></table></figure>
<p><code>占位符</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">p = tf.placeholder(tf.float32)</div></pre></td></tr></table></figure>
<p>以上代码都是以最小能用原则传的参，感兴趣的可以去看看源码，这里主要是往 \( Python \) 语法上拉，先使用起来再以后自己深究为什么要设计成这样的数据结构对计算图是必须的。</p>
<p>接下来就是有哪些“线”：</p>
<p><code>四则运算</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">add = tf.add(a, b)</div><div class="line">sub = tf.subtract(a, b)</div><div class="line">mul = tf.multiply(a, b)</div><div class="line">div = tf.divide(a, b)</div></pre></td></tr></table></figure>
<p>其他的就不再介绍了，详情可看 \( XXX\<em> ops.py \) 的源码。比如以上的操作定义在 \( math\</em> ops.py \)。</p>
<h4 id="1-1-2-计算流程介绍"><a href="#1-1-2-计算流程介绍" class="headerlink" title="1.1.2 计算流程介绍"></a>1.1.2 计算流程介绍</h4><p>知道了常见数据和计算方法下面介绍计算流程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line"><span class="comment"># Step1: 创建数据</span></div><div class="line">a, b, c, d = tf.constant(<span class="number">1</span>), tf.constant(<span class="number">2</span>), tf.constant(<span class="number">3</span>),tf.constant(<span class="number">4</span>)</div><div class="line"></div><div class="line"><span class="comment"># Step2: 构造计算图</span></div><div class="line">add = tf.add(a,b)</div><div class="line">mul = tf.multiply(add, c)</div><div class="line">sub = tf.subtract(mul, d)</div><div class="line"></div><div class="line"><span class="comment"># Step3: 进行计算</span></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    print(sess.run(sub))</div></pre></td></tr></table></figure>
<p>上面这个例子是一个标准的常量计算过程，你可以试着 <code>print(a, add)</code> 看看你创建的是个什么东西，你会发现他是一个 \( Tensor \) 而且里面的值是 \( 0 \)。可以猜测，这里只打印不计算，看 \( Tensor \) 源码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># ops.py</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="keyword">return</span> <span class="string">"&lt;tf.Tensor '%s' shape=%s dtype=%s&gt;"</span> % (self.name, self.get_shape(),self._dtype.name)</div><div class="line"></div><div class="line"><span class="meta">@property</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">name</span><span class="params">(self)</span>:</span></div><div class="line">    <span class="string">"""The string name of this tensor."""</span></div><div class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self._op.name:</div><div class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"Operation was not named: %s"</span> % self._op)</div><div class="line">    <span class="keyword">return</span> <span class="string">"%s:%d"</span> % (self._op.name, self._value_index)</div></pre></td></tr></table></figure>
<p>学会了计算常量，变量是不是也一样？如果你试过就知道是不一样的，变量需要初始化操作。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">v = tf.Variable(<span class="number">2</span>)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    print(v, sess.run(v))</div></pre></td></tr></table></figure>
<p>到这里可能会疑问，那变量和常量有什么区别？从字面意思可以知道变量应该是可变的，方便我们在计算过程中随时调整参数，下面通过一段代码介绍如何使用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line">v = tf.Variable(<span class="number">2</span>)</div><div class="line"><span class="comment"># 将 v 的值自乘 2</span></div><div class="line">update = tf.assign(v, tf.multiply(v, tf.constant(<span class="number">2</span>)))</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>):</div><div class="line">        print(<span class="string">"-----------------------"</span>)</div><div class="line">        <span class="keyword">print</span> <span class="string">"Before : "</span>, sess.run(v)</div><div class="line">        sess.run(update)</div><div class="line">        <span class="keyword">print</span> <span class="string">"After : "</span>, sess.run(v)</div><div class="line"></div><div class="line"><span class="comment"># output:</span></div><div class="line"><span class="comment"># -----------------------</span></div><div class="line"><span class="comment"># Before :  2</span></div><div class="line"><span class="comment"># After :  4</span></div><div class="line"><span class="comment"># -----------------------</span></div><div class="line"><span class="comment"># Before :  4</span></div><div class="line"><span class="comment"># After :  8</span></div><div class="line"><span class="comment"># -----------------------</span></div><div class="line"><span class="comment"># Before :  8</span></div><div class="line"><span class="comment"># After :  16</span></div><div class="line"><span class="comment"># -----------------------</span></div><div class="line"><span class="comment"># Before :  16</span></div><div class="line"><span class="comment"># After :  32</span></div></pre></td></tr></table></figure>
<p>但是如果我们不想每次都设置-更新-计算-更新-计算……而是直接把数据写入计算，那占位符就起作用了。同样举个小例子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">c = tf.constant(<span class="number">2</span>)</div><div class="line"><span class="comment"># 注意类型一致，这里是 tf.int32</span></div><div class="line">p = tf.placeholder(tf.int32)</div><div class="line">mul = tf.multiply(c, p)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    <span class="comment"># tmp = 2 相当于上一个例子变量的初始值是 2</span></div><div class="line">    tmp = <span class="number">2</span>;</div><div class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">4</span>):</div><div class="line">        <span class="comment"># 直接填充 feed_dict</span></div><div class="line">        tmp = sess.run(mul, feed_dict=&#123;p:tmp&#125;)</div><div class="line">        <span class="keyword">print</span> tmp</div><div class="line"></div><div class="line"><span class="comment"># output:</span></div><div class="line"><span class="comment"># 4</span></div><div class="line"><span class="comment"># 8</span></div><div class="line"><span class="comment"># 16</span></div><div class="line"><span class="comment"># 32</span></div></pre></td></tr></table></figure>
<p>下面总结下计算过程：</p>
<ul>
<li>创建数据：可以创建常量、变量和占位符。</li>
<li>构建图：通过前面的数据构建一张图。</li>
<li>初始化：把变量初始化。</li>
<li>计算：必须通过开启一个 Session 来计算图</li>
</ul>
<h3 id="1-2-可视化"><a href="#1-2-可视化" class="headerlink" title="1.2 可视化"></a>1.2 可视化</h3><p>\( TensorFlow \)提供了一个可视化工具——\( TensorBoard \)，下面开始介绍如何使用。</p>
<p>这里对上面二叉树的例子进行可视化处理。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"></div><div class="line">a, b, c, d = tf.constant(<span class="number">1</span>), tf.constant(<span class="number">2</span>), tf.constant(<span class="number">3</span>),tf.constant(<span class="number">4</span>)</div><div class="line">add = tf.add(a,b)</div><div class="line">mul = tf.multiply(add, c)</div><div class="line">sub = tf.subtract(mul, d)</div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    writer = tf.summary.FileWriter(<span class="string">'./graphs'</span>, sess.graph)</div><div class="line">    print(sess.run(sub))</div><div class="line">writer.close()</div></pre></td></tr></table></figure>
<p>然后使用命令行到存储 <code>graphs</code> 的文件夹下执行 <code>tensorboard --logdir=&quot;./graphs&quot;</code> 命令，然后按照提示在浏览器中打开 <code>http://localhost:6006</code> 如果成功显示 \( TensorBoard \) 界面就说明成功了。</p>
<p><img src="/2018/02/03/TensorFlowIntroduction/1_2_01.png" alt=""></p>
<h2 id="2-利用-TensorFlow-进行机器学习"><a href="#2-利用-TensorFlow-进行机器学习" class="headerlink" title="2 利用 TensorFlow 进行机器学习"></a>2 利用 TensorFlow 进行机器学习</h2><p>这里也算是机器学习的入门介绍吧。直接介绍机器学习相关知识可能不太现实，而且笔者也是在学习阶段，所以举一些小例子来体会机器学习的过程吧。</p>
<h3 id="2-1-线性回归"><a href="#2-1-线性回归" class="headerlink" title="2.1 线性回归"></a>2.1 线性回归</h3><p>这里我们使用最熟悉的线性回归来体会一下机器学习的过程：</p>
<h4 id="2-1-1-准备数据"><a href="#2-1-1-准备数据" class="headerlink" title="2.1.1 准备数据"></a>2.1.1 准备数据</h4><p>这里很简单，就是模拟一个线性回归，所以我们直接自己拟定一些数据好预测结果和自己设想的是否一致。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">train_X = numpy.asarray([<span class="number">1.1</span>, <span class="number">1.8</span>, <span class="number">3.2</span>, <span class="number">4.7</span>, <span class="number">5.9</span>, <span class="number">6.7</span>])</div><div class="line">train_Y = numpy.asarray([<span class="number">1.2</span>, <span class="number">2.1</span>, <span class="number">3.1</span>, <span class="number">4.6</span>, <span class="number">5.5</span>, <span class="number">6.9</span>])</div></pre></td></tr></table></figure>
<h4 id="2-1-2-构建模型"><a href="#2-1-2-构建模型" class="headerlink" title="2.1.2 构建模型"></a>2.1.2 构建模型</h4><p>我们采用占位符的形式进行计算，在运算时直接导入数据便可。<br>这里因为我们采用线性回归，所以目标函数是形如 \( Y = XW + b \) 的形式的一次函数。也就是说，我们通过给出的点去拟合一条比较符合这些点分布的直线。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">X = tf.placeholder(tf.float32)</div><div class="line">Y = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">W = tf.Variable(<span class="number">-1.</span>, name=<span class="string">"weight"</span>)</div><div class="line">b = tf.Variable(<span class="number">-1.</span>, name=<span class="string">"bias"</span>)</div><div class="line"></div><div class="line"><span class="comment"># linear model </span></div><div class="line"><span class="comment"># activation = X*W + b</span></div><div class="line">activation = tf.add(tf.multiply(X, W), b)</div></pre></td></tr></table></figure>
<h4 id="2-1-3-参数评估"><a href="#2-1-3-参数评估" class="headerlink" title="2.1.3 参数评估"></a>2.1.3 参数评估</h4><p>我们采用每个点给出的纵坐标和线性模型算出的纵坐标的差\( (activation - Y) \)的平方和\( (tf.reduce\_ sum(tf.pow(activation - Y, 2))) \)作为损失函数，在训练中采用梯度下降算法尽量使和最小，学利率选择 \( 0.01 \)。其中的数学原理这里就不介绍了，以后会写关于机器学习算法的相关文章。<br>一般选取损失函数和通过某些最优化手段更新权重是这里的一大难点，如果要知道原理，需要学习大量数学基础知识（概率论，线性代数，微积分……）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">learning_rate = <span class="number">0.01</span></div><div class="line"></div><div class="line">cost = tf.reduce_sum(tf.pow(activation - Y, <span class="number">2</span>))</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</div></pre></td></tr></table></figure>
<h4 id="2-1-4-训练数据"><a href="#2-1-4-训练数据" class="headerlink" title="2.1.4 训练数据"></a>2.1.4 训练数据</h4><p>这里就是取数据，喂给图中的输入节点，然后模型会自己进行优化，可以将数据多次迭代使得拟合函数能够更好的适应这些数据点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div></pre></td><td class="code"><pre><div class="line">training_epochs = <span class="number">2000</span></div><div class="line">display_step = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</div><div class="line">        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> zip(train_X, train_Y):</div><div class="line">            sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;)</div><div class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>,</div><div class="line">                  <span class="string">"&#123;:.9f&#125;"</span>.format(sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;)), <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>,</div><div class="line">                  sess.run(b))</div><div class="line">    print(<span class="string">"Optimization Finished!"</span>)</div><div class="line">    print(<span class="string">"cost="</span>, sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;), <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b))</div><div class="line">```     </div><div class="line"></div><div class="line"><span class="comment">#### 2.1.5 可视化</span></div><div class="line">可以直接绘制二维图形看结果。不熟悉的可以参考[Matplotlib 教程](https://liam0205.me/<span class="number">2014</span>/<span class="number">09</span>/<span class="number">11</span>/matplotlib-tutorial-zh-cn/)</div><div class="line"></div><div class="line">```python</div><div class="line">    writer = tf.summary.FileWriter(<span class="string">'./graphs'</span>, sess.graph)</div><div class="line"></div><div class="line">    plt.scatter(train_X, train_Y, color=<span class="string">'red'</span>, label=<span class="string">'Original data'</span>)</div><div class="line">    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), color=<span class="string">'blue'</span>, label=<span class="string">'Fitted line'</span>)</div><div class="line">    plt.show()</div><div class="line">writer.close()</div></pre></td></tr></table></figure>
<p>二维图：<br><img src="/2018/02/03/TensorFlowIntroduction/2_1_5_01.png" alt=""><br>数据流图：<br><img src="/2018/02/03/TensorFlowIntroduction/2_1_5_02.png" alt=""></p>
<h4 id="2-1-6-小结"><a href="#2-1-6-小结" class="headerlink" title="2.1.6 小结"></a>2.1.6 小结</h4><p>其实整个过程如果不深究其中的原理，还是很好理解的，无非就是<code>提供数据-选取拟合函数-构建图-选取损失函数-最优化-训练数据（更新权重）-得出结论</code>。这个过程符合我们对线性回归这个问题解决的基本思路的预期。当然，笔者认为这只是开始，要想深入，学习必要的数学知识是机器学习的必经之路。</p>
<p>这里可以参考<a href="https://juejin.im/entry/5904542c570c350058168c76" target="_blank" rel="external">TensorFlow 入门</a><br>整体代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># coding: utf-8</span></div><div class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</div><div class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</div><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">train_X = numpy.asarray([<span class="number">1.1</span>, <span class="number">1.8</span>, <span class="number">3.2</span>, <span class="number">4.7</span>, <span class="number">5.9</span>, <span class="number">6.7</span>])</div><div class="line">train_Y = numpy.asarray([<span class="number">1.2</span>, <span class="number">2.1</span>, <span class="number">3.1</span>, <span class="number">4.6</span>, <span class="number">5.5</span>, <span class="number">6.9</span>])</div><div class="line"></div><div class="line">X = tf.placeholder(tf.float32)</div><div class="line">Y = tf.placeholder(tf.float32)</div><div class="line"></div><div class="line">W = tf.Variable(<span class="number">-1.</span>, name=<span class="string">"weight"</span>)</div><div class="line">b = tf.Variable(<span class="number">-1.</span>, name=<span class="string">"bias"</span>)</div><div class="line"></div><div class="line">activation = tf.add(tf.multiply(X, W), b)</div><div class="line"></div><div class="line">learning_rate = <span class="number">0.01</span></div><div class="line"></div><div class="line">cost = tf.reduce_sum(tf.pow(activation - Y, <span class="number">2</span>))</div><div class="line">optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)</div><div class="line"></div><div class="line">training_epochs = <span class="number">2000</span></div><div class="line">display_step = <span class="number">100</span></div><div class="line"></div><div class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</div><div class="line">    sess.run(tf.global_variables_initializer())</div><div class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(training_epochs):</div><div class="line">        <span class="keyword">for</span> (x, y) <span class="keyword">in</span> zip(train_X, train_Y):</div><div class="line">            sess.run(optimizer, feed_dict=&#123;X: x, Y: y&#125;)</div><div class="line">        <span class="keyword">if</span> epoch % display_step == <span class="number">0</span>:</div><div class="line">            print(<span class="string">"Epoch:"</span>, <span class="string">'%04d'</span> % (epoch + <span class="number">1</span>), <span class="string">"cost="</span>,</div><div class="line">                  <span class="string">"&#123;:.9f&#125;"</span>.format(sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;)), <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>,</div><div class="line">                  sess.run(b))</div><div class="line">    print(<span class="string">"Optimization Finished!"</span>)</div><div class="line">    print(<span class="string">"cost="</span>, sess.run(cost, feed_dict=&#123;X: train_X, Y: train_Y&#125;), <span class="string">"W="</span>, sess.run(W), <span class="string">"b="</span>, sess.run(b))</div><div class="line">    </div><div class="line">    writer = tf.summary.FileWriter(<span class="string">'./graphs'</span>, sess.graph)</div><div class="line"></div><div class="line">    plt.scatter(train_X, train_Y, color=<span class="string">'red'</span>, label=<span class="string">'Original data'</span>)</div><div class="line">    plt.plot(train_X, sess.run(W) * train_X + sess.run(b), color=<span class="string">'blue'</span>, label=<span class="string">'Fitted line'</span>)</div><div class="line">    plt.show()</div><div class="line">writer.close()</div><div class="line"></div><div class="line"><span class="comment"># output:</span></div><div class="line"><span class="comment"># Epoch: 0001 cost= 0.785177052 W= 1.07263 b= -0.448403</span></div><div class="line"><span class="comment"># Epoch: 0101 cost= 0.440001398 W= 1.02555 b= -0.0137608</span></div><div class="line"><span class="comment"># Epoch: 0201 cost= 0.437495589 W= 1.02078 b= 0.0176154</span></div><div class="line"><span class="comment"># Epoch: 0301 cost= 0.437433660 W= 1.02043 b= 0.0199056</span></div><div class="line"><span class="comment"># Epoch: 0401 cost= 0.437430561 W= 1.02041 b= 0.0200727</span></div><div class="line"><span class="comment"># Epoch: 0501 cost= 0.437429130 W= 1.0204 b= 0.0200851</span></div><div class="line"><span class="comment"># Epoch: 0601 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 0701 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 0801 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 0901 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1001 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1101 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1201 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1301 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1401 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1501 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1601 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1701 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1801 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Epoch: 1901 cost= 0.437429696 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># Optimization Finished!</span></div><div class="line"><span class="comment"># cost= 0.43743 W= 1.0204 b= 0.0200854</span></div><div class="line"><span class="comment"># 可以看到迭代次数到 500 次左右数据就稳定了。</span></div></pre></td></tr></table></figure>
<h2 id="3-总结"><a href="#3-总结" class="headerlink" title="3 总结"></a>3 总结</h2><p>其实这只是一个开始，还有好多好多东西要去学习。越来越觉得基础的重要性，不仅仅是计算机基础，数学基础也是同等重要，特别是未来的物联网趋势，可能编码这种专业越来越淡化，只是作为某些专业人员的一种工具/技能使用。马上面临毕业，只能自己慢慢啃这些东西了……</p>
<h2 id="4-参考资料"><a href="#4-参考资料" class="headerlink" title="4 参考资料"></a>4 参考资料</h2><ul>
<li><a href="https://www.tensorflow.org/" target="_blank" rel="external">0. 官网</a></li>
<li><a href="http://wiki.jikexueyuan.com/project/tensorflow-zh/get_started/os_setup.html" target="_blank" rel="external">1. 中文:**学院翻译</a></li>
<li><a href="https://liam0205.me/2014/09/11/matplotlib-tutorial-zh-cn/" target="_blank" rel="external">2. Matplotlib 教程</a></li>
<li><a href="https://juejin.im/entry/5904542c570c350058168c76" target="_blank" rel="external">3. TensorFlow 入门</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;blockquote&gt;
&lt;p&gt;写在前面：紧跟时代步伐，开始学习机器学习，抱着争取在毕业之前多看看各个方向是什么样子的心态，发现这是一个很有潜力也很有趣的领域（keng）。// 然后就开始补数学了……&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="Python" scheme="http://fitzeng.org/tags/Python/"/>
    
      <category term="TensorFlow" scheme="http://fitzeng.org/tags/TensorFlow/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
  </entry>
  
  <entry>
    <title>机器学习之重拾概率论</title>
    <link href="http://fitzeng.org/2018/01/26/reProbabilityTheory/"/>
    <id>http://fitzeng.org/2018/01/26/reProbabilityTheory/</id>
    <published>2018-01-26T03:27:00.000Z</published>
    <updated>2018-01-26T08:12:37.271Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><blockquote>
<p>这里主要是围绕浙大的《概率论与数理统计》第四版的内容进行总结, 其中截取了一些图片作为内容有助于对公式的理解。笔者把这篇文章作为自己的读书笔记, 为以后系统回顾概率论提供便捷。同时会继续更新, 保证自己读懂之后再来书写。</p>
</blockquote>
<a id="more"></a>
<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><h3 id="1-1-事件关系"><a href="#1-1-事件关系" class="headerlink" title="1.1 事件关系"></a>1.1 事件关系</h3><ol>
<li><code>包含事件</code> \( A \subset B \) : 事件 \( B \) 包含事件 \( A \)，事件  \( A \) 发生必然导致事件  \( B \) 的发生。</li>
<li><code>和事件</code> \( A \cup B = \{ x|x \in A\ or\ x \in B \} \) : \( A \) \( B \) 中至少有一个事件发生。</li>
<li>\( \bigcup\limits_{k=1}\limits^{n} A_k \) : \( n \) 个事件 \( A_1, A_2, \ldots , A_n \) 的和事件。</li>
<li>\( \bigcup\limits_{k=1}\limits^{\infty} A_k \) : 可列个事件 \( A_1, A_2, \ldots \) 的和事件。</li>
<li><code>积事件</code> \( A \cap B = \{ x|x \in A\ and\ x \in B \} \) : \( A \) \( B \) 事件同时发生。</li>
<li>\( \bigcap\limits_{k=1}\limits^{n} A_k \) : \( n \) 个事件 \( A_1, A_2, \ldots , A_n \) 的积事件。</li>
<li>\( \bigcap\limits_{k=1}\limits^{\infty} A_k \) : 可列个事件 \( A_1, A_2, \ldots \) 的积事件。</li>
<li><code>差事件</code> \( A - B = \{x|x=\in A\ and\ \notin B \} \) : 当且仅当 \( A \) 发生， \( B \) 不发生。</li>
<li><code>互斥事件</code> \( A \cap B = \emptyset \) : \( A \) 与 \( B \) 事件不相容，不能同时发生。</li>
<li><code>对立事件</code> \( A \cup B = S\ and\ A \cap B = \emptyset \) : \( A \) \( B \)事件互逆，每次实验 \( A, B \) 必有一个发生且只有一个发生。\( A \) 的对立事件记为 \( \overline{A}.\ \overline{A} = S - A \)。</li>
</ol>
<p><img src="/2018/01/26/reProbabilityTheory/1_1_01.png" alt="事件关系图示"></p>
<h3 id="1-2-基本运算"><a href="#1-2-基本运算" class="headerlink" title="1.2 基本运算"></a>1.2 基本运算</h3><ol>
<li><code>交换律</code> $$ A \cup B = B \cup A;\ A \cap B = B \cap A $$</li>
<li><code>结合律</code> $$ A \cup (B \cup C) = (A \cup B) \cup C \\ A \cap (B \cap C) = (A \cap B) \cap C $$</li>
<li><code>分配率</code> $$ A \cup (B \cap C) = (A \cup B) \cap (A \cup C) \\ A \cap (B \cup C) = (A \cap B) \cup (A \cap C) $$</li>
<li><code>德摩根率</code> $$ \overline{A \cap B} = \overline{A} \cap \overline{B};\ \overline{A \cap B} = \overline{A} \cup \overline{B} $$</li>
</ol>
<h3 id="1-3-概率"><a href="#1-3-概率" class="headerlink" title="1.3 概率"></a>1.3 概率</h3><ol>
<li><code>非负性</code> : 对于每一个事件 \( A \), 有 \( P(A) \geq 0 \);</li>
<li><code>规范性</code> : 对于必然事件 \( S \), 有 \( P(S) = 1 \);</li>
<li><code>可列可加性</code> : 设 \( A_1, A_2, \ldots \) 是两两不相容的事件，即对于 \( A_iA_j = \emptyset,\ i \neq j, i, j = 1, 2, \ldots \), 有 \( P(A_1 \cup A_2 \cup \ldots) = P(A_1) + P(A_2) + \ldots \)。</li>
</ol>
<p>Tips : \( n \to \infty \Longrightarrow f_n(A) = P(A) \)</p>
<h4 id="1-3-1-基本性质"><a href="#1-3-1-基本性质" class="headerlink" title="1.3.1 基本性质"></a>1.3.1 基本性质</h4><ol>
<li>\( P(\emptyset) = 0\ and\ P(A) \leq 1  \)</li>
<li><code>有限可加性</code> : \( A_1, A_2, \ldots, A_n \) 为互斥事件, 则 $$ P(A_1 \cup A_2 \cup \ldots \cup A_n) = P(A_1) + P(A_2) + \ldots + P(A_n) $$</li>
<li>若 \( A \in B \), 则 $$ P(B - A) = P(B) - P(A) \\ P(B) \geq P(A) $$ 证明提示 : \( B = A \cup (B - A)\ and\ A(B - A) = \emptyset \)</li>
<li>\( P(\overline{A}) = 1 - P(A) \)</li>
<li><code>加法公式</code> : 任意 \( A, B \) 事件, 有 $$ P(A \cup B) = P(A) + P(B) - P(AB) $$ 证明提示 : \( A \cup B = A \cup (B - AB) \)</li>
<li><code>加法公式推广</code><br>$$ P(A_1 \cup A_2 \cup A_3) = P(A_1) + P(A_2) + P(A_3) - P(A_1A_2) - P(A_1A_3) - P(A_2A_3) + P(A_1A_2A_3) $$ 一般的有<br>$$<br>\begin{aligned} P(A_1 \cup A_2 \cup \ldots \cup A_n) = {}<br>&amp; \sum_{i = 1}^{n}P(A_i) - \sum_{1 \leq i &lt; j \leq n}P(A_iA_j) {} \\<br>&amp; + \sum_{1 \leq i &lt; j &lt; k \leq n}P(A_iA_jA_k) + \ldots + (-1)^{n-1}P(A_1A_2 \ldots A_n) \end{aligned}<br>$$</li>
</ol>
<h3 id="1-4-古典概型"><a href="#1-4-古典概型" class="headerlink" title="1.4 古典概型"></a>1.4 古典概型</h3><blockquote>
<p>也称等可能事件, 样本空间包含有限个元素, 每个基本事件发生的可能性相同。</p>
</blockquote>
<h4 id="1-4-1-基本性质"><a href="#1-4-1-基本性质" class="headerlink" title="1.4.1 基本性质"></a>1.4.1 基本性质</h4><ol>
<li><code>等可能性</code> : 在样本空间 \( S = \{ e_1, e_2, \ldots, e_n \} \) 则 $$ P(\{e_1\}) = P(\{e_2\}) = \ldots = P(\{e_n\}) $$</li>
<li><code>两两不相容</code> : 有<br>$$<br>\begin{aligned}<br>1 {}<br>&amp; = P(S) = P(\{e_1\} \cup \{e_2\} \cup \ldots \cup \{e_n\}) {} \\<br>&amp; = P(\{e_1\}) + P(\{e_2\}) + \ldots + P(\{e_n\}) {} \\<br>&amp; = nP(\{e_i\}) \\<br>&amp; P(\{e_i\}) = \frac{1}{n}, i = 1, 2, \ldots, n.<br>\end{aligned}<br>$$</li>
<li>若事件 \( A \) 包含 \( k \) 个基本事件, 即 \( A = \{e_{i_1}\} \cup \{e_{i_2}\} \cup \dots \cup \{e_{i_k}\} \) 这里 \( i_1, i_2, \ldots, i_k \) 是 \( 1, 2, \ldots, n \) 中某 \( k \)个不同的数。则有 $$ P(A) = \sum<em>{j = 1}^{k} P(\{ e\</em>{i_j} \}) = \frac{k}{n} = \frac{the\ number\ of\ event\ A}{the\ number\ of\ event\ S} $$</li>
</ol>
<h3 id="1-5-条件概率"><a href="#1-5-条件概率" class="headerlink" title="1.5 条件概率"></a>1.5 条件概率</h3><blockquote>
<p>主要理解就是在事件 \( A \) 发生了的情况下, 事件 \( B \) 发生的概率 \( P(B|A) \)。可理解样本空间为事件 \( A \) 发生之后的。</p>
</blockquote>
<h4 id="1-5-1-定义"><a href="#1-5-1-定义" class="headerlink" title="1.5.1 定义"></a>1.5.1 定义</h4><p>事件 \( A \) 发生的条件下发生事件 \( B \) 的条件概率 : $$ P(B|A) = \frac{P(AB)}{P(A)} $$ 如果不太好理解可以换成 \( P(AB) = P(A)P(B|A) \), 事件 \( AB \) 发生的过程是事件 \( A \) 发生后, 再发生事件 \( B \), 这样就可以得出事件 \( AB \) 的概率 \( P(AB) \) 与 事件 \( A \) 发生的概率 \( P(A) \) 以及事件 \( A \) 后发生的事件 \( B \) 的概率 \( P(B|A) \) 的乘积相等。 </p>
<p>同理, 概率的<code>非负性</code>、<code>规范性</code>、<code>可列可加性</code>条件概率同样具备, 下面解释第三条。</p>
<ol>
<li><code>可列可加性</code> : 设 \( B_1, B_2, \ldots \) 是两两不相容的事件, 则 $$ P(\bigcup\limits_{i=1}\limits^{\infty} B_i | A) = \sum\limits_{i = 1}\limits^{\infty} P(B_i | A) $$ 理解 : 体会基于事件 \( A \) 后发生的事件 \( B_i \) 的样本空间是事件 \( A \) 发生后的样本空间, 典型例子是在一个黑盒中不放回的取球, 求多次取出某种组合球的概率。</li>
<li><code>乘法定理</code> : 设 \( P(A) &gt; 0 \) 则$$ P(AB) = P(B|A)P(A) $$</li>
<li><code>乘法定理推广</code> : 设 \( P(AB) &gt; 0 \) 则$$ P(ABC) = P(C|AB)P(B|A)P(A) $$ Tips : \( P(A) \geq P(AB) &gt; 0 \) 是隐藏条件。<br>一般化, 设 \( A_1, A_2, \ldots, A_n \ and\  n \geq 2 \ and \ P(A_1A_2\ldots A_{n-1}) &gt; 0\), 则有 $$ P(A_1A_2\ldots A_{n}) = P(A_n|A_1A_2\ldots A_{n-1})P(A_{n-1}A_1A_2\ldots A_{n-2}) \ldots P(A_2|A_1)P(A_1)$$ 很好理解, 从后到前, 一个一个事件发生去思考。</li>
</ol>
<h4 id="1-5-2-重要公式"><a href="#1-5-2-重要公式" class="headerlink" title="1.5.2 重要公式"></a>1.5.2 重要公式</h4><ol>
<li><p><code>全概率公式</code> : 设实验 \( E \) 的样本空间为 \( S \), \( A \) 为 \( E \) 的事件, \( B_1, B_2, \ldots, B_n \) 为 \( S \) 的一个划分, 且 \( P(B_i) &gt; 0 (i = 0, 1, \dots, n) \), 则 $$ P(A) = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \ldots + P(A|B_n)P(B_n) $$ 照前面样本空间来理解就好了, \( P(A|B_i) \) 代表 \( A \) 事件在 \( B_i \) 样本空间的概率, 乘以 \( P(B_i) \) 代表在样本空间 \( S \) 中, \( B_i \) 中的样本触发事件 \( A \) 的概率。当所有划分样本的概率加起来时, 就是事件 \( A \) 在 \( S \) 中发生的概率了。<br>证明：假设 \( P(B_i) &gt; 0 \ (i = 1, 2, \ldots, n) \ and \ (AB_i)(AB_j) = \emptyset, i \neq j, i, j = 1, 2, \ldots, n \)<br>$$<br>A = AS = A(B_1 \cup B_2 \cup \ldots \cup B_n) = AB_1 \cup AB_2 \cup \ldots \cup AB_n, \\<br>\begin{aligned}<br>P(A) {} &amp; = P(AB_1) + P(AB_2) + \ldots + P(AB_n)  {} \\<br>&amp; = P(A|B_1)P(B_1) + P(A|B_2)P(B_2) + \ldots + P(A|B_n)P(B_n)<br>\end{aligned}<br>$$</p>
</li>
<li><p><code>贝叶斯公式</code> : 设实验 \( E \) 的样本空间为 \( S \), \( A \) 为 \( E \) 的事件, \( B_1, B_2, \ldots, B_n \) 为 \( S \) 的一个划分, 且 \( P(A) &gt; 0,P(B_i) &gt; 0 (i = 0, 1, \dots, n) \), 则 $$ P(B_i|A) = \frac{P(A|B_i)P(B_i)}{\sum\limits_{j = 1}\limits^{n}P(A|B_j)P(B_j)}, i= 1,2, \ldots, n.$$ 当 \( n = 2 \) 时, 这两个公式分别为 : $$ P(A) = P(A|B)P(B) + P(A|\overline{B})P(\overline{B}) = P(AB) + P(A\overline{B}) \\ P(B|A) = \frac{P(AB)}{P(A)} = \frac{P(A|B)P(B)}{P(A|B)P(B) + P(A|\overline{B})P(\overline{B})} = \frac{P(A|B)P(B)}{P(AB) + P(A\overline{B})} $$</p>
</li>
</ol>
<p>这两个公式比较重要, 这里截取一个例子供加深理解:<br><img src="/2018/01/26/reProbabilityTheory/1_5_2_01.png" alt="全概率和贝叶斯公式应用1"><br><img src="/2018/01/26/reProbabilityTheory/1_5_2_02.png" alt="全概率和贝叶斯公式应用2"><br><img src="/2018/01/26/reProbabilityTheory/1_5_2_03.png" alt="全概率和贝叶斯公式应用3"></p>
<h3 id="1-6-独立性"><a href="#1-6-独立性" class="headerlink" title="1.6 独立性"></a>1.6 独立性</h3><blockquote>
<p>理解事件 \( A, B \) 有 \( P(A|B) = P(A) \) 条件成立时, \( A, B \) 相互独立。</p>
</blockquote>
<h4 id="1-6-1-定义"><a href="#1-6-1-定义" class="headerlink" title="1.6.1 定义"></a>1.6.1 定义</h4><p>设事件 \( A, B \) 满足 $$ P(AB) = P(A)P(B) $$ 则称事件 \( A, B \) 互相独立。</p>
<h4 id="1-6-2-性质"><a href="#1-6-2-性质" class="headerlink" title="1.6.2 性质"></a>1.6.2 性质</h4><p>若 \( A, B \) 互相独立, 则 \( (A,\overline{B})\ (\overline{A}, B)\ (\overline{A}, \overline{B}) \) 也相互独立。</p>
<p><code>多事件独立</code> : \( A, B, C \) 独立条件 $$ \begin{aligned} &amp; P(AB) = P(A)P(B) \\ &amp; P(AC) = P(A)P(C) \\ &amp; P(BC) = P(B)P(C) \\ &amp; P(ABC) = P(A)P(B)P(C) \end{aligned}$$ 一般, 设事件 \( A_1, A_2, \ldots, A_n \) 是 \( n \ (n \geq 2) \) 个事件, 如果对于其中任意 \( 2, 3, \ldots, n \)个事件的积事件的概率都等于各事件概率之积, 则称事件 \( A_1, A_2, \ldots, A_n \) 相互独立。</p>
<p>Tips : 通过关键词自己一一回顾这一章的内容吧。<br><code>随机试验</code>, <code>样本空间</code>, <code>随机事件</code>, <code>基本事件</code>, <code>频率</code>, <code>概率</code>, <code>古典概型</code>, \( A \) 的对立事件 \( \overline{A} \) 及其概率, <code>两互不相容事件的和事件的概率</code>, <code>概率的加法定理</code>, <code>条件概率</code>, <code>概率的乘法公式</code>, <code>全概率公式</code>, <code>贝叶斯公式</code>, <code>事件的独立性</code>, <code>实际推断原理</code></p>
<h2 id="2-随机变量及其分布"><a href="#2-随机变量及其分布" class="headerlink" title="2. 随机变量及其分布"></a>2. 随机变量及其分布</h2><h3 id="2-1-随机变量"><a href="#2-1-随机变量" class="headerlink" title="2.1 随机变量"></a>2.1 随机变量</h3><blockquote>
<p>在前面随机试验中, 结果可以用数来表示, 但是有些描述起来不方便, 为了处理方便, 将 \( S \) 中的每一个元素 \( e \) 与实数 \( x \) 对应起来, 随机变量就这样引入了。</p>
</blockquote>
<h3 id="2-2-离散型随机变量及其分布"><a href="#2-2-离散型随机变量及其分布" class="headerlink" title="2.2 离散型随机变量及其分布"></a>2.2 离散型随机变量及其分布</h3><blockquote>
<p>随机变量 \( X \) 的取值是离散的, 如 \( 0, 1, 2, 3 \)</p>
</blockquote>
<h4 id="2-2-1-定义"><a href="#2-2-1-定义" class="headerlink" title="2.2.1 定义"></a>2.2.1 定义</h4><p>设离散型随机变量 \( X \) 所有可能的取值为 \( x_k(k = 1, 2, \ldots) , X \) 取各个可能值的概率, 及时间 \( \{ X = x_k\} \) 的概率为 : $$ P\{X = x_k\} = p_k, k = 1, 2, \ldots . $$ 同理, 概率的<code>非负性</code>、<code>规范性</code>、<code>可列可加性</code>离散型随机变量概率同样具备。<br>用表格表示 \( X \) 的分布律:<br>$$<br>\begin{array}{c|ccccc}<br>X &amp; x_1 &amp; x_2 &amp; \ldots &amp; x_n &amp; \ldots \\<br>\hline<br>p_k &amp; p_1 &amp; p_2 &amp; \ldots &amp; p_n &amp; \dots<br>\end{array}<br>$$</p>
<h4 id="2-2-2-几种典型的离散分布"><a href="#2-2-2-几种典型的离散分布" class="headerlink" title="2.2.2 几种典型的离散分布"></a>2.2.2 几种典型的离散分布</h4><ol>
<li><code>0-1 分布</code> : 随机变量 \( X \) 只能取值 \( 0, 1 \) 分布律是 $$ P\{X = k\} = p^k(1 - p)^{1 - k}, k = 0, 1 \ (0 &lt; p &lt; 1) $$<br>对于随机试验 \( S = \{ e_1, e_2 \}\), 我们总能在 \( S \) 上定义一个服从 （0 — 1）分布的随机变量 \( X \) : $$ X = X(e) = \begin{cases} 0,\ when\ e = e_1 \\ 1,\ when\ e = e_2 \end{cases} $$ 来描述。</li>
<li><code>伯努利试验、二项分布</code> : 试验 \( E \) 只有两个可能的结果 \( \{ A, \overline{A} \} \), 则称 \( E \) 为伯努利试验。将 \( E \) 重复地进行 \( n \) 次, 称这一串重复的独立实试验为重复伯努利试验。<br>设 \( P(A) = p \) 则在 \( n \) 次试验中 \( A \) 发生 \( k \) 次的概率为 : $$ P\{X = k\} = \left( \begin{array}{c} n \\ k \end{array} \right) p^k{(1 - p)}^{n - k}, k = 0, 1, 2, \ldots, n.$$ 显然 $$ \sum\limits_{k = 0}\limits^{n}P\{X = k\} = \sum\limits_{k = 0}\limits^{n} \left( \begin{array}{c} n \\ k \end{array} \right) p^k{(1 - p)}^{n - k} = {(p + (1 - p))}^n = 1 $$ 二项式 \( {(p + (1 - p))}^n \) 的第 \( k \) 项展开 \( \left( \begin{array}{c} n \\ k \end{array} \right) p^k{(1 - p)}^{n - k} \)。 所以, 我们称 \( X \) 服从参数为 \( n, p \) 的二项分布, 记作 : $$ X \sim b(n, p) $$ 特别的, 当 \( n = 1 \)时为二项分布 : $$ P\{X = k\} = p^k{(1 - p)}^{1 - k}, k = 0, 1.$$ </li>
<li><code>泊松分布</code> : 随机变量 \( X = \{0, 1, 2, \ldots \} \), 而各个概率的取值为 $$ P\{X = k\} = \frac{\lambda^k e^{-\lambda}}{k!}, k = 0, 1, 2, \ldots, \lambda &gt; 0 $$ 则称 \( X \)服从参数为 \( \lambda \) 的泊松分布, 记作 : $$ X \sim \pi(\lambda) $$ 显然 $$ \sum\limits_{k = 0}^{\infty}P\{X = k\} = \sum\limits_{k = 0}^{\infty} \frac{\lambda^k e^{-\lambda}}{k!} = e^{-\lambda} \sum\limits_{k = 0}^{\infty}\frac{\lambda^k}{k!} = e^{-\lambda}e^\lambda = 1 $$ 具体使用后面会慢慢介绍。</li>
<li><code>泊松定理</code> : 设 \( \lambda &gt; 0, np_n = \lambda \ n \)为任意正整数, 则对于任意一个固定的非负整数 \( k \), 有 $$ \lim_{n \rightarrow \infty} \left( \begin{array}{c} n \\ k \end{array} \right) p_n^k{(1 - p_n)}^{n - k} = \frac{\lambda^k e^{-\lambda}}{k!} $$ 这就是用泊松分布逼近二项分布的泊松定理。证明截图如下 :<br><img src="/2018/01/26/reProbabilityTheory/2_2_2_01.png" alt="泊松定理证明"><br>上面的证明说明了, 当 \( n \rightarrow \infty\) 时, 以 \( n, p \) 为参数的二项分布概率值可以由参数为 \( \lambda = np \) 的泊松分布的概率值近似。<br>举个栗子<br><img src="/2018/01/26/reProbabilityTheory/2_2_2_02.png" alt="泊松定理举个栗子"></li>
</ol>
<h3 id="2-3-随机变量的分布函数"><a href="#2-3-随机变量的分布函数" class="headerlink" title="2.3 随机变量的分布函数"></a>2.3 随机变量的分布函数</h3><blockquote>
<p>对于非离散的随机变量 \( X \) 指定某一实数值得概率都为 \( 0 \)。所以这里就改为研究随机变量所落区间的概率 : $$ P\{x_1 &lt; X \leq x_2\} = P(X \leq x_2) - P(X \leq x_1) $$ 画个数轴就很好理解了。</p>
</blockquote>
<h4 id="2-3-1-定义"><a href="#2-3-1-定义" class="headerlink" title="2.3.1 定义"></a>2.3.1 定义</h4><p>设 \( X \) 是一个随机变量, \( x \) 是任意实数, 函数 : $$ F(x) = P\{ X \leq x\}, -\infty &lt; x &lt; \infty $$ 称为 \( X \) 的<code>分布函数</code>。<br>对任意实数 \( x_1, x_2 (x_1 &lt; x_2) \), 有 $$ P\{x_1 &lt; X \leq x_2\} = P(X \leq x_2) - P(X \leq x_1) = F(x_2) - F(x_1) $$</p>
<h4 id="2-3-2-基本性质"><a href="#2-3-2-基本性质" class="headerlink" title="2.3.2 基本性质"></a>2.3.2 基本性质</h4><ol>
<li>\( F(x) \) 是一个不减函数。</li>
<li>\( 0 \leq F(x) \leq 1 \ and\ F(-\infty) = \lim\limits_{x\rightarrow-\infty}F(x) = 0 \ F(\infty) = \lim\limits_{x\rightarrow\infty}F(x) = 1 \)</li>
</ol>
<h3 id="2-4-连续型随机变量及其概率密度"><a href="#2-4-连续型随机变量及其概率密度" class="headerlink" title="2.4 连续型随机变量及其概率密度"></a>2.4 连续型随机变量及其概率密度</h3><blockquote>
<p>如上一节分析, 随机变量 \( X \) 的分布函数 \( F(x) \)存在非负函数 \( f(x) \), 使得任意实数 \( x \)有 $$ F(x) = \int_{-\infty}^{x}f(x)dt $$ 则称 \( X \)为<code>连续型随机变量</code>, 其中函数 \( f(x) \) 称为 \( X \) 的<code>概率密度函数</code>, 简称<code>概率密度</code>。</p>
</blockquote>
<ol>
<li>\( f(x) \geq 0 \)</li>
<li>\( \int_{-\infty}^{\infty} f(x)dx = 1 \)</li>
<li>对于任意实数 \( x_1, x_2 (x_1 \leq x_2) \) $$ P(x_1 &lt; X \leq x_2) = F(x_2)   - F(x_1) = \int_{x_1}^{x_2}f(x)dx $$</li>
<li>若 \( f(x) \) 在 \( x \)出连续, 则 \( F^{‘}(x) = f(x) \)</li>
</ol>
<h4 id="2-4-1-几种典型的连续型随机变量"><a href="#2-4-1-几种典型的连续型随机变量" class="headerlink" title="2.4.1 几种典型的连续型随机变量"></a>2.4.1 几种典型的连续型随机变量</h4><ol>
<li><code>均匀分布</code> : 若连续型随机变量 \( X \) 的概率密度为 $$ f(x) = \begin{cases} \frac{1}{b - a},\ a &lt; x &lt; b \\ 0,\ others \end{cases} $$ 则称 \( X \) 在区间 \( (a, b) \) 上服从均匀分布。记为 \( X \sim U(a, b) \)。分布函数为 $$ F(x) = \begin{cases} 0,\ x &lt; a \\ \frac{x - a}{b - a},\ a \leq x &lt; b \\ 1,\ x \geq b  \end{cases} $$<br><img src="/2018/01/26/reProbabilityTheory/2_4_1_01.png" alt="均匀分布例子"></li>
<li><code>指数分布</code> : 若连续型随机变量 \( X \) 的概率密度为 $$ f(x) = \begin{cases} \frac{1}{\theta}e^{-x/\theta},\  x &gt; 0 \\ 0,\ others \end{cases} $$ 其中 \( \theta &gt; 0 \) 为常数, 则称 \( X \) 服从参数为 \( \theta \) 的指数分布。分布函数为 $$ f(x) = \begin{cases} 1 - e^{-x/\theta},\  x &gt; 0 \\ 0,\ others \end{cases} $$<br><code>指数分布的无记忆性</code> : 对于任意 \( s, t &gt; 0 \), 有 $$ P\{ X &gt; s + t | X &gt; s\} = P\{ X &gt; t \} $$ 证明 : <img src="/2018/01/26/reProbabilityTheory/2_4_1_02.png" alt="指数分布的无记忆性的证明"> </li>
<li><code>正态分布</code> : 若连续型随机变量 \( X \) 的概率密度为 $$ f(x) = \frac{1}{\sqrt{2\pi}\sigma} e ^{- \frac{(x - \mu)^2}{2\sigma^2}}, \ -\infty &lt; x &lt; \infty $$ 其中 \( \mu, \sigma (\sigma &gt; 0) \) 为常数, 则称 \( X \) 服从参数为 \( \mu, \sigma \) 的正态分布或高斯分布, 记为 \( X \sim N(\mu, \sigma^2) \)。证明 : <img src="/2018/01/26/reProbabilityTheory/2_4_1_03.png" alt=""><br>曲线关于 \(  x = \mu \) 对称, 这表明对于任意 \( h &gt; 0 \) 有 $$ P\{ \mu - h &lt; X \leq \mu \} = P\{ \mu &lt; X \leq \mu + h \} $$ 当 \( x = \mu \) 时取最大值 $$ f(\mu) = \frac{1}{\sqrt{2\pi}\sigma} $$ 在 \( x = \mu \pm \sigma \) 处曲线有拐点。特别, 当 \( \mu = 0, \sigma = 1 \) 时, 称随机变量 \( X \) 服从标准正态分布。其概率密度和分布函数分别用 \( \varphi (x), \Phi (x) \) 表示, 即有 $$ \varphi (x) = \frac{1}{\sqrt{2\pi}}e^{-t^2/2} \\ \Phi (x) = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{x}e^{-t^2/2}dt \\ \Phi (-x) = 1 - \Phi (x) $$ <img src="/2018/01/26/reProbabilityTheory/2_4_1_04.png" alt=""></li>
</ol>
<h3 id="2-5-随机变量的函数的分布"><a href="#2-5-随机变量的函数的分布" class="headerlink" title="2.5 随机变量的函数的分布"></a>2.5 随机变量的函数的分布</h3><blockquote>
<p>这里主要介绍在已知随机变量 \( X \) 的概率求 \( Y = g(X) \ g(\cdot) \) 是已知连续函数。</p>
</blockquote>
<p>举个栗子<br><img src="/2018/01/26/reProbabilityTheory/2_5_01.png" alt=""></p>
<p>设随机变量 \( X \) 具有概率密度 \( f_X(x), \ -\infty &lt; x &lt; \infty \), 又设函数 \( g(x) \) 处处可导, 恒有 \( g(x)^{‘} &gt; 0 \ or \  g(x)^{‘} &lt; 0 \), 则 \( Y = g(X) \) 是连续型随机变量, 其概率密度为 $$ f_Y(y) = \begin{cases} f_X[h(y)]|h^{‘}(y)|, \ \alpha &lt; y &lt; \beta \\ 0, \ others \end{cases} $$ 其中 \( \alpha = min\{ g(-\infty), g(\infty) \} \ \beta = max\{ g(-\infty), g(\infty) \} \ h(x) \) 是 \( g(x) \) 的反函数。<br>证明提示 : 见上一个🌰</p>
<p><img src="/2018/01/26/reProbabilityTheory/2_5_02.png" alt=""></p>
<p>小结 :<br><img src="/2018/01/26/reProbabilityTheory/2_5_03.png" alt=""><br><img src="/2018/01/26/reProbabilityTheory/2_5_04.png" alt=""><br><img src="/2018/01/26/reProbabilityTheory/2_5_05.png" alt=""></p>
<p><img src="/2018/01/26/reProbabilityTheory/summary.jpg" alt=""></p>
<h2 id="3-TODO"><a href="#3-TODO" class="headerlink" title="3. TODO"></a>3. TODO</h2><p>未完待续…</p>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;这里主要是围绕浙大的《概率论与数理统计》第四版的内容进行总结, 其中截取了一些图片作为内容有助于对公式的理解。笔者把这篇文章作为自己的读书笔记, 为以后系统回顾概率论提供便捷。同时会继续更新, 保证自己读懂之后再来书写。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="MachineLearning" scheme="http://fitzeng.org/tags/MachineLearning/"/>
    
      <category term="ProbabilityTheory" scheme="http://fitzeng.org/tags/ProbabilityTheory/"/>
    
  </entry>
  
  <entry>
    <title>MarkDown 插入数学公式实验大集合</title>
    <link href="http://fitzeng.org/2018/01/23/LaTexFormula/"/>
    <id>http://fitzeng.org/2018/01/23/LaTexFormula/</id>
    <published>2018-01-23T03:27:00.000Z</published>
    <updated>2018-01-23T14:47:46.407Z</updated>
    
    <content type="html"><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><blockquote>
<p>最近在学习一些机器学习相关的知识，想把自己学习的东西通过 MD 的形式在线记录下来，但是之前一直没有开始行动，因为里面的公式什么的感觉实在是麻烦。于是今天打算花点时间了解一下<a href="http://blog.csdn.net/xiahouzuoxin/article/details/26478179" target="_blank" rel="external"><code>如何在 markdown 中插入数学公式</code></a>，发现其实很简单，大概花一个小时左右就能知道如何编写了。</p>
</blockquote>
<a id="more"></a>
<h2 id="1-基础认识"><a href="#1-基础认识" class="headerlink" title="1. 基础认识"></a>1. 基础认识</h2><blockquote>
<p>笔者认为所谓插入数学公式其实就是引入一种规则，然后通过<code>模板？</code>渲染成公式，不知道这个理解对不对，不对望指正。其实你以前可能就看到过有的博客本该出现公式的时候不显示，点击后会链接到一个 new tab 然后显示一张公式的图片，有时却出现一大堆的代码。这里就是通过这段代码解析成公式然后显示的。</p>
</blockquote>
<p>这里我们选取 MathJax 引擎。<br>引入脚本，把下面代码插入 MD 文件里面，如果你怕这份在线文件源别人访问不到的话，可以把这个下下来自己做一个源，这样比较稳定缺点是要自己手动更新源。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">&lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;</div></pre></td></tr></table></figure>
<p>好了到这里就可以插入公式了，如果你懂 LaTeX 的话那看一两个例子就知道了，不懂也没关系，自己写一写代码就知道了，可以找一个可以预览 MD 的工具一直尝试。</p>
<h3 id="1-1-插入方式"><a href="#1-1-插入方式" class="headerlink" title="1.1 插入方式"></a>1.1 插入方式</h3><blockquote>
<p>这里分两种，一种是行间插入，另一种是另取一行</p>
</blockquote>
<h4 id="1-1-1-行间插入"><a href="#1-1-1-行间插入" class="headerlink" title="1.1.1 行间插入"></a>1.1.1 行间插入</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">\\(a + b\\)</div></pre></td></tr></table></figure>
<p>这里是行间插入公式 a + b : \(a + b\)，特点就是通过<code>(</code> 和 <code>)</code> 包含公式，然后为了模板引擎能够区分该 <code>(</code> 不是普通文本的 <code>(</code> 而是公式的 <code>(</code>，通过 <code>\\</code> 转义一下。这样应该就很好理解这个语法构成了。注意这里方式不唯一，这是笔者喜欢的方式，其他的使用方式自行搜索。下面的介绍同样是这样。</p>
<h4 id="1-1-2-另取一行"><a href="#1-1-2-另取一行" class="headerlink" title="1.1.2 另取一行"></a>1.1.2 另取一行</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$$a + b$$</div></pre></td></tr></table></figure>
<p>这里是另取一行$$a + b$$ 特点就是通过<code>$$</code>包含公式。</p>
<p>笔者认为第二种方式更好，以下没看 JS 源码纯属猜测：行间的需要考虑到当前行的行高并对公式进行处理，而另取一行就更简单一些，可能解析起来更快。最最最最最最主要是看起来漂亮 ^_^ 不太要考虑空间不够换行。</p>
<h3 id="1-2-基本类型的插入"><a href="#1-2-基本类型的插入" class="headerlink" title="1.2 基本类型的插入"></a>1.2 基本类型的插入</h3><blockquote>
<p>这里对 <a href="http://www.cnblogs.com/houkai/p/3399646.html" target="_blank" rel="external">@houkai ：LATEX数学公式基本语法</a> 的思路稍加修改，然后进行介绍。</p>
</blockquote>
<h4 id="1-2-1-上、下标"><a href="#1-2-1-上、下标" class="headerlink" title="1.2.1 上、下标"></a>1.2.1 上、下标</h4><p>先看结果再总结语法吧。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">$$x_1$$</div><div class="line"></div><div class="line">$$x_1^2$$</div><div class="line"></div><div class="line">$$x^2_1$$</div><div class="line"></div><div class="line">$$x_&#123;22&#125;^&#123;(n)&#125;$$</div><div class="line"></div><div class="line">$$&#123;&#125;^*x^*$$</div><div class="line"></div><div class="line">$$x_&#123;balabala&#125;^&#123;bala&#125;&amp;&amp;</div></pre></td></tr></table></figure>
<p>$$x_1$$</p>
<p>$$x_1^2$$</p>
<p>$$x^2_1$$</p>
<p>$$x_{22}^{(n)}$$</p>
<p>$${}^*x^*$$</p>
<p>$$x_{balabala}^{bala}$$</p>
<p>可以看到 <code>x</code> 元素的上标通过 <code>^</code> 符号后接的内容体现，下表通过 <code>_</code> 符号后接的内容体现，多于一位是要加 <code>{}</code> 包裹的。<br>笔者习惯先下标后上标的写法，和我的书写习惯一致：<code>x_{balabala}^{bala}</code>，不管你使用哪一种风格，最好自己注意统一，不要混用。</p>
<h4 id="1-2-2-分式"><a href="#1-2-2-分式" class="headerlink" title="1.2.2 分式"></a>1.2.2 分式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$$\frac&#123;x+y&#125;&#123;2&#125;$$</div><div class="line"></div><div class="line">$$\frac&#123;1&#125;&#123;1+\frac&#123;1&#125;&#123;2&#125;&#125;$$</div></pre></td></tr></table></figure>
<p>$$\frac{x+y}{2}$$</p>
<p>$$\frac{1}{1+\frac{1}{2}}$$</p>
<p>这里就出现了一个 <code>frac{}{}</code> 函数的东西，同样，为了区分这是函数不是几个字母，通过 <code>\frac</code> 转义，于是 <code>frac</code> 被解析成函数，然后第一个 <code>{}</code> 里面的被解析成分子，第二个 <code>{}</code> 被解析成分母。这里可以试试分数的行间解析\(\frac{1}{1+\frac{1}{2}}\)。我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果我要看行间填充效果。</p>
<h4 id="1-2-3-根式"><a href="#1-2-3-根式" class="headerlink" title="1.2.3 根式"></a>1.2.3 根式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">$$\sqrt&#123;2&#125;&lt;\sqrt[3]&#123;3&#125;$$</div><div class="line"></div><div class="line">$$\sqrt&#123;1+\sqrt[p]&#123;1+a^2&#125;&#125;$$</div><div class="line"></div><div class="line">$$\sqrt&#123;1+\sqrt[^p\!]&#123;1+a^2&#125;&#125;$$</div></pre></td></tr></table></figure>
<p>$$\sqrt{2}&lt;\sqrt[3]{3}$$</p>
<p>$$\sqrt{1+\sqrt[p]{1+a^2}}$$</p>
<p>$$\sqrt{1+\sqrt[^p]{1+a^2}}$$</p>
<p>读到这里你已经了解了函数的概念，那么这历久很简单了，语法就是 <code>sqrt[]{}</code> 。<code>[]</code> 中代表是几次根式，<code>{}</code> 代表根号下的表达式。第二和第三个的区别在于为了美观微调位置 ^_^。</p>
<h4 id="1-2-4-求和、积分"><a href="#1-2-4-求和、积分" class="headerlink" title="1.2.4 求和、积分"></a>1.2.4 求和、积分</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$$\sum_&#123;k=1&#125;^&#123;n&#125;\frac&#123;1&#125;&#123;k&#125;$$</div><div class="line"></div><div class="line">\\(\sum_&#123;k=1&#125;^n\frac&#123;1&#125;&#123;k&#125;\\)</div><div class="line"></div><div class="line">$$\int_a^b f(x)dx$$</div><div class="line"></div><div class="line">\\(\int_a^b f(x)dx\\)</div></pre></td></tr></table></figure>
<p>$$\sum_{k=1}^{n}\frac{1}{k}$$</p>
<p>\(\sum_{k=1}^n\frac{1}{k}\)</p>
<p>$$\int_{a}^b f(x)dx$$</p>
<p>\(\int_a^b f(x)dx\)</p>
<p>这里很容易看出求和函数表达式 <code>sum_{起点}^{终点}表达式</code>，积分函数表达式 <code>int_下限^上限 被积函数d被积量</code>。还有一个有趣的是行间的公式都被压缩了。</p>
<h4 id="1-2-5-空格"><a href="#1-2-5-空格" class="headerlink" title="1.2.5 空格"></a>1.2.5 空格</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">紧贴 $a\\!b$</div><div class="line">没有空格 $ab$</div><div class="line">小空格 a\,b</div><div class="line">中等空格 a\;b</div><div class="line">大空格 a\ b</div><div class="line">quad空格 $a\quad b$</div><div class="line">两个quad空格 $a\qquad b$</div></pre></td></tr></table></figure>
<p>$$a\!b$$<br>$$ab$$<br>$$a\,b$$<br>$$a\;b$$<br>$$a\ b$$<br>$$a\quad b$$<br>$$a\qquad b$$</p>
<p>这个直接看上面的文字，介绍很清楚，主要指微调距离，使得公式更加漂亮。请比较下面的积分公式：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">$$\int_a^b f(x)\mathrm&#123;d&#125;x$$</div><div class="line"></div><div class="line">$$\int_a^b f(x)\,\mathrm&#123;d&#125;x$$</div></pre></td></tr></table></figure>
<p>$$\int_a^b f(x)\mathrm{d}x$$</p>
<p>$$\int_a^b f(x)\,\mathrm{d}x$$</p>
<h4 id="1-2-6-公式界定符"><a href="#1-2-6-公式界定符" class="headerlink" title="1.2.6 公式界定符"></a>1.2.6 公式界定符</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">\\( ( \\)</div><div class="line">\\( ) \\)</div><div class="line">\\( [ \\)</div><div class="line">\\( ] \\)</div><div class="line">\\( \\&#123; \\)</div><div class="line">\\( \\&#125; \\)</div><div class="line">\\( | \\)</div><div class="line">\\( \\| \\)</div></pre></td></tr></table></figure>
<p>主要符号有<br>\( ( \)<br>\( ) \)<br>\( [ \)<br>\( ] \)<br>\( \{ \)<br>\( \} \)<br>\( | \)<br>\( \| \)<br>那么如何使用呢？<br>通过 <code>\left</code> 和 <code>\right</code> 后面跟界定符来对同时进行界定。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">$$\left(\sum_&#123;k=\frac&#123;1&#125;&#123;2&#125;&#125;^&#123;N^2&#125;\frac&#123;1&#125;&#123;k&#125;\right)$$</div></pre></td></tr></table></figure>
<p>$$\left(\sum_{k=\frac{1}{2}}^{N^2}\frac{1}{k}\right)$$</p>
<h4 id="1-2-7-矩阵"><a href="#1-2-7-矩阵" class="headerlink" title="1.2.7 矩阵"></a>1.2.7 矩阵</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$$\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;$$</div><div class="line"></div><div class="line">$$\begin&#123;pmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;pmatrix&#125;$$</div><div class="line"></div><div class="line">$$\begin&#123;bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;bmatrix&#125;$$</div><div class="line"></div><div class="line">$$\begin&#123;Bmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Bmatrix&#125;$$</div><div class="line"></div><div class="line">$$\begin&#123;vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;vmatrix&#125;$$</div><div class="line"></div><div class="line">$$\left|\begin&#123;matrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;matrix&#125;\right|$$</div><div class="line"></div><div class="line">$$\begin&#123;Vmatrix&#125;1 &amp; 2\\\\3 &amp;4\end&#123;Vmatrix&#125;$$</div></pre></td></tr></table></figure>
<p>$$\begin{matrix}1 &amp; 2\\3 &amp;4\end{matrix}$$</p>
<p>$$\begin{pmatrix}1 &amp; 2\\3 &amp;4\end{pmatrix}$$</p>
<p>$$\begin{bmatrix}1 &amp; 2\\3 &amp;4\end{bmatrix}$$</p>
<p>$$\begin{Bmatrix}1 &amp; 2\\3 &amp;4\end{Bmatrix}$$</p>
<p>$$\begin{vmatrix}1 &amp; 2\\3 &amp;4\end{vmatrix}$$</p>
<p>$$\left|\begin{matrix}1 &amp; 2\\3 &amp;4\end{matrix}\right|$$</p>
<p>$$\begin{Vmatrix}1 &amp; 2\\3 &amp;4\end{Vmatrix}$$</p>
<p>类似于 left right，这里是 begin 和 end。而且里面有具体的矩阵语法，<code>&amp;</code> 区分行间元素，<code>\\\\</code> 代表换行。可以理解为 HTML 的标签之类的。</p>
<h4 id="1-2-8-排版数组"><a href="#1-2-8-排版数组" class="headerlink" title="1.2.8 排版数组"></a>1.2.8 排版数组</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">\mathbf&#123;X&#125; =</div><div class="line">\left( \begin&#123;array&#125;&#123;ccc&#125;</div><div class="line">x\_&#123;11&#125; &amp; x\_&#123;12&#125; &amp; \ldots \\\\</div><div class="line">x\_&#123;21&#125; &amp; x\_&#123;22&#125; &amp; \ldots \\\\</div><div class="line">\vdots &amp; \vdots &amp; \ddots</div><div class="line">\end&#123;array&#125; \right)</div></pre></td></tr></table></figure>
<p>$$<br>\mathbf{X} =<br>\left( \begin{array}{ccc}<br>x_{11} &amp; x_{12} &amp; \ldots \\<br>x_{21} &amp; x_{22} &amp; \ldots \\<br>\vdots &amp; \vdots &amp; \ddots<br>\end{array} \right)<br>$$</p>
<h2 id="2-常用公式举例"><a href="#2-常用公式举例" class="headerlink" title="2. 常用公式举例"></a>2. 常用公式举例</h2><blockquote>
<p>持续更新……</p>
</blockquote>
<h3 id="2-1-多行公式"><a href="#2-1-多行公式" class="headerlink" title="2.1 多行公式"></a>2.1 多行公式</h3><blockquote>
<p>主要是各种方程的表达</p>
</blockquote>
<h4 id="2-1-1-长公式"><a href="#2-1-1-长公式" class="headerlink" title="2.1.1 长公式"></a>2.1.1 长公式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\begin&#123;multline&#125;</div><div class="line">x = a+b+c+&#123;&#125; \\\\</div><div class="line">d+e+f+g</div><div class="line">\end&#123;multline&#125;</div><div class="line">$$</div><div class="line"></div><div class="line">$$</div><div class="line">\begin&#123;aligned&#125;</div><div class="line">x =&#123;&#125;&amp; a+b+c+&#123;&#125; \\\\</div><div class="line">&amp;d+e+f+g</div><div class="line">\end&#123;aligned&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>不对齐</p>
<p>$$<br>\left| \begin{multline}<br>x = a+b+c+{} \\<br>d+e+f+g<br>\end{multline} \right|<br>$$</p>
<p>对齐</p>
<p>$$<br>\left| \begin{aligned}<br>x ={}&amp; a+b+c+{} \\<br>&amp;d+e+f+g<br>\end{aligned} \right|<br>$$</p>
<h4 id="2-1-2-公式组"><a href="#2-1-2-公式组" class="headerlink" title="2.1.2 公式组"></a>2.1.2 公式组</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\begin&#123;gather&#125;</div><div class="line">a = b+c+d \\\\</div><div class="line">x = y+z</div><div class="line">\end&#123;gather&#125;</div><div class="line">$$</div><div class="line"></div><div class="line">$$</div><div class="line">\begin&#123;align&#125;</div><div class="line">a &amp;= b+c+d \\\\</div><div class="line">x &amp;= y+z</div><div class="line">\end&#123;align&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>$$<br>\begin{gather}<br>a = b+c+d \\<br>x = y+z<br>\end{gather}<br>$$</p>
<p>$$<br>\begin{align}<br>a &amp;= b+c+d \\<br>x &amp;= y+z<br>\end{align}<br>$$</p>
<h4 id="2-1-3-分段函数"><a href="#2-1-3-分段函数" class="headerlink" title="2.1.3 分段函数"></a>2.1.3 分段函数</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">y=\begin&#123;cases&#125;</div><div class="line">-x,\quad x\leq 0 \\\\</div><div class="line">x,\quad x&gt;0</div><div class="line">\end&#123;cases&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>$$<br>y=\begin{cases}<br>-x,\quad x\leq 0 \\<br>x,\quad x&gt;0<br>\end{cases}<br>$$</p>
<p>里面用到了 \(\leq\) 符号，下一章会介绍常用数学符号。</p>
<h3 id="2-2-数组的其他使用"><a href="#2-2-数组的其他使用" class="headerlink" title="2.2 数组的其他使用"></a>2.2 数组的其他使用</h3><h4 id="2-2-1-划线"><a href="#2-2-1-划线" class="headerlink" title="2.2.1 划线"></a>2.2.1 划线</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\left(\begin&#123;array&#125;&#123;|c|c|&#125;</div><div class="line">1 &amp; 2 \\\\</div><div class="line">\\hline</div><div class="line">3 &amp; 4</div><div class="line">\end&#123;array&#125;\right)</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>$$<br>\left( \begin{array}{|c|c|}<br>1 &amp; \ldots \\<br>\hline<br>\vdots &amp; \ddots<br>\end{array} \right)<br>$$</p>
<h4 id="2-2-2-制表"><a href="#2-2-2-制表" class="headerlink" title="2.2.2 制表"></a>2.2.2 制表</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\begin&#123;array&#125;&#123;|c|c|&#125;</div><div class="line">\hline</div><div class="line">&#123;1111111111&#125; &amp; 2 \\\\</div><div class="line">\hline</div><div class="line">3 &amp; 4 \\\\</div><div class="line">\hline</div><div class="line">\end&#123;array&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>$$<br>\begin{array}{|c|c|}<br>\hline<br>{1111111111} &amp; 2 \\<br>\hline<br>{balabala} &amp; 你好啊 {}^\land\_{}^\land \\<br>\hline<br>\end{array}<br>$$</p>
<p>可以看到，其实其他很多东西都可以很灵活的表达出来。碰到其他有趣的我会继续写出来的。</p>
<h2 id="3-常用数学符号"><a href="#3-常用数学符号" class="headerlink" title="3. 常用数学符号"></a>3. 常用数学符号</h2><blockquote>
<p>这里提供一个<a href="http://files.cnblogs.com/houkai/LATEX%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7%E8%A1%A8.rar" target="_blank" rel="external">文档下载</a>，如果上面的链接失效，也可以到我的 <a href="https://github.com/mk43/BlogResource/blob/master/LaTex/LATEX%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7%E8%A1%A8.pdf" target="_blank" rel="external">GitHub 下载 pdf 版</a>。下面举几个例子。</p>
</blockquote>
<h3 id="3-1-希腊字母"><a href="#3-1-希腊字母" class="headerlink" title="3.1 希腊字母"></a>3.1 希腊字母</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line">$$</div><div class="line">\begin&#123;array&#125;&#123;|c|c|c|c|c|c|c|c|&#125;</div><div class="line">\hline</div><div class="line">&#123;\alpha&#125; &amp; &#123;\backslash alpha&#125; &amp; &#123;\theta&#125; &amp; &#123;\backslash theta&#125; &amp; &#123;o&#125; &amp; &#123;o&#125; &amp; &#123;\upsilon&#125; &amp; &#123;\backslash upsilon&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\beta&#125; &amp; &#123;\backslash beta&#125; &amp; &#123;\vartheta&#125; &amp; &#123;\backslash vartheta&#125; &amp; &#123;\pi&#125; &amp; &#123;\backslash pi&#125; &amp; &#123;\phi&#125; &amp; &#123;\backslash phi&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\gamma&#125; &amp; &#123;\backslash gamma&#125; &amp; &#123;\iota&#125; &amp; &#123;\backslash iota&#125; &amp; &#123;\varpi&#125; &amp; &#123;\backslash varpi&#125; &amp; &#123;\varphi&#125; &amp; &#123;\backslash varphi&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\delta&#125; &amp; &#123;\backslash delta&#125; &amp; &#123;\kappa&#125; &amp; &#123;\backslash kappa&#125; &amp; &#123;\rho&#125; &amp; &#123;\backslash rho&#125; &amp; &#123;\chi&#125; &amp; &#123;\backslash chi&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\epsilon&#125; &amp; &#123;\backslash epsilon&#125; &amp; &#123;\lambda&#125; &amp; &#123;\backslash lambda&#125; &amp; &#123;\varrho&#125; &amp; &#123;\backslash varrho&#125; &amp; &#123;\psi&#125; &amp; &#123;\backslash psi&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\varepsilon&#125; &amp; &#123;\backslash varepsilon&#125; &amp; &#123;\mu&#125; &amp; &#123;\backslash mu&#125; &amp; &#123;\sigma&#125; &amp; &#123;\backslash sigma&#125; &amp; &#123;\omega&#125; &amp; &#123;\backslash omega&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\zeta&#125; &amp; &#123;\backslash zeta&#125; &amp; &#123;\nu&#125; &amp; &#123;\backslash nu&#125; &amp; &#123;\varsigma&#125; &amp; &#123;\backslash varsigma&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\eta&#125; &amp; &#123;\backslash eta&#125; &amp; &#123;\xi&#125; &amp; &#123;\backslash xi&#125; &amp; &#123;\tau&#125; &amp; &#123;\backslash tau&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\Gamma&#125; &amp; &#123;\backslash Gamma&#125; &amp; &#123;\Lambda&#125; &amp; &#123;\backslash Lambda&#125; &amp; &#123;\Sigma&#125; &amp; &#123;\backslash Sigma&#125; &amp; &#123;\Psi&#125; &amp; &#123;\backslash Psi&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\Delta&#125; &amp; &#123;\backslash Delta&#125; &amp; &#123;\Xi&#125; &amp; &#123;\backslash Xi&#125; &amp; &#123;\Upsilon&#125; &amp; &#123;\backslash Upsilon&#125; &amp; &#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; \\\\</div><div class="line">\hline</div><div class="line">&#123;\Omega&#125; &amp; &#123;\backslash Omega&#125; &amp; &#123;\Pi&#125; &amp; &#123;\backslash Pi&#125; &amp; &#123;\Phi&#125; &amp; &#123;\backslash Phi&#125; &amp; &#123;&#125; &amp; &#123;&#125; \\\\</div><div class="line">\hline</div><div class="line">\end&#123;array&#125;</div><div class="line">$$</div></pre></td></tr></table></figure>
<p>$$<br>\begin{array}{|c|c|c|c|c|c|c|c|}<br>\hline<br>{\alpha} &amp; {\backslash alpha} &amp; {\theta} &amp; {\backslash theta} &amp; {o} &amp; {o} &amp; {\upsilon} &amp; {\backslash upsilon} \\<br>\hline<br>{\beta} &amp; {\backslash beta} &amp; {\vartheta} &amp; {\backslash vartheta} &amp; {\pi} &amp; {\backslash pi} &amp; {\phi} &amp; {\backslash phi} \\<br>\hline<br>{\gamma} &amp; {\backslash gamma} &amp; {\iota} &amp; {\backslash iota} &amp; {\varpi} &amp; {\backslash varpi} &amp; {\varphi} &amp; {\backslash varphi} \\<br>\hline<br>{\delta} &amp; {\backslash delta} &amp; {\kappa} &amp; {\backslash kappa} &amp; {\rho} &amp; {\backslash rho} &amp; {\chi} &amp; {\backslash chi} \\<br>\hline<br>{\epsilon} &amp; {\backslash epsilon} &amp; {\lambda} &amp; {\backslash lambda} &amp; {\varrho} &amp; {\backslash varrho} &amp; {\psi} &amp; {\backslash psi} \\<br>\hline<br>{\varepsilon} &amp; {\backslash varepsilon} &amp; {\mu} &amp; {\backslash mu} &amp; {\sigma} &amp; {\backslash sigma} &amp; {\omega} &amp; {\backslash omega} \\<br>\hline<br>{\zeta} &amp; {\backslash zeta} &amp; {\nu} &amp; {\backslash nu} &amp; {\varsigma} &amp; {\backslash varsigma} &amp; {} &amp; {} \\<br>\hline<br>{\eta} &amp; {\backslash eta} &amp; {\xi} &amp; {\backslash xi} &amp; {\tau} &amp; {\backslash tau} &amp; {} &amp; {} \\<br>\hline<br>{\Gamma} &amp; {\backslash Gamma} &amp; {\Lambda} &amp; {\backslash Lambda} &amp; {\Sigma} &amp; {\backslash Sigma} &amp; {\Psi} &amp; {\backslash Psi} \\<br>\hline<br>{\Delta} &amp; {\backslash Delta} &amp; {\Xi} &amp; {\backslash Xi} &amp; {\Upsilon} &amp; {\backslash Upsilon} &amp; {\Omega} &amp; {\backslash Omega} \\<br>\hline<br>{\Omega} &amp; {\backslash Omega} &amp; {\Pi} &amp; {\backslash Pi} &amp; {\Phi} &amp; {\backslash Phi} &amp; {} &amp; {} \\<br>\hline<br>\end{array}<br>$$</p>
<p>写太累了😂😂😂。。。其他的详见 <a href="https://github.com/mk43/BlogResource/blob/master/LaTex/LATEX%E6%95%B0%E5%AD%A6%E7%AC%A6%E5%8F%B7%E8%A1%A8.pdf" target="_blank" rel="external">PDF</a>。</p>
<h2 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h2><blockquote>
<p>通过这样梳理一下基本的公式都能插入了，而且也会如何查资料。对于自己日后学习 LaTeX 写论文有很大帮助。以下建议带有很强的主观性，仅供参考。</p>
</blockquote>
<ul>
<li>公式一律使用另取一行，并且上下都空一行</li>
<li>一个公式一个语句，不要写在一个 <code>$$***$$</code> 里，保证<code>独立性</code>，一个公式错误不影响另一个公式。</li>
<li>风格统一，不要混用。比如上下标的写法：<code>x_{balabala}^{bala}</code></li>
<li>行间字母可以使用 <code>\\(a\\)</code> 代替 <code>a</code> ，养成自己的写作风格。</li>
</ul>
<p>最后：我的 <a href="http://fitzeng.org/">Blog</a> 和 <a href="https://github.com/mk43" target="_blank" rel="external">GitHub</a>，感谢阅读。</p>
<h2 id="5-参考资料"><a href="#5-参考资料" class="headerlink" title="5. 参考资料"></a>5. 参考资料</h2><blockquote>
<p>十分感谢以下作者的无私分享。</p>
</blockquote>
<ol>
<li><a href="http://blog.csdn.net/xiahouzuoxin/article/details/26478179" target="_blank" rel="external">Markdown中插入数学公式的方法</a></li>
<li><a href="http://www.cnblogs.com/houkai/p/3399646.html" target="_blank" rel="external">LATEX数学公式基本语法</a></li>
<li><a href="https://liam0205.me/2014/09/08/latex-introduction/" target="_blank" rel="external">一份其实很短的 LaTeX 入门文档</a></li>
</ol>
]]></content>
    
    <summary type="html">
    
      &lt;script type=&quot;text/javascript&quot; src=&quot;http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default&quot;&gt;&lt;/script&gt;

&lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;最近在学习一些机器学习相关的知识，想把自己学习的东西通过 MD 的形式在线记录下来，但是之前一直没有开始行动，因为里面的公式什么的感觉实在是麻烦。于是今天打算花点时间了解一下&lt;a href=&quot;http://blog.csdn.net/xiahouzuoxin/article/details/26478179&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;code&gt;如何在 markdown 中插入数学公式&lt;/code&gt;&lt;/a&gt;，发现其实很简单，大概花一个小时左右就能知道如何编写了。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Math" scheme="http://fitzeng.org/tags/Math/"/>
    
      <category term="MarkDown" scheme="http://fitzeng.org/tags/MarkDown/"/>
    
      <category term="Formula" scheme="http://fitzeng.org/tags/Formula/"/>
    
      <category term="LaTeX" scheme="http://fitzeng.org/tags/LaTeX/"/>
    
  </entry>
  
  <entry>
    <title>GitNote</title>
    <link href="http://fitzeng.org/2017/12/16/GitNote/"/>
    <id>http://fitzeng.org/2017/12/16/GitNote/</id>
    <published>2017-12-16T03:27:00.000Z</published>
    <updated>2017-12-17T06:36:06.885Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><blockquote>
<p>在使用 Git 时有一种感觉就是只会 <code>add | commit | push</code> 三个命令。因为这三个命令似乎足以把代码提交到 GitHub 上，即使是碰到某个问题，也可以直接 Google。但是对于其中的逻辑和版本管理的精髓没有体会到，而只是作为一个在线代码仓库在使用，这样做也没有什么不可以，但是如果能系统的了解为什么要有版本管理工具，以及版本管理工具能够给我们提供怎么样的管理服务，对于使用工具的我们来说有很大帮助。不用记住命令，只是以后遇到问题时知道这个工具有实现的方式就足够了。</p>
<p>现在我系统的看了一下官方的 Git 入门教程 <a href="https://www.nowcoder.com/courses/2#chapter-14" target="_blank" rel="external">Git 官方教程（中字）</a> 里面通过描述对命令的使用情景进而选择命令进行操作的方式，使得命令很容易理解也容易加深记忆。比起单一的命令用文字解释这种方式，对于不熟悉 Git 的人来说是十分友好的。但是，看完容易忘，所以写了这篇整理笔记用作辅助查询。</p>
</blockquote>
<a id="more"></a>
<h2 id="1-基本命令"><a href="#1-基本命令" class="headerlink" title="1. 基本命令"></a>1. 基本命令</h2><h3 id="1-1-了解帮助命令"><a href="#1-1-了解帮助命令" class="headerlink" title="1.1 了解帮助命令"></a>1.1 了解帮助命令</h3><ul>
<li><code>git help</code> : 查看命令</li>
<li><code>git help add</code> : 查看 <code>git add</code> 命令的具体解释</li>
</ul>
<h3 id="1-2-仓库初始化"><a href="#1-2-仓库初始化" class="headerlink" title="1.2 仓库初始化"></a>1.2 仓库初始化</h3><ul>
<li><code>git init</code> : 创建 <code>.git</code>, 适合在已存在项目追加版本控制</li>
<li><code>git init projectname</code> : 创建 <code>projectname/.git</code>, 适合项目开始时加入版本控制</li>
</ul>
<h3 id="1-3-文件基本操作"><a href="#1-3-文件基本操作" class="headerlink" title="1.3 文件基本操作"></a>1.3 文件基本操作</h3><ul>
<li><code>git add filename/*</code> : 添加文件[产生暂存文件]</li>
<li><code>git commmit -m &quot;message&quot;</code> : 将添加的文件提交到本地仓库[产生提交文件]</li>
<li><code>git rm filename</code> : 移除文件，使用 <code>rm filename</code> 的有暂存</li>
<li><code>git add -u .</code> : 如果之前使用非 git 命令删除文件，可以使用这个命令把当前目录的文件重新遍历清除</li>
<li><code>git rm --cache filename</code> : 暂存但是不参与跟踪</li>
<li><code>git mv filepath newfilepath</code> : 移动文件</li>
<li><code>git rm filepath &amp;&amp; git add newfilepath</code> : 移动文件，之前使用非 git 命令移动文件</li>
<li><code>git add -A .</code> : 如果之前使用非 git 命令移动文件，可以使用这个命令把当前目录的文件重新遍历移动，和 <code>rm</code> 命令类似</li>
<li><code>git reset etc...</code> : 历史提交管理(回退，合并…)，checkout 更关注文件</li>
</ul>
<h3 id="1-4-查看文件修改"><a href="#1-4-查看文件修改" class="headerlink" title="1.4 查看文件修改"></a>1.4 查看文件修改</h3><ul>
<li><code>git status</code> : 查看文件信息</li>
<li><code>git diff</code> : 查看修改[工作树和暂存文件]</li>
<li><code>git diff --staged</code> : 查看修改[暂存文件和最近提交文件]</li>
<li><code>git diff HEAD</code> : 查看修改[工作树和最近提交文件]</li>
<li><code>git diff --word-diff</code> : 查看修改的单词用颜色标出</li>
<li><code>git diff --stat</code> : 查看修改的文件名</li>
</ul>
<blockquote>
<p>参考资料 <a href="http://www.cnblogs.com/feeland/p/4500721.html" target="_blank" rel="external">0. Git 学习（三）本地仓库操作——git add &amp; commit</a> 了解 git 版本库实现</p>
</blockquote>
<h3 id="1-5-查看提交-Log"><a href="#1-5-查看提交-Log" class="headerlink" title="1.5 查看提交 Log"></a>1.5 查看提交 Log</h3><ul>
<li><code>git log</code> : 显示提交信息</li>
<li><code>git log --oneline</code> : 显示提交 Message</li>
<li><code>git log --stat</code> : 显示提交文件名级详细修改信息</li>
<li><code>git log --patch</code> : 显示提交文件内容级详细修改信息 </li>
<li><code>git log --graph</code> : 用图显示提交记录</li>
<li><code>git log --graph --all --decorate --oneline</code> : 去除冗余信息，更加直观显示每条分支每次提交</li>
<li><code>git log --stat -- filename</code> : 文件提交记录（不记录路径移动）</li>
<li><code>git log --stat -M --follow -- filename</code> : 看到完整的文件操作过程</li>
</ul>
<h3 id="1-6-忽略文件"><a href="#1-6-忽略文件" class="headerlink" title="1.6 忽略文件"></a>1.6 忽略文件</h3><ul>
<li><code>touch .gitignore</code> : 创建文件(次级目录也可以创建)</li>
<li><code>vim .gitignore</code> : 编辑文件添加 ignore 文件。<code>*.log | tmp/ | .sass-cache etc...</code></li>
<li><code>git ls-files --others --ignored --exclude-standard</code> : 查看被 ignore 的文件</li>
<li><code>git reflog</code> : 详细修改日志</li>
</ul>
<h3 id="1-7-分支操作"><a href="#1-7-分支操作" class="headerlink" title="1.7 分支操作"></a>1.7 分支操作</h3><ul>
<li><code>git branch branchname</code> : 创建分支</li>
<li><code>git branch</code> : 显示分支</li>
<li><code>git branch -d branchname</code> : 删除分支</li>
<li><code>git branch -D branchname</code> : 删除未合并分支</li>
<li><code>git checkout branchname</code> : 切换分支</li>
<li><code>git checkout commitID</code> : 工作树切换到 commitID 时</li>
<li><code>git checkout -- filename</code> : 清理掉最后一次提交内容</li>
<li><code>git checkout -b branchname</code> : 创建新分支并且进入该分支</li>
<li><code>git merge branchname</code> : 和并 branchname 分支到目前所在分支(合并时文件冲突要手动解决)</li>
<li><code>git merge --abort</code> : 清除工作目录和暂存区</li>
<li><code>git merge squash branchname</code> : 将合并的分支改变变成一个 commit</li>
<li><code>git rebase branchname</code> : 将当前分支历史提交合并到 branchname 分支</li>
</ul>
<blockquote>
<p>参考资料 <a href="https://github.com/geeeeeeeeek/git-recipes/wiki/5.1-%E4%BB%A3%E7%A0%81%E5%90%88%E5%B9%B6%EF%BC%9AMerge%E3%80%81Rebase-%E7%9A%84%E9%80%89%E6%8B%A9" target="_blank" rel="external">2. 代码合并：Merge、Rebase 的选择</a></p>
</blockquote>
<h3 id="1-8-远程操作"><a href="#1-8-远程操作" class="headerlink" title="1.8 远程操作"></a>1.8 远程操作</h3><ul>
<li><code>git remote add origin  https://github.com/accountname/projectname</code></li>
<li><code>git remote set-url origin newUrl</code> : 改变 URL</li>
<li><code>git remote rm origin</code> : 删除</li>
<li><code>git remote -v</code> : 查看 URL</li>
<li><code>git fetch origin</code> : 抓取远程分支，本地会有一个 <code>remotehostname/branchname</code> 的分支，一般用于查看伙伴代码</li>
<li><code>git pull origin</code> : 和 fetch 类似，但是是取回远程更新和本地合并。相当于先 fetch 再 merge。</li>
<li><code>git push origin</code> : push 到远程仓库</li>
</ul>
<blockquote>
<p>参考资料 <a href="http://www.ruanyifeng.com/blog/2014/06/git_remote.html" target="_blank" rel="external">1. Git远程操作详解</a></p>
</blockquote>
<h2 id="2-总结"><a href="#2-总结" class="headerlink" title="2. 总结"></a>2. 总结</h2><blockquote>
<p>其实在开始把 Git 整个命令流程学习下来之前，对 Git 理解十分浅显，使用命令也是抱着试试看的心态，错了搜索解决方法重试，对了就不追究原因了。这种方式感觉更加浪费时间，而且很可能多次遇到重复问题。觉得以后学习某个东西还是得大概了解整个系统才好，想快速入门未尝不可，但是闲下来的时候要去多问问为什么，这样有了宏观的概念，出了问题说不定能自己摸索着解决。还有一点，实践确实是比单一的阅读或看视频更加记忆深刻，对命令的理解也是同理，所以最好都敲一遍。</p>
</blockquote>
<ul>
<li><a href="https://github.com/mk43" target="_blank" rel="external">GitHub</a></li>
<li><a href="http://fitzeng.org">Blog</a></li>
<li><a href="https://juejin.im/user/5791a7a30a2b580061a0e352" target="_blank" rel="external">掘金</a></li>
</ul>
<p>多谢阅读 ^_^</p>
<h2 id="3-参考资料"><a href="#3-参考资料" class="headerlink" title="3. 参考资料"></a>3. 参考资料</h2><ul>
<li><a href="https://www.nowcoder.com/courses/2#chapter-14" target="_blank" rel="external">-1. GitHub&amp;Git入门基础</a></li>
<li><a href="http://www.cnblogs.com/feeland/p/4500721.html" target="_blank" rel="external">0. Git 学习（三）本地仓库操作——git add &amp; commit</a></li>
<li><a href="http://www.ruanyifeng.com/blog/2014/06/git_remote.html" target="_blank" rel="external">1. Git远程操作详解</a></li>
<li><a href="https://github.com/geeeeeeeeek/git-recipes/wiki/5.1-%E4%BB%A3%E7%A0%81%E5%90%88%E5%B9%B6%EF%BC%9AMerge%E3%80%81Rebase-%E7%9A%84%E9%80%89%E6%8B%A9" target="_blank" rel="external">2. 代码合并：Merge、Rebase 的选择</a></li>
<li><a href="https://github.com/geeeeeeeeek/git-recipes" target="_blank" rel="external">3. GitHub 高质量的Git中文教程</a></li>
</ul>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;在使用 Git 时有一种感觉就是只会 &lt;code&gt;add | commit | push&lt;/code&gt; 三个命令。因为这三个命令似乎足以把代码提交到 GitHub 上，即使是碰到某个问题，也可以直接 Google。但是对于其中的逻辑和版本管理的精髓没有体会到，而只是作为一个在线代码仓库在使用，这样做也没有什么不可以，但是如果能系统的了解为什么要有版本管理工具，以及版本管理工具能够给我们提供怎么样的管理服务，对于使用工具的我们来说有很大帮助。不用记住命令，只是以后遇到问题时知道这个工具有实现的方式就足够了。&lt;/p&gt;
&lt;p&gt;现在我系统的看了一下官方的 Git 入门教程 &lt;a href=&quot;https://www.nowcoder.com/courses/2#chapter-14&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Git 官方教程（中字）&lt;/a&gt; 里面通过描述对命令的使用情景进而选择命令进行操作的方式，使得命令很容易理解也容易加深记忆。比起单一的命令用文字解释这种方式，对于不熟悉 Git 的人来说是十分友好的。但是，看完容易忘，所以写了这篇整理笔记用作辅助查询。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Git" scheme="http://fitzeng.org/tags/Git/"/>
    
      <category term="GitHub" scheme="http://fitzeng.org/tags/GitHub/"/>
    
      <category term="Tool" scheme="http://fitzeng.org/tags/Tool/"/>
    
  </entry>
  
  <entry>
    <title>一个小白使用 devstack 部署 openstack 的心路历程</title>
    <link href="http://fitzeng.org/2017/11/04/deployOpenstackByDevstack/"/>
    <id>http://fitzeng.org/2017/11/04/deployOpenstackByDevstack/</id>
    <published>2017-11-04T03:27:00.000Z</published>
    <updated>2017-11-04T04:57:37.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h2><blockquote>
<p>作为一个想要入门云计算的人来说，大多数人的第一步就是学习 openstack，而学习 openstack 的人面临的第一步就是第一个‘难题’，使用自动化部署工具 devstack 部署 openstack 环境。第一次接触这个东西，花了两天多时间在 Ubuntu Server 和 Ubuntu Desktop 的 16.04 版本上成功部署。有很多人说 Desktop 版有很多坑，亲测问题确实比 Server 版多，但都是可解决的。其中最对初学者不友好的就是网络问题，下面会介绍为什么。所以如果你自己买了国外的 VPS 那就好办了，因为有个骚操作要用到，如果你网速好可能你遇不到这些问题就可以忽略。</p>
<p>下面会分两个部分介绍，都会将 Server 和 Desktop 上的部署过程描述一遍。<br>教程推荐 <a href="https://docs.openstack.org/devstack/latest/" target="_blank" rel="external">官方的 Doc</a>和<a href="https://zhuanlan.zhihu.com/p/28996062" target="_blank" rel="external">避坑指南</a><br>教程这个东西对于初学者不宜太多，容易乱，只要有一个正确的执行框架就好。碰到其他 bug 直接 Google 就好。</p>
</blockquote>
<a id="more"></a>
<p>然后介绍下我的环境吧</p>
<ul>
<li>Mac 10.12.6</li>
<li>VirtualBox 5.1.28</li>
<li>Ubuntu Server 16.04 4G+20G (临时测试 devstack，听说坑少)</li>
<li>Ubuntu Desktop 16.04 4G+80G (平时使用)</li>
<li>VPS(最好有) (由于是乞丐版，不适合直接部署和平时学习)</li>
</ul>
<h2 id="1-Ubuntu-Server-版"><a href="#1-Ubuntu-Server-版" class="headerlink" title="1.Ubuntu Server 版"></a>1.Ubuntu Server 版</h2><h3 id="安装-Ubuntu-Server"><a href="#安装-Ubuntu-Server" class="headerlink" title="安装 Ubuntu Server"></a>安装 Ubuntu Server</h3><p>首先肯定是要在 Virtual Box 安装 Ubuntu Server 了，这一步略过。相信你已经是接触过一段时间虚拟机的人了，但是一点注意，竟可能分多一点内存和硬盘。由于我不打算日后再这 Server 版使用，所以我的配置是 4G + 20G</p>
<h3 id="SSH-登录虚拟机"><a href="#SSH-登录虚拟机" class="headerlink" title="SSH 登录虚拟机"></a>SSH 登录虚拟机</h3><p>当你创建完成之后面临的一个问题就是那个界面太丑了。。。所以如果可以在宿主机上操作就好了，SSH 正好满足你。<br>至于 SSH 不通使用不了的自己查查资料吧，这里我主要介绍网卡配置，我使用了两个网卡：<br>第一个：<br><img src="/2017/11/04/deployOpenstackByDevstack/wk1.png" alt=""><br><img src="/2017/11/04/deployOpenstackByDevstack/wk1port.png" alt=""><br>做端口映射，将主机的 2222 映射到虚拟机的 22，这条是为了以后使用 SSH。<br>第二个：<br><img src="/2017/11/04/deployOpenstackByDevstack/wk2.png" alt=""><br>配完之后再在全局配置中设置你所选中的网卡启用 DHCP。对于网卡各种连接模式不熟的可以查查资料了解一下。</p>
<p>然后连接就是直接在主机下使用  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">ssh -p 2222 fitzeng@127.0.0.1</div></pre></td></tr></table></figure>
<p>fitzeng 改成你的用户名。<br>如果你出现各种问题连不上可以注意一下两点：<br>1.防火墙<br>2.把 <code>~/.ssh</code> 文件夹下的 <code>known_hosts</code> 文件删了再重连</p>
<h3 id="开始部署"><a href="#开始部署" class="headerlink" title="开始部署"></a>开始部署</h3><blockquote>
<p>这里的主教程以官方提供的为准，并且那些注意点我会更新。</p>
</blockquote>
<p>部署的脚本要求是拥有 root 权限的非 root 用户。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">sudo useradd -s /bin/bash -d /opt/stack -m stack</div><div class="line"></div><div class="line">echo &quot;stack ALL=(ALL) NOPASSWD: ALL&quot; | sudo tee /etc/sudoers.d/stack</div><div class="line">sudo su - stack</div><div class="line"></div><div class="line">cd /opt/stack</div><div class="line">git clone https://git.openstack.org/openstack-dev/devstack</div><div class="line">cd devstack</div></pre></td></tr></table></figure>
<p>如果上面 clone 太慢或者 clone 不下来的话可以试试 github 的源。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git clone https://github.com/openstack-dev/devstack.git</div></pre></td></tr></table></figure>
<p>然后就是添加配置了，如果不懂推荐直接使用官方页面介绍的。或者使用以下命令</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">cp samples/local.conf ./</div><div class="line">vim local.conf</div></pre></td></tr></table></figure>
<p>如果你幸运，讲道理最后执行 <code>./stack.sh</code> 直接一路到底。。。但是还有很多坑正在等待着我们。<br>但但是有一个很好的是他的 Log 和报错十分清新，很快可以定位问题所在，有时候直接搜 Log 都会出现解决方法。<br>如果脚本直接退出提示没有 HOST_IP。那么直接在 <code>local.conf</code> 后面添加</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">HOST_IP=x.x.x.x</div><div class="line">GIT_BASE=https://github.com</div></pre></td></tr></table></figure>
<p>HOST_IP具体是什么在你的虚拟机上 ifconfig 查看。然后推荐把 git 源换成 github 的。<br>这里你可以检测一下你的源有没有问题 <code>apt-get update</code> 有的话直接把有问题的源在 <code>/etc/apt/sources.list.d/</code> 目录下移除，移除前建议备份一下。然后推荐 <code>apt-get upgrade</code> 一下，Python 版本保持默认的 2.7.X 就好，如果出现什么和 Python 3.4 不匹配的 Log 直接忽略。如果你换成 3.4 很多库会出问题。如果你是 Python 3.X，可以把 <code>/user/bin/</code> 下的 Python2.X 链接到该目录下的 Python 文件。这时执行 <code>python -V</code> 就能看到结果了。</p>
<p>但但但是，上面只是解决了有形的 Bug，还有就是无形的 Bug，你将面临网络问题，如果你想顺畅点可以直接更换源。<br>修改 pip 源</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line">mkdir ~/.pip</div><div class="line">vim ~/.pip/pip.conf</div><div class="line"></div><div class="line">填入：</div><div class="line">[global]</div><div class="line">trusted-host=mirrors.aliyun.com</div><div class="line">index-url=http://mirrors.aliyun.com/pypi/simple</div></pre></td></tr></table></figure>
<p>修改 sources.list：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line">sudo cp /etc/apt/sources.list /etc/apt/sources.list.bak</div><div class="line">sudo vim /etc/apt/sources.list</div><div class="line"></div><div class="line">填入：</div><div class="line"># deb cdrom:[Ubuntu 16.04 LTS _Xenial Xerus_ - Release amd64 (20160420.1)]/ xenial main restricted</div><div class="line">deb-src http://archive.ubuntu.com/ubuntu xenial main restricted #Added by software-properties</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial main restricted</div><div class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial main restricted multiverse universe #Added by software-properties</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted</div><div class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-updates main restricted multiverse universe #Added by software-properties</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial universe</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates universe</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial multiverse</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-updates multiverse</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse</div><div class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-backports main restricted universe multiverse #Added by software-properties</div><div class="line">deb http://archive.canonical.com/ubuntu xenial partner</div><div class="line">deb-src http://archive.canonical.com/ubuntu xenial partner</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted</div><div class="line">deb-src http://mirrors.aliyun.com/ubuntu/ xenial-security main restricted multiverse universe #Added by software-properties</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security universe</div><div class="line">deb http://mirrors.aliyun.com/ubuntu/ xenial-security multiverse</div></pre></td></tr></table></figure>
<p>都改成 aliyun 的。到这时候如果你的网络没什么问题，可能会出点环境小问题，dkpg 和各种包等之类的问题，一搜网上基本都有解决方案。</p>
<p>但但但是，如果你的网速下载某些包不超 10Kb/s 那就要用骚操作了。。。因为会一直卡着，网一断又得重新开始，先 unstack，clean 再 stack，十分不友好。出现问题大多是在下载某个 git 仓库和某些包的时候。尤其是 <code>nova</code> <code>horizon</code> 之类的，大小到了 300+M。</p>
<p>这里介绍一个方法：<br>思路是先 SSH 上你的国外 VPS，下载你的 git 仓库或其他文件。然后再 SCP 到你的虚拟机上。主要是这样不会中断，而且无形中就可以是多线程操作，开几个终端 SCP 好几个文件。<br>看看速度对比效果吧，<br>虚拟机上下载：<br><img src="/2017/11/04/deployOpenstackByDevstack/horizon2.png" alt=""><br>VPS 上下载：<br><img src="/2017/11/04/deployOpenstackByDevstack/horizon.png" alt=""><br>之后自己 SCP 就好了</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">sudo scp -P 10800 -r root@xx.xx.xx.xx:/fitzeng/horizon /etc/stack/</div></pre></td></tr></table></figure>
<p>-r 是 cp 文件夹，然后端口，IP 填你自己的后面跟目录。这里可能也有点慢，但是比之前的好而且稳定。</p>
<p><img src="/2017/11/04/deployOpenstackByDevstack/nova.png" alt=""><br><img src="/2017/11/04/deployOpenstackByDevstack/cirros.png" alt=""><br>这一切操作都源于友好的 log 机制，看上面的图片我们可以知道下载地址和存放目录，所以，知道这些手段就多了起来。<br>网速够快也可以直接在本地 clone。<br><img src="/2017/11/04/deployOpenstackByDevstack/nova2.png" alt=""></p>
<p>有了这些操作基本就意味着你解决了网络问题，借助 google 基本可以解决其他库和环境的问题。<br>成功图上传一波：<br><img src="/2017/11/04/deployOpenstackByDevstack/finish.png" alt=""></p>
<h2 id="2-Ubuntu-Desktop-版"><a href="#2-Ubuntu-Desktop-版" class="headerlink" title="2.Ubuntu Desktop 版"></a>2.Ubuntu Desktop 版</h2><p>基本步骤是和前面一致的，出的问题可能就是你之前在 Ubuntu 上装过各种软件(我装的 Sogou 输入法，里面的 fcitx 源影响了速度，甚至有时候直接卡这不动)，更改了软件源或者做过其它的工具更改，按照前面的配置亲测可行。如果你之前在 Ubuntu Server 版上装过，直接把文件 SCP 过来，如果虚拟机之间不能通讯，可以先 SCP 到宿主机，再从宿主机通过文件共享的方式共享到 Ubuntu Desktop。<br>然后运行就可以了，有了前面的基础就很简单了。</p>
<p>那就看直接看结果吧：<br><img src="/2017/11/04/deployOpenstackByDevstack/d1.png" alt=""><br><img src="/2017/11/04/deployOpenstackByDevstack/d2.png" alt=""><br><img src="/2017/11/04/deployOpenstackByDevstack/d3.png" alt=""><br><img src="/2017/11/04/deployOpenstackByDevstack/d4.png" alt=""></p>
<h2 id="3-后记"><a href="#3-后记" class="headerlink" title="3.后记"></a>3.后记</h2><p>说实话，这不太算技术文章，纯属个人记录。本来不太想写，但是感觉国内环境对开发者有点不友好，如果这篇文章能对初学者有部分帮助我就满意了，能够使初学者继续学习下去。然后这是部署完之后写的，部署的过程远不如写的这么轻松，但是我现在有信心去解决部署过程中碰到的问题，这才是重点。希望你也是。每个人的环境都不一样，出现的问题也不可能一样，所以如果你照上面做了还没有解决可以留言大家一起讨论。</p>
<p>最后：<br>多谢阅读<br>祝大家一遍过 ^_^</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.前言&quot;&gt;&lt;/a&gt;0.前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;作为一个想要入门云计算的人来说，大多数人的第一步就是学习 openstack，而学习 openstack 的人面临的第一步就是第一个‘难题’，使用自动化部署工具 devstack 部署 openstack 环境。第一次接触这个东西，花了两天多时间在 Ubuntu Server 和 Ubuntu Desktop 的 16.04 版本上成功部署。有很多人说 Desktop 版有很多坑，亲测问题确实比 Server 版多，但都是可解决的。其中最对初学者不友好的就是网络问题，下面会介绍为什么。所以如果你自己买了国外的 VPS 那就好办了，因为有个骚操作要用到，如果你网速好可能你遇不到这些问题就可以忽略。&lt;/p&gt;
&lt;p&gt;下面会分两个部分介绍，都会将 Server 和 Desktop 上的部署过程描述一遍。&lt;br&gt;教程推荐 &lt;a href=&quot;https://docs.openstack.org/devstack/latest/&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;官方的 Doc&lt;/a&gt;和&lt;a href=&quot;https://zhuanlan.zhihu.com/p/28996062&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;避坑指南&lt;/a&gt;&lt;br&gt;教程这个东西对于初学者不宜太多，容易乱，只要有一个正确的执行框架就好。碰到其他 bug 直接 Google 就好。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="devstack" scheme="http://fitzeng.org/tags/devstack/"/>
    
      <category term="openstack" scheme="http://fitzeng.org/tags/openstack/"/>
    
  </entry>
  
  <entry>
    <title>完美解决 Linux 的【dpkg： warning： files list file for package &#39;XXXXXXX&#39; missing, assuming package has no files currently installed】Bug</title>
    <link href="http://fitzeng.org/2017/11/04/linuxDpkgBug/"/>
    <id>http://fitzeng.org/2017/11/04/linuxDpkgBug/</id>
    <published>2017-11-04T03:27:00.000Z</published>
    <updated>2017-11-04T15:12:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0.前言"></a>0.前言</h2><blockquote>
<p>估计是之前动了或者损坏了 <code>/var/lib/dpkg/info</code> 里面的文件，每次执行 apt 类的命令总是输出一大段东西，在网上找了很多资料，有解决方案，但是不全。。。很多都是失败的。最后我发现 reinstall 可以解决，所以打算写个脚本执行文件。</p>
</blockquote>
<a id="more"></a>
<h2 id="1-解决方法"><a href="#1-解决方法" class="headerlink" title="1.解决方法"></a>1.解决方法</h2><h3 id="1-1-创建一下三个文件"><a href="#1-1-创建一下三个文件" class="headerlink" title="1.1 创建一下三个文件"></a>1.1 创建一下三个文件</h3><ul>
<li>fixit.py</li>
<li>fix.sh</li>
<li>txt</li>
</ul>
<h3 id="1-2-填写内容"><a href="#1-2-填写内容" class="headerlink" title="1.2 填写内容"></a>1.2 填写内容</h3><p>先来最简单的 <code>fix.sh</code>，不用填写内容，是空文件。</p>
<p>接着就写 <code>txt</code>，直接把错误日志复制进去，如：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">dpkg: warning: files list file for package &apos;libodbc1:amd64&apos; missing; assuming package has no files currently installed</div><div class="line">dpkg: warning: files list file for package &apos;dh-autoreconf&apos; missing; assuming package has no files currently installed</div><div class="line">dpkg: warning: files list file for package &apos;erlang-webtool&apos; missing; assuming package has no files currently installed</div><div class="line">dpkg: warning: files list file for package &apos;libhtml-template-perl&apos; missing; assuming package has no files currently installed</div><div class="line">.......</div><div class="line">dpkg: warning: files list file for package &apos;libvirt-dev:amd64&apos; missing; assuming package has no files currently installed</div><div class="line">dpkg: warning: files list file for package &apos;autopoint&apos; missing; assuming package has no files currently installed</div><div class="line">dpkg: warning: files list file for package &apos;libconfig-general-perl&apos; missing; assuming package has no files currently installed</div><div class="line">dpkg: warning: files list file for package &apos;ubuntu-cloud-keyring&apos; missing; assuming package has no files currently installed</div><div class="line">dpkg: warning: files list file for package &apos;tgt&apos; missing; assuming package has no files currently installed</div><div class="line">dpkg: warning: files list file for package &apos;libfdt1:amd64&apos; missing; assuming package has no files currently installed</div></pre></td></tr></table></figure>
<p>下面写 <code>fixit.py</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">#!/usr/bin/env python</div><div class="line"># -*- coding: utf-8 -*-</div><div class="line"></div><div class="line">__author__ = &apos;Fitzeng&apos;</div><div class="line"></div><div class="line">import re</div><div class="line"></div><div class="line">def main():</div><div class="line">    fix = open(&apos;fix.sh&apos;, &apos;w+&apos;)</div><div class="line">    for line in open(&quot;txt&quot;):</div><div class="line">        pkg = re.match(re.compile(&apos;&apos;&apos;dpkg: warning: files list file for package &apos;(.+)&apos; &apos;&apos;&apos;), line)</div><div class="line">        if pkg:</div><div class="line">            cmd = &quot;sudo apt-get install --reinstall &quot; + pkg.group(1)</div><div class="line">            fix.write(cmd + &apos;\n&apos;)</div><div class="line"></div><div class="line">if __name__ == &quot;__main__&quot;:</div><div class="line">    main()</div></pre></td></tr></table></figure>
<h3 id="1-3-执行命令"><a href="#1-3-执行命令" class="headerlink" title="1.3 执行命令"></a>1.3 执行命令</h3><p>如果权限不够可以直接先 <code>chmod 777 *</code>，然后执行 <code>python fixit.py</code>，这时 <code>fix.sh</code> 就变成下面的样子了。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install --reinstall libodbc1:amd64</div><div class="line">sudo apt-get install --reinstall dh-autoreconf</div><div class="line">sudo apt-get install --reinstall erlang-webtool</div><div class="line">sudo apt-get install --reinstall libhtml-template-perl</div><div class="line">sudo apt-get install --reinstall erlang-base</div><div class="line"></div><div class="line">.......</div><div class="line"></div><div class="line">sudo apt-get install --reinstall qemu-system-misc</div><div class="line">sudo apt-get install --reinstall libvirt-dev:amd64</div><div class="line">sudo apt-get install --reinstall autopoint</div><div class="line">sudo apt-get install --reinstall libconfig-general-perl</div><div class="line">sudo apt-get install --reinstall ubuntu-cloud-keyring</div><div class="line">sudo apt-get install --reinstall tgt</div><div class="line">sudo apt-get install --reinstall libfdt1:amd64</div></pre></td></tr></table></figure>
<p>最后执行 <code>./fix.sh</code>。</p>
<p>然后就是等待执行结束了。</p>
<p>效果如下，一行一行 <code>dpkg: warning:</code> 在减少。<br><img src="/2017/11/04/linuxDpkgBug/debuging.png" alt=""><br><img src="/2017/11/04/linuxDpkgBug/debuging1.png" alt=""></p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0.前言&quot;&gt;&lt;/a&gt;0.前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;估计是之前动了或者损坏了 &lt;code&gt;/var/lib/dpkg/info&lt;/code&gt; 里面的文件，每次执行 apt 类的命令总是输出一大段东西，在网上找了很多资料，有解决方案，但是不全。。。很多都是失败的。最后我发现 reinstall 可以解决，所以打算写个脚本执行文件。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="linux" scheme="http://fitzeng.org/tags/linux/"/>
    
      <category term="dpkg" scheme="http://fitzeng.org/tags/dpkg/"/>
    
  </entry>
  
  <entry>
    <title>Restart</title>
    <link href="http://fitzeng.org/2017/09/22/restart/"/>
    <id>http://fitzeng.org/2017/09/22/restart/</id>
    <published>2017-09-22T03:27:00.000Z</published>
    <updated>2017-09-23T14:55:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0-前言"><a href="#0-前言" class="headerlink" title="0. 前言"></a>0. 前言</h2><blockquote>
<p>我的求职生活是从今年的 04-16 找实习开始到 09-22 拿到优招(秋招) Offer结束，中间共经历过了三家公司面试，面试经验不足，而且最近的面试已距现在两个多月了，细节记不清了，所以这不太算是面经，只是我个人对我自己找工作这小半年的总结以及我对找工作这件事的认识。首先说明我不是大神，只能从平静叙述整个找工作的过程，希望你能从中体会到一点什么就很满意了，在此先祝大家找工作顺利！</p>
</blockquote>
<a id="more"></a>
<h2 id="1-面试过程"><a href="#1-面试过程" class="headerlink" title="1. 面试过程"></a>1. 面试过程</h2><blockquote>
<p>自己要介绍自己在三家公司的面试过程，不涉及面试题目什么的，纯属叙述整个心理过程。不过后面我会补充一些资料和我参考的一些其他面试者的面经。</p>
</blockquote>
<p>下面我根据邮件理一下时间线：</p>
<h4 id="第一家公司（腾讯）实习-跪"><a href="#第一家公司（腾讯）实习-跪" class="headerlink" title="第一家公司（腾讯）实习-跪"></a>第一家公司（腾讯）实习-跪</h4><p><code>一面：04-16</code><br>大概记忆就是啥都问，读过我以前文章的都知道，我以前是写 Android 的，但是他问了 Linux，C/C++，操作系统，JVM，设计模式，算法，Andriod，还有一些小智力测试题。目前我只能记清这些知识点，这个过程是一开始问了十多分钟项目，再问问 Android 基础(答得很烂)，再问 JVM(也很烂😭)，接着就给我一张纸上面两道题手写代码，面试官就喝水去了，然后我没意识到有两面题，只做了第一面的两道告诉面试官我做好了，他过来了，我跟他讲解代码（因为字迹实在难看清😂），在他的引导下慢慢把第二题进行优化。接着就是问问计算机基础的一些知识。面了有将近一个小时，这是我第一次面试，流了一身的汗。<br>我对面试结果是很不乐观的，因为我是在 04-1X 才知道这是已经接近实习招聘尾声。基本上大多数公司的内推什么的都弄完了，而我也是后知后觉，04-14 晚上做简历，因为听说腾讯 04-15 截止投递实习简历，到 04-15 早上，我做完简历就投了。可能是里面有一些学长学姐吧，当天下午就收到内推通知并告知完善简历，接着晚上就收到通知明天去面试。整个感觉很惊喜，但是其实后来跪了之后想想其实是自己没有做好准备，根本不知道准备实习人家从三月份就开始找了，所以有信息来源很重要。本来以为一面要挂的结果第二天晚上就通知过了并且过一天第二面。</p>
<p><code>二面：04-18</code><br>这一面很崩，后来才知道可能是压力面。说实话，第一面能过之后我已经放松了很多。二面内容和 Android 一毛钱关系都没有。同样一开始聊聊项目，然后桌子上有一堆大概十厘米厚的 A4 纸，拿一张，口述一道题然后写代码，这里提醒下大家如果遇到这种情况，最好在动笔前问清楚问题条件，比如我一动笔写了一点然后问他字符串是以什么形式存储的，然后他回答了 char *，接着给我感觉有点不太舒服，气氛很严肃，但是我又问了一个问题：是写伪代码还是？结果崩了，他立马说，都这时候了还写什么伪代码！好，我默默的划掉，又写了一段代码，由于混在一起，我就把代码又抄了一遍，但是有些语句一开始抄漏所以出现了两句代码在一行的情况。。。他看了边叹气边说：“这些的什么，乱七八糟的，缩进XXXXXXXXX”，然后我弱弱地说要不我再抄一遍把格式调调？结果他说还调什么调，浪费我时间！😂😂😂😂😂😂把我吓坏了，然后弱弱的给他讲解了代码，由于代码写的还不错，这里他没揪我的小辫子。然后他又说给你来一道博弈题？？？我黑人问号，我面的是 Android 吗？口述完题目后，他看到了我有点高兴，问我是不是看过类似的题目？我诚实的回答是。结果他说给你出道简单一点的题？？？？？念完题目之后我一脸懵逼，结果最后我对题目的看法和面试官不符，然后挂了。总结就是：抱歉，我不会演戏。。。当时想如果我稍微冷静一点，把博弈题拿下应该这面能过吧，但是没有如果。。。</p>
<h4 id="第二家公司（华为）1-实习-跪"><a href="#第二家公司（华为）1-实习-跪" class="headerlink" title="第二家公司（华为）1. 实习-跪"></a>第二家公司（华为）1. 实习-跪</h4><p><code>技术面：05-13</code><br>由于面试官不是做 Android，给我感觉连 Java 相关都不是，所以全程在聊天，什么你家在哪里？为什么不读研？你能接受加班吗？之类的。。。。大概二十多分钟，结束得很轻松。</p>
<p><code>综合面：05-13</code><br>由于技术面没问技术，所以这面大概就是和前面问的问题差多，时间也是差不多，感觉很水。。。</p>
<p>补充：前面都是同一天面完，感觉很诡异，不太像面试。。。回到学校后，过几天面试状态就是录用排序中。算是面试过了，然后一两周过后有同班的同学收到实习 offer，而我没消息。直到听说实习 offer 都发完了我才知道我挂了。所以说华为的面试通过和 offer 还有一段资源池的距离。后来一个学长 hr 来到学校找同学谈话，这时我被告知技术面给了个 B+。。。这个对我后面影响挺大的，然后综合面是 OK 的。所以面试时算过，本来也可以给 offer 但是没有岗位，只招一个 Android 实习的 🙂。但是这时候我还可以用这个安慰自己，不是你差，是市场需求，这是没办法的事。后续的是就是学长跟我谈了谈人生规划啊，然后推荐我参加优招。后面就是优招的故事了；</p>
<h4 id="第三家公司（阿里）实习-跪"><a href="#第三家公司（阿里）实习-跪" class="headerlink" title="第三家公司（阿里）实习-跪"></a>第三家公司（阿里）实习-跪</h4><p><code>一面：05-20</code><br>阿里是在线面试，给我的感觉是面过最难的，问得我无从回答，因为我基本没有项目经验，什么说说你在开发中碰到印象最深刻的问题是什么？遇到网上怎么也查不到答案的问题怎么办？还有你对 RN（当时只知道有个 RN 开发，并不清楚是什么） 开发怎么看？反正我答不上来，所以直接毙了。后面他问我有什么问题问他，我问他他对我面试感觉怎么样？他说我基础不行，然后我又问了要怎么学习？他说 Java 要学好，反射注解什么的。总之就是基础要扎实。。。</p>
<h4 id="第二家公司（华为）2-优招-过"><a href="#第二家公司（华为）2-优招-过" class="headerlink" title="第二家公司（华为）2. 优招-过"></a>第二家公司（华为）2. 优招-过</h4><p><code>补技术面：07-14</code><br>由于前面的实习面试给了个 B+，所以这次要补一个技术面。面试官也不是做 Android 的，不过是和 Java 相关的，所以基本就是问我 Java 了。也是开始聊聊项目和生活，然后问问 Java 内存泄漏，GC，接着考考算法，一道迷宫的题目，说下用深度优先和广度优先遍历的实现思想，进而问如果要找出一条最少步骤的路走出迷宫怎么办？基本都答上来了，最后他说我问你一个 Java 问题，我以为会很难。结果他说被 final 关键字修饰的类会怎么样？我迟疑了一下，说了答案。。。然后面试就结束了。给我的感觉应该是能过的。</p>
<p><code>BOSS 面：07-14</code><br>等了好久，最后排队排不上上午的了，然后发了餐券去吃了个饭回来面试。面试过程和实习面试时差不多，但是那个面试官的级别感觉挺高的，白头发特别多，估计 50+ 吧，最后加了个群，面试结束了。感觉没什么感觉，当时面试的研究生特别多，所以我感觉可能要 GG。</p>
<p>等了两个多月后，等到 09-21 发来的签约短信，09-22 以白菜价签了三方。是自己满意的城市加上整个暑假都在学校，09 月份投了十多家公司和做了 7-8份笔试题，心太累实在是不想等了。。。签的过程中有个更改城市的小插曲，hr 很好，忙到晚上 7 点多才结束，在这表示感谢。还有一个小插曲是当天有一个腾讯的面试，这是我的收到的第一个秋招面试，由于签了华为的三方，所以也没有去面了，准确说，其实我没有秋招，除了投了十几分简历和做了几次笔试。</p>
<h2 id="2-面试准备"><a href="#2-面试准备" class="headerlink" title="2. 面试准备"></a>2. 面试准备</h2><blockquote>
<p>这里主要叙述我的暑假和九月上旬这个面试准备过程和自己对面是准备的看法。</p>
</blockquote>
<p>说实话，没有实习 offer 对我算是有点打击的。所以我就打算暑假不回家好好复习基础知识，我觉得对于一个本科生来说，基础扎实才是你的优势，对于这里可能每个人看法不一，对于每个方向可能有差距吧，这里我只是平静地表达我的观点。</p>
<p>所以我基本放下 Android 了，只是挑热点面试问题过过。说下我做过的努力：</p>
<p>1.<a href="https://juejin.im/post/598454dcf265da3e26095dca" target="_blank" rel="external">重拾数据结构</a><br>2.<a href="https://juejin.im/post/59a7b8c9f265da24777a07da" target="_blank" rel="external">重拾操作系统</a><br>3.<a href="https://juejin.im/post/59ad4cd56fb9a02477075780" target="_blank" rel="external">重读 JVM</a><br>4.<a href="http://fitzeng.org/2017/07/08/AndroidBlogCollection/">Android Blog Collection</a><br>5.<a href="http://fitzeng.org/2017/09/11/handler/">Handler 机制再了解</a></p>
<p>以上是以 Blog 形式进行了总结的，还有部分书籍没有总结的。</p>
<p><img src="/2017/09/22/restart/books.png" alt=""></p>
<p>里面的看了有 30% 吧，都是采取不重要的粗读重要的精读模式。电子版下载 <a href="http://pan.baidu.com/s/1o7LcZCe" target="_blank" rel="external">链接 : http://pan.baidu.com/s/1o7LcZCe</a> 密码 : 0p0c 。希望大家有条件去支持纸质书吧，我对知识创造者还是很尊重的，因为自己发现写 Blog 每个礼拜一篇都写不来，但是对于部分学习计算机的人来说，书确实有点多，也要花很多钱，所以取舍之下就是有条件的现在支持原作者，没条件的以后有条件了再支持。好像偏题了。。。</p>
<p>期间看过 Android 开发艺术探索，Android 群英传，Android 进阶之光。之前还看过 编程之美，编程珠玑，编程之法，都没有自己敲代码实现，所以处于有思路但不能写代码实现的状态。然后就是一些专业课课本了。对于读书这件事我觉得没有必要为一个知识点死磕，我一般就是往后翻，基本两天跳着‘看’完一本书。时间固定的情况下宁愿跳着多看几遍，也不追求一遍仔细看完。因为你面临的结果就是记不住，没有重点的重复刺激下知识网络建立不起来。往往阅读到后面，前面的疑问就会迎刃而解。</p>
<p>然后说说岗位吧，以后我可能就不会再写和 Android 相关的文章了，签的工作是云计算所以打算学学 Python 和 OpenStack 之类的，从新开始接触一个新的领域。所以如果你不是对一个岗位持有非做不可的态度，其实我觉得那就不要太在意那些花哨的东西，注重基础的积累。比如之前面试有问我学过什么框架吗？我说没学过，然后这个话题就跳过了。当然学过加分，没学过我觉得不减分吧，毕竟应届生。但是一个 Java 基础的语言特性答不上来那就有点说不过去了。而且框架的精髓在读源码之后转化为自己的理解从而应用在自己代码中，而不是会使用框架，这种价值不大。如果你是大神，基础扎实，那这些就是你的进阶，你的加分项，本质不一样。就像试卷的必答题答完了肯定是去答选做题，但是有些人连必答题都没做完就跟着别人翻卷子是不是有点。。。总之就是注重语言本身，而不是语言衍生品。</p>
<p>接着说说面经，这是很好的东西，但是也是很不好的东西，看你的认知。有人对面经的态度是题库，觉得看完记着我面试就能过了，同一个问题，不同的人说相同的答案面试官也能知道哪个厉害哪个水。大家可以试试对一个自己不理解的技术问题尝试解释会怎样？你一迟疑，面试官就会反问你，最后只有一个字崩！而且那些面经提供的答案往往是符合作者的思考思路，不一定对你适用。所以我对面经的态度是类似于考纲，至于考纲和题库的区别留给读者思考吧。</p>
<p>然后我面试准备过程中参考的好的‘考纲’</p>
<ol>
<li><a href="http://www.jianshu.com/p/f661953ba032" target="_blank" rel="external">[干货，阅后进BAT不是梦] 面试心得与总结—BAT、网易、蘑菇街</a></li>
<li>自己动手丰衣足食，前面一篇是我认为的好面经，可以作为查找参考。</li>
</ol>
<h2 id="3-最后"><a href="#3-最后" class="headerlink" title="3. 最后"></a>3. 最后</h2><p>如果你觉得对你有帮助就点下赞吧，让更多的人看到，希望能帮到更多的人。这是我第一篇求赞的文章，因为确实想和大家分享我的想法。谢谢。</p>
<p>再如果你有相同或者不同的意见，欢迎评论区留言大家互相讨论呀~</p>
<p>然后国际惯例：<a href="http://fitzeng.org/">GitHub </a> &amp;&amp; <a href="https://github.com/mk43" target="_blank" rel="external"> Blog</a> 欢迎来观光 ^_^</p>
<p>多谢阅读！</p>
]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;0-前言&quot;&gt;&lt;a href=&quot;#0-前言&quot; class=&quot;headerlink&quot; title=&quot;0. 前言&quot;&gt;&lt;/a&gt;0. 前言&lt;/h2&gt;&lt;blockquote&gt;
&lt;p&gt;我的求职生活是从今年的 04-16 找实习开始到 09-22 拿到优招(秋招) Offer结束，中间共经历过了三家公司面试，面试经验不足，而且最近的面试已距现在两个多月了，细节记不清了，所以这不太算是面经，只是我个人对我自己找工作这小半年的总结以及我对找工作这件事的认识。首先说明我不是大神，只能从平静叙述整个找工作的过程，希望你能从中体会到一点什么就很满意了，在此先祝大家找工作顺利！&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="Life" scheme="http://fitzeng.org/tags/Life/"/>
    
      <category term="Job" scheme="http://fitzeng.org/tags/Job/"/>
    
  </entry>
  
</feed>
